{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348e8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pylab\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "from numpy.linalg import *\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from models.PredatorPolicy import PredatorPolicy\n",
    "from models.PreyPolicy import PreyPolicy\n",
    "\n",
    "mpl.use('TkAgg')\n",
    "from utils.couzin_utils import run_couzin_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ModularNetworks import PairwiseInteraction, Attention\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModularPolicy(nn.Module):\n",
    "    def __init__(self, features=4):\n",
    "        super(ModularPolicy, self).__init__()\n",
    "\n",
    "        self.pairwise = PairwiseInteraction(features)\n",
    "        self.attention = Attention(features)\n",
    "\n",
    "    def forward(self, states, deterministic=True):\n",
    "        mu, sigma = self.pairwise(states)\n",
    "\n",
    "        weights_logit = self.attention(states)\n",
    "        weights = torch.softmax(weights_logit, dim=1)\n",
    "\n",
    "        if deterministic:\n",
    "            scaled_action = torch.sigmoid(mu)\n",
    "            action = (scaled_action * weights).sum(dim=1)\n",
    "            return action\n",
    "        else:\n",
    "            action = Normal(mu, sigma).rsample()\n",
    "            scaled_action = torch.sigmoid(action)\n",
    "            action = (scaled_action * weights).sum(dim=1)\n",
    "            return action\n",
    "        \n",
    "    def set_parameters(self, init=True):\n",
    "        if init is True:\n",
    "            for layer in self.modules():\n",
    "                if hasattr(layer, 'reset_parameters'):\n",
    "                    layer.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "def pretrain_policy(policy, expert_data, batch_size=256, epochs=250, lr=1e-3, deterministic=True, device='cpu'):\n",
    "    policy.to(device)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    frames, agents, neigh, features = expert_data.shape\n",
    "    expert_data = expert_data.view(frames * agents, neigh, features)\n",
    "    \n",
    "    states = expert_data[..., :4]\n",
    "    actions = expert_data[:, 0, 4]\n",
    "\n",
    "    dataset = TensorDataset(states, actions)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        for s, a in loader:\n",
    "            states = s.to(device)\n",
    "            actions = a.to(device)\n",
    "\n",
    "            action_prey = policy.forward(states, deterministic=deterministic)\n",
    "\n",
    "            loss = F.mse_loss(action_prey, actions)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2dff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_14096\\2545238268.py:59: UserWarning: Using a target size (torch.Size([3200])) that is different to the input size (torch.Size([3200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(action_prey, actions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.465000\n",
      "Epoch 50/100, Loss: 0.381186\n",
      "Epoch 75/100, Loss: 0.363026\n",
      "Epoch 100/100, Loss: 0.359746\n",
      "Pretraining done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc_folder = \"BC Training - 19.12.2025_14.25 - Couzin Data\"\n",
    "\n",
    "model_folder = rf\"..\\data\\2. Training\\training\"\n",
    "bc_path = os.path.join(model_folder, \"BC\", bc_folder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# BC Simulation\n",
    "pred_policy = torch.load(os.path.join(bc_path, \"bc_pred_policy.pt\"), weights_only=False)\n",
    "\n",
    "expert_pred_tensor, expert_prey_tensor, logs = run_couzin_simulation(visualization=\"off\", max_steps=100, alpha=0.01, \n",
    "                                                       constant_speed=2, shark_speed=5, area_width=50, area_height=50, \n",
    "                                                       number_of_sharks=1, n=32)\n",
    "\n",
    "\n",
    "policy = ModularPolicy().to(device)\n",
    "pretrain_policy(policy, expert_data=expert_prey_tensor, batch_size=4048, epochs=100, lr=1e-3, deterministic=True, device=device)\n",
    "print(\"Pretraining done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_id, speed, area_width, area_height):\n",
    "        self.id = agent_id\n",
    "        self.pos = np.array([np.random.uniform(0, area_width),\n",
    "                             np.random.uniform(0, area_height)], dtype=np.float64)\n",
    "\n",
    "        self.theta = np.random.uniform(-np.pi, np.pi)\n",
    "        self.vel = np.array([np.cos(self.theta), np.sin(self.theta)], dtype=np.float64) * speed\n",
    "\n",
    "    def update_position(self, dt):\n",
    "        self.pos += self.vel * dt\n",
    "\n",
    "\n",
    "def apply_turnrate_on_theta(agent, dtheta_raw, speed, max_turn=np.pi/12):\n",
    "    d = float(dtheta_raw)\n",
    "\n",
    "    d = (np.clip(d, 0.0, 1.0) * 2.0 - 1.0) * max_turn\n",
    "\n",
    "    agent.theta = (agent.theta + d + np.pi) % (2*np.pi) - np.pi\n",
    "    agent.vel = np.array([np.cos(agent.theta), np.sin(agent.theta)], dtype=np.float64) * speed\n",
    "\n",
    "\n",
    "\n",
    "def enforce_walls(agent, area_width, area_height):\n",
    "    bounced = False\n",
    "\n",
    "    if agent.pos[0] < 0:\n",
    "        agent.pos[0] = 0\n",
    "        agent.theta = np.pi - agent.theta\n",
    "        bounced = True\n",
    "    elif agent.pos[0] > area_width:\n",
    "        agent.pos[0] = area_width\n",
    "        agent.theta = np.pi - agent.theta\n",
    "        bounced = True\n",
    "\n",
    "    if agent.pos[1] < 0:\n",
    "        agent.pos[1] = 0\n",
    "        agent.theta = -agent.theta\n",
    "        bounced = True\n",
    "    elif agent.pos[1] > area_height:\n",
    "        agent.pos[1] = area_height\n",
    "        agent.theta = -agent.theta\n",
    "        bounced = True\n",
    "\n",
    "    if bounced:\n",
    "        agent.theta = (agent.theta + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "\n",
    "\n",
    "def get_state_tensors(prey_log_step, pred_log_step, n_pred=1, \n",
    "                      area_width=50, area_height=50,\n",
    "                      prey_speed=5, pred_speed=5, device=\"cuda\"):\n",
    "    \n",
    "    combined = np.vstack([pred_log_step, prey_log_step]).astype(np.float32)  # [N,6]\n",
    "    n_agents = combined.shape[0]\n",
    "\n",
    "    xs, ys  = combined[:, 0], combined[:, 1]\n",
    "    vxs, vys = combined[:, 2], combined[:, 3]\n",
    "    dir_x, dir_y = combined[:, 4], combined[:, 5]\n",
    "\n",
    "    theta = np.arctan2(dir_y, dir_x).astype(np.float32)\n",
    "    theta_norm = (theta / np.pi).astype(np.float32)\n",
    "    cos_t = np.cos(theta).astype(np.float32)\n",
    "    sin_t = np.sin(theta).astype(np.float32)\n",
    "\n",
    "    xs_scaled = np.clip(xs, 0, area_width) / float(area_width)\n",
    "    ys_scaled = np.clip(ys, 0, area_height) / float(area_height)\n",
    "\n",
    "    dx = xs_scaled[None, :] - xs_scaled[:, None]\n",
    "    dy = ys_scaled[None, :] - ys_scaled[:, None]\n",
    "\n",
    "    rel_vx = cos_t[:, None] * vxs[None, :] + sin_t[:, None] * vys[None, :]\n",
    "    rel_vy = -sin_t[:, None] * vxs[None, :] + cos_t[:, None] * vys[None, :]\n",
    "\n",
    "    speed = max(prey_speed, pred_speed)\n",
    "    rel_vx = np.clip(rel_vx, -speed, speed) / speed\n",
    "    rel_vy = np.clip(rel_vy, -speed, speed) / speed\n",
    "\n",
    "    theta_mat = np.tile(theta_norm[:, None], (1, n_agents))\n",
    "    features = np.stack([dx, dy, rel_vx, rel_vy, theta_mat], axis=-1).astype(np.float32)\n",
    "\n",
    "    mask = ~np.eye(n_agents, dtype=bool)\n",
    "    neigh = features[mask].reshape(n_agents, n_agents-1, 5)  # [agent, neigh, feat]\n",
    "\n",
    "    tensor = torch.from_numpy(neigh).unsqueeze(0).to(device=device, dtype=torch.float32)\n",
    "    step, agent, neigh, feat = tensor.size()\n",
    "\n",
    "    tensor = tensor.view(step * agent, neigh, feat)  # [1*agent, N-1, 5]\n",
    "\n",
    "    pred_tensor = tensor[:n_pred]    # [n_pred, N-1, 5] (kann 0 sein)\n",
    "    prey_tensor = tensor[n_pred:]    # [n_prey, N-1, 5]\n",
    "\n",
    "    pred_states = pred_tensor[..., :4]\n",
    "    prey_states = prey_tensor[..., :4]\n",
    "\n",
    "    return pred_states, prey_states\n",
    "\n",
    "\n",
    "\n",
    "def run_env_simulation(prey_policy=None, pred_policy=None, n_prey=32, n_pred=1, max_steps=100, prey_speed=15, pred_speed=15, area_width=50, area_height=50, visualization='off', device=\"cpu\"):\n",
    "\n",
    "    prey_policy.to(device)\n",
    "    pred_policy.to(device)\n",
    "\n",
    "    prey = [Agent(i, prey_speed, area_width, area_height) for i in range(n_prey)]\n",
    "    pred = [Agent(i, pred_speed, area_width, area_height) for i in range(n_pred)]\n",
    "\n",
    "    prey_pos = np.zeros((n_prey, 2))\n",
    "    prey_vel = np.zeros((n_prey, 2))\n",
    "    \n",
    "    pred_pos = np.zeros((n_pred, 2))\n",
    "    pred_vel = np.zeros((n_pred, 2))\n",
    "\n",
    "    t = 0\n",
    "\n",
    "    if visualization == 'on':\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    prey_log = np.zeros((max_steps, n_prey, 6))\n",
    "    predator_log = np.zeros((max_steps, n_pred, 6))\n",
    "\n",
    "    pred_tensor_list = []\n",
    "    prey_tensor_list = []\n",
    "\n",
    "    while t < max_steps:\n",
    "        # Prey\n",
    "        for i, agent in enumerate(prey):\n",
    "            prey_pos[i, :] = agent.pos\n",
    "            prey_vel[i, :] = agent.vel\n",
    "\n",
    "            vel_norm = (norm(agent.vel[0:2]) + 1e-12)\n",
    "            if vel_norm > 1e-12:\n",
    "                dir_xy = agent.vel[0:2] / vel_norm\n",
    "            else:\n",
    "                dir_xy = np.zeros(2, dtype=np.float64)\n",
    "\n",
    "            prey_log[t, i, 0:2] = agent.pos[0:2]\n",
    "            prey_log[t, i, 2:4] = agent.vel[0:2]\n",
    "            prey_log[t, i, 4:6] = dir_xy\n",
    "\n",
    "        if n_pred > 0:\n",
    "            for i, predator in enumerate(pred):\n",
    "                pred_pos[i, :] = predator.pos\n",
    "\n",
    "                v = norm(predator.vel)\n",
    "                pred_vel[i, :] = predator.vel / (v + 1e-12) / 80 * area_width\n",
    "\n",
    "                vel_norm_s = norm(predator.vel[0:2])\n",
    "                if vel_norm_s > 1e-12:\n",
    "                    dir_xy_s = predator.vel[0:2] / vel_norm_s\n",
    "                else:\n",
    "                    dir_xy_s = np.zeros(2, dtype=np.float64)\n",
    "\n",
    "                predator_log[t, i, 0:2] = predator.pos[0:2]\n",
    "                predator_log[t, i, 2:4] = predator.vel[0:2]\n",
    "                predator_log[t, i, 4:6] = dir_xy_s\n",
    "\n",
    "        # --- Visualization ---\n",
    "        if visualization == 'on':\n",
    "            ax.clear()\n",
    "\n",
    "            pylab.quiver(\n",
    "                prey_pos[:, 0], prey_pos[:, 1],\n",
    "                prey_vel[:, 0], prey_vel[:, 1],\n",
    "                scale=120,\n",
    "                width=0.01,\n",
    "                headwidth=3,\n",
    "                headlength=3,\n",
    "                headaxislength=3,\n",
    "            )\n",
    "\n",
    "            if n_pred > 0:\n",
    "                pylab.quiver(\n",
    "                    pred_pos[:, 0], pred_pos[:, 1],\n",
    "                    pred_vel[:, 0], pred_vel[:, 1],\n",
    "                    color=\"#FF0000\",\n",
    "                    scale=15,\n",
    "                    width=0.01,\n",
    "                    headwidth=3,\n",
    "                    headlength=3,\n",
    "                    headaxislength=3,\n",
    "                )\n",
    "            ax.set_aspect('equal', 'box')\n",
    "            ax.set_xlim(0, area_width)\n",
    "            ax.set_ylim(0, area_height)\n",
    "\n",
    "            plt.pause(0.00000001)\n",
    "\n",
    "        pred_states, prey_states = get_state_tensors(prey_log[t], predator_log[t], n_pred=n_pred, area_width=area_width, area_height=area_height, prey_speed=prey_speed, pred_speed=pred_speed, device=device)  # [1,agent,neigh,feat]\n",
    "\n",
    "        if n_pred > 0:\n",
    "            pred_states = pred_tensor[..., :4]\n",
    "            pred_actions = pred_policy.forward(pred_states)\n",
    "\n",
    "            agents, neigh, feat = pred_states.shape\n",
    "            pred_actions_exp = pred_actions.unsqueeze(1).expand(-1, neigh, -1)\n",
    "            step_pred_tensor = torch.cat([pred_states, pred_actions_exp], dim=0)\n",
    "            pred_tensor_list.append(step_pred_tensor)\n",
    "\n",
    "\n",
    "        prey_actions = prey_policy.forward(prey_states, deterministic=True)\n",
    "\n",
    "        agents, neigh, feat = prey_states.shape\n",
    "        prey_actions_exp = prey_actions.unsqueeze(1).expand(-1, neigh, -1)  # [A, neigh, 1]\n",
    "\n",
    "        step_prey_tensor = torch.cat([prey_states, prey_actions_exp], dim=-1)  # [A, neigh, 5]\n",
    "        prey_tensor_list.append(step_prey_tensor)\n",
    "\n",
    "        for i, agent in enumerate(prey):\n",
    "            apply_turnrate_on_theta(agent, prey_actions[i], prey_speed)\n",
    "\n",
    "        if n_pred > 0:\n",
    "            for i, predator in enumerate(pred):\n",
    "                apply_turnrate_on_theta(predator, pred_actions[i], pred_speed)\n",
    "\n",
    "        for agent in prey:\n",
    "            enforce_walls(agent, area_width, area_height)\n",
    "            agent.update_position(dt=1.0)\n",
    "\n",
    "        if n_pred > 0:\n",
    "            for predator in pred:\n",
    "                enforce_walls(predator, area_width, area_height)\n",
    "                predator.update_position(dt=1.0)\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    pred_tensor = torch.stack(pred_tensor_list, dim=0) if n_pred > 0 else 0\n",
    "    prey_tensor = torch.stack(prey_tensor_list, dim=0) \n",
    "\n",
    "    return pred_tensor, prey_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pred_tensor, gen_prey_tensor = run_env_simulation(visualization='off', prey_policy=policy, pred_policy=pred_policy, \n",
    "                                                      n_prey=32, n_pred=0, max_steps=100, \n",
    "                                                      pred_speed=5, prey_speed=5,\n",
    "                                                      area_width=50, area_height=50, device=device)\n",
    "\n",
    "expert_pred_tensor, expert_prey_tensor, logs = run_couzin_simulation(visualization=\"on\", max_steps=100, alpha=0.01, \n",
    "                                                       constant_speed=5, shark_speed=5, area_width=50, area_height=50, \n",
    "                                                       number_of_sharks=0, n=32)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
