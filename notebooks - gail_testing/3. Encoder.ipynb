{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.sim_utils import *\n",
    "import torch.nn.functional as F\n",
    "from utils.couzin_utils import *\n",
    "from torch.distributions import Normal\n",
    "from models.ModularNetworks import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfcfd1",
   "metadata": {},
   "source": [
    "### AutoEncoder for Spatial Temporal Representation Learning\n",
    "\n",
    "https://github.com/HSoo-Kim/SpatioTemporal-AutoEncoder/blob/main/Models/ST_AutoEncoder.py\n",
    "\n",
    "https://github.com/AlexanderFabisch/vtae/blob/master/trajectory_vae.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d5301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert data shape: torch.Size([500, 32, 31, 5])\n"
     ]
    }
   ],
   "source": [
    "#policy_path = rf'..\\models\\modular_policy.pt'\n",
    "#policy = torch.load(policy_path, weights_only=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "_, exp_tensor, _ = run_couzin_simulation(visualization=\"off\", max_steps=500, alpha=0.01, \n",
    "                                                       constant_speed=5, shark_speed=5, area_width=50, area_height=50, \n",
    "                                                       number_of_sharks=0, n=32)\n",
    "\n",
    "# [n_frames, agents, neigh, features]\n",
    "print(\"Expert data shape:\", exp_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8305b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, consecutive_frames=10, batch_size=10):\n",
    "    frames, agents, neigh, features = data.shape\n",
    "\n",
    "    window_list = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        start = torch.randint(0, frames - consecutive_frames + 1, (1,), device=data.device).item()\n",
    "        end = start + consecutive_frames\n",
    "        window = data[start:end]  # [T, A, N, F]\n",
    "        window_list.append(window)\n",
    "\n",
    "    stacked_windows = torch.stack(window_list, dim=0)  # [count, T, A, N, F]\n",
    "\n",
    "    return stacked_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6baffce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled expert batch shape: torch.Size([10, 10, 32, 31, 5])\n"
     ]
    }
   ],
   "source": [
    "expert_batch = sample_data(exp_tensor, consecutive_frames=10, batch_size=10)\n",
    "print(\"Sampled expert batch shape:\", expert_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "271bad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborPooling(nn.Module):\n",
    "    def __init__(self, features=4, embd_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(features, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, embd_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(features)\n",
    "\n",
    "    def forward(self, states):\n",
    "        embed = self.embed(states)\n",
    "        weights_logit = self.attention(states)\n",
    "\n",
    "        weights = torch.softmax(weights_logit, dim=2)\n",
    "        pooled = (embed * weights).sum(dim=2)\n",
    "\n",
    "        return pooled #torch.Size([100, 32, 31])\n",
    "    \n",
    "\n",
    "class AgentPooling(nn.Module):\n",
    "    def __init__(self, embd_dim=32, z=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(embd_dim, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, z)\n",
    "        )\n",
    "\n",
    "    def forward(self, pooled_embd):\n",
    "        embed = self.embed(pooled_embd)\n",
    "        return embed # torch.Size([100, 32, 1])\n",
    "\n",
    "\n",
    "class TransitionEncoder(nn.Module):\n",
    "    def __init__(self, features=4, embd_dim=32, z=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.neigh_pooling = NeighborPooling(features=features, embd_dim=embd_dim)\n",
    "        self.agent_pooling = AgentEmbedding(embd_dim=embd_dim, z=z)\n",
    "\n",
    "    def forward(self, states):\n",
    "        batch, frames, agents, neigh, features = states.shape\n",
    "        flat = states.view(batch*frames, agents, neigh, features)\n",
    "\n",
    "        pooled_embd = self.neigh_pooling(flat)\n",
    "        pooled = pooled_embd.view(batch, frames, agents, -1) \n",
    "\n",
    "        z_state = self.agent_pooling(pooled)\n",
    "        \n",
    "        z_t   = z_state[:, :-1]\n",
    "        z_tp1 = z_state[:,  1:]\n",
    "        dz = z_tp1 - z_t\n",
    "        transition_feature = torch.cat([z_t, dz], dim=-1)\n",
    "        return transition_feature\n",
    "    \n",
    "\n",
    "class TransitionDecoder(nn.Module):\n",
    "    def __init__(self, z=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, z)\n",
    "        )\n",
    "    def forward(self, z_t):\n",
    "        return self.net(z_t)\n",
    "    \n",
    "\n",
    "class TransitionAE(nn.Module):\n",
    "    def __init__(self, features=4, embd_dim=32, z=32):\n",
    "        super().__init__()\n",
    "        self.encoder = TransitionEncoder(features=features, embd_dim=embd_dim, z=z)\n",
    "        self.decoder = TransitionDecoder(z=z)\n",
    "\n",
    "    def forward(self, states):\n",
    "        dz, z_state = self.encoder(states)\n",
    "        z_t = z_state[:, :-1]\n",
    "        z_tp1 = z_state[:, 1:]\n",
    "        z_hat_tp1 = self.decoder(z_t)\n",
    "        return z_state, dz, z_hat_tp1, z_tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84a2f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=0.012030\n",
      "epoch 2: loss=0.010073\n",
      "epoch 3: loss=0.008443\n",
      "epoch 4: loss=0.007064\n",
      "epoch 5: loss=0.005863\n",
      "epoch 6: loss=0.004832\n",
      "epoch 7: loss=0.003960\n",
      "epoch 8: loss=0.003226\n",
      "epoch 9: loss=0.002629\n",
      "epoch 10: loss=0.002142\n",
      "epoch 11: loss=0.001747\n",
      "epoch 12: loss=0.001430\n",
      "epoch 13: loss=0.001176\n",
      "epoch 14: loss=0.000986\n",
      "epoch 15: loss=0.000825\n",
      "epoch 16: loss=0.000693\n",
      "epoch 17: loss=0.000593\n",
      "epoch 18: loss=0.000490\n",
      "epoch 19: loss=0.000407\n",
      "epoch 20: loss=0.000346\n",
      "epoch 21: loss=0.000292\n",
      "epoch 22: loss=0.000243\n",
      "epoch 23: loss=0.000217\n",
      "epoch 24: loss=0.000186\n",
      "epoch 25: loss=0.000171\n",
      "epoch 26: loss=0.000153\n",
      "epoch 27: loss=0.000136\n",
      "epoch 28: loss=0.000129\n",
      "epoch 29: loss=0.000118\n",
      "epoch 30: loss=0.000115\n",
      "epoch 31: loss=0.000100\n",
      "epoch 32: loss=0.000093\n",
      "epoch 33: loss=0.000087\n",
      "epoch 34: loss=0.000085\n",
      "epoch 35: loss=0.000080\n",
      "epoch 36: loss=0.000081\n",
      "epoch 37: loss=0.000080\n",
      "epoch 38: loss=0.000078\n",
      "epoch 39: loss=0.000073\n",
      "epoch 40: loss=0.000074\n",
      "epoch 41: loss=0.000068\n",
      "epoch 42: loss=0.000061\n",
      "epoch 43: loss=0.000048\n",
      "epoch 44: loss=0.000040\n",
      "epoch 45: loss=0.000034\n",
      "epoch 46: loss=0.000027\n",
      "epoch 47: loss=0.000022\n",
      "epoch 48: loss=0.000017\n",
      "epoch 49: loss=0.000013\n",
      "epoch 50: loss=0.000013\n",
      "epoch 51: loss=0.000010\n",
      "epoch 52: loss=0.000010\n",
      "epoch 53: loss=0.000009\n",
      "epoch 54: loss=0.000013\n",
      "epoch 55: loss=0.000014\n",
      "epoch 56: loss=0.000012\n",
      "epoch 57: loss=0.000014\n",
      "epoch 58: loss=0.000015\n",
      "epoch 59: loss=0.000013\n",
      "epoch 60: loss=0.000014\n",
      "epoch 61: loss=0.000016\n",
      "epoch 62: loss=0.000011\n",
      "epoch 63: loss=0.000010\n",
      "epoch 64: loss=0.000012\n",
      "epoch 65: loss=0.000009\n",
      "epoch 66: loss=0.000008\n",
      "epoch 67: loss=0.000008\n",
      "epoch 68: loss=0.000009\n",
      "epoch 69: loss=0.000007\n",
      "epoch 70: loss=0.000006\n",
      "epoch 71: loss=0.000006\n",
      "epoch 72: loss=0.000007\n",
      "epoch 73: loss=0.000006\n",
      "epoch 74: loss=0.000005\n",
      "epoch 75: loss=0.000006\n",
      "epoch 76: loss=0.000004\n",
      "epoch 77: loss=0.000007\n",
      "epoch 78: loss=0.000006\n",
      "epoch 79: loss=0.000005\n",
      "epoch 80: loss=0.000006\n",
      "epoch 81: loss=0.000005\n",
      "epoch 82: loss=0.000005\n",
      "epoch 83: loss=0.000005\n",
      "epoch 84: loss=0.000005\n",
      "epoch 85: loss=0.000005\n",
      "epoch 86: loss=0.000004\n",
      "epoch 87: loss=0.000004\n",
      "epoch 88: loss=0.000004\n",
      "epoch 89: loss=0.000004\n",
      "epoch 90: loss=0.000004\n",
      "epoch 91: loss=0.000004\n",
      "epoch 92: loss=0.000003\n",
      "epoch 93: loss=0.000005\n",
      "epoch 94: loss=0.000004\n",
      "epoch 95: loss=0.000005\n",
      "epoch 96: loss=0.000004\n",
      "epoch 97: loss=0.000005\n",
      "epoch 98: loss=0.000004\n",
      "epoch 99: loss=0.000004\n",
      "epoch 100: loss=0.000004\n",
      "epoch 101: loss=0.000004\n",
      "epoch 102: loss=0.000004\n",
      "epoch 103: loss=0.000004\n",
      "epoch 104: loss=0.000003\n",
      "epoch 105: loss=0.000004\n",
      "epoch 106: loss=0.000004\n",
      "epoch 107: loss=0.000004\n",
      "epoch 108: loss=0.000003\n",
      "epoch 109: loss=0.000003\n",
      "epoch 110: loss=0.000004\n",
      "epoch 111: loss=0.000004\n",
      "epoch 112: loss=0.000004\n",
      "epoch 113: loss=0.000003\n",
      "epoch 114: loss=0.000003\n",
      "epoch 115: loss=0.000003\n",
      "epoch 116: loss=0.000004\n",
      "epoch 117: loss=0.000004\n",
      "epoch 118: loss=0.000003\n",
      "epoch 119: loss=0.000003\n",
      "epoch 120: loss=0.000004\n",
      "epoch 121: loss=0.000004\n",
      "epoch 122: loss=0.000002\n",
      "epoch 123: loss=0.000003\n",
      "epoch 124: loss=0.000003\n",
      "epoch 125: loss=0.000003\n",
      "epoch 126: loss=0.000003\n",
      "epoch 127: loss=0.000002\n",
      "epoch 128: loss=0.000003\n",
      "epoch 129: loss=0.000002\n",
      "epoch 130: loss=0.000003\n",
      "epoch 131: loss=0.000002\n",
      "epoch 132: loss=0.000002\n",
      "epoch 133: loss=0.000002\n",
      "epoch 134: loss=0.000002\n",
      "epoch 135: loss=0.000002\n",
      "epoch 136: loss=0.000003\n",
      "epoch 137: loss=0.000002\n",
      "epoch 138: loss=0.000002\n",
      "epoch 139: loss=0.000003\n",
      "epoch 140: loss=0.000003\n",
      "epoch 141: loss=0.000002\n",
      "epoch 142: loss=0.000003\n",
      "epoch 143: loss=0.000002\n",
      "epoch 144: loss=0.000002\n",
      "epoch 145: loss=0.000002\n",
      "epoch 146: loss=0.000003\n",
      "epoch 147: loss=0.000002\n",
      "epoch 148: loss=0.000002\n",
      "epoch 149: loss=0.000002\n",
      "epoch 150: loss=0.000002\n",
      "epoch 151: loss=0.000002\n",
      "epoch 152: loss=0.000002\n",
      "epoch 153: loss=0.000003\n",
      "epoch 154: loss=0.000001\n",
      "epoch 155: loss=0.000003\n",
      "epoch 156: loss=0.000002\n",
      "epoch 157: loss=0.000002\n",
      "epoch 158: loss=0.000002\n",
      "epoch 159: loss=0.000002\n",
      "epoch 160: loss=0.000002\n",
      "epoch 161: loss=0.000002\n",
      "epoch 162: loss=0.000002\n",
      "epoch 163: loss=0.000001\n",
      "epoch 164: loss=0.000002\n",
      "epoch 165: loss=0.000002\n",
      "epoch 166: loss=0.000002\n",
      "epoch 167: loss=0.000002\n",
      "epoch 168: loss=0.000002\n",
      "epoch 169: loss=0.000002\n",
      "epoch 170: loss=0.000002\n",
      "epoch 171: loss=0.000002\n",
      "epoch 172: loss=0.000001\n",
      "epoch 173: loss=0.000002\n",
      "epoch 174: loss=0.000002\n",
      "epoch 175: loss=0.000002\n",
      "epoch 176: loss=0.000002\n",
      "epoch 177: loss=0.000002\n",
      "epoch 178: loss=0.000002\n",
      "epoch 179: loss=0.000001\n",
      "epoch 180: loss=0.000002\n",
      "epoch 181: loss=0.000002\n",
      "epoch 182: loss=0.000002\n",
      "epoch 183: loss=0.000002\n",
      "epoch 184: loss=0.000001\n",
      "epoch 185: loss=0.000001\n",
      "epoch 186: loss=0.000002\n",
      "epoch 187: loss=0.000002\n",
      "epoch 188: loss=0.000001\n",
      "epoch 189: loss=0.000002\n",
      "epoch 190: loss=0.000002\n",
      "epoch 191: loss=0.000002\n",
      "epoch 192: loss=0.000002\n",
      "epoch 193: loss=0.000001\n",
      "epoch 194: loss=0.000002\n",
      "epoch 195: loss=0.000002\n",
      "epoch 196: loss=0.000002\n",
      "epoch 197: loss=0.000002\n",
      "epoch 198: loss=0.000001\n",
      "epoch 199: loss=0.000001\n",
      "epoch 200: loss=0.000001\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransitionAE(features=4, embd_dim=32, z=32).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "_, exp_tensor, _ = run_couzin_simulation(visualization=\"off\", max_steps=500, alpha=0.01, \n",
    "                                                       constant_speed=5, shark_speed=5, area_width=50, area_height=50, \n",
    "                                                       number_of_sharks=0, n=32)\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "\n",
    "    expert_batch = sample_data(exp_tensor, consecutive_frames=10, batch_size=10)\n",
    "    states = expert_batch[..., :4].to(device)  # [B,T,A,N,4]\n",
    "    _, _, z_hat_tp1, z_tp1 = model(states)\n",
    "\n",
    "    loss = F.mse_loss(z_hat_tp1, z_tp1)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"epoch {epoch}: loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937c9e3",
   "metadata": {},
   "source": [
    "### Encoder-Only mit VicReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9dcf0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborPooling(nn.Module):\n",
    "    def __init__(self, features=4, embd_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = features * 2\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, embd_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(in_dim)\n",
    "\n",
    "    def forward(self, states, neigh_mask=None, feat_mask=None):\n",
    "        if feat_mask is None:\n",
    "            feat_mask = torch.ones_like(states)\n",
    "\n",
    "        cat_states = torch.cat([states, feat_mask], dim=-1)\n",
    "\n",
    "        embed = self.embed(cat_states)\n",
    "        weights_logit = self.attention(cat_states)\n",
    "\n",
    "        if neigh_mask is not None:\n",
    "            weights_logit = weights_logit.masked_fill(neigh_mask == 0, float(\"-inf\"))\n",
    "\n",
    "\n",
    "        weights = torch.softmax(weights_logit, dim=2)\n",
    "        pooled = (embed * weights).sum(dim=2)\n",
    "\n",
    "        return pooled\n",
    "    \n",
    "\n",
    "class AgentEmbedding(nn.Module):\n",
    "    def __init__(self, embd_dim=32, z=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(embd_dim, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, z)\n",
    "        )\n",
    "\n",
    "    def forward(self, pooled_embd):\n",
    "        embed = self.embed(pooled_embd)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class TransitionEncoder(nn.Module):\n",
    "    def __init__(self, features=4, embd_dim=32, z=32):\n",
    "        super().__init__()\n",
    "        self.z = z\n",
    "        self.neigh_pooling = NeighborPooling(features=features, embd_dim=embd_dim)\n",
    "        self.agent_pooling = AgentEmbedding(embd_dim=embd_dim, z=z)\n",
    "\n",
    "    def forward(self, states, neigh_mask=None, feat_mask=None):\n",
    "        batch, frames, agents, neigh, features = states.shape\n",
    "        flat = states.reshape(batch * frames, agents, neigh, features)\n",
    "\n",
    "        if neigh_mask is not None:\n",
    "            neigh_mask = neigh_mask.reshape(batch * frames, agents, neigh, 1)\n",
    "\n",
    "        if feat_mask is not None:\n",
    "            feat_mask = feat_mask.expand(batch, frames, agents, neigh, features)\n",
    "            feat_mask = feat_mask.reshape(batch * frames, agents, neigh, features) \n",
    "\n",
    "        pooled_embd = self.neigh_pooling(flat, neigh_mask=neigh_mask, feat_mask=feat_mask)\n",
    "        pooled = pooled_embd.view(batch, frames, agents, -1)\n",
    "\n",
    "        z_state = self.agent_pooling(pooled)\n",
    "        \n",
    "        z_t   = z_state[:, :-1]\n",
    "        z_tp1 = z_state[:,  1:]\n",
    "        dz = z_tp1 - z_t\n",
    "        transition_feature = torch.cat([z_t, dz], dim=-1)\n",
    "        return z_state, transition_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "815bc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryAugmentation(nn.Module):\n",
    "    def __init__(self, noise_std=0.01, neigh_drop=0.10, feat_drop=0.05):\n",
    "        super().__init__()\n",
    "        self.noise_std = noise_std\n",
    "        self.neigh_drop = neigh_drop\n",
    "        self.feat_drop = feat_drop\n",
    "\n",
    "    def forward(self, states):\n",
    "        states = states.clone()\n",
    "        batch, frames, agents, neigh, features = states.shape\n",
    "        device = states.device\n",
    "\n",
    "        if self.noise_std > 0:\n",
    "            states = states + torch.randn_like(states) * self.noise_std\n",
    "\n",
    "        neigh_mask = torch.ones((batch, frames, agents, neigh, 1), device=device, dtype=states.dtype)\n",
    "        feat_mask  = torch.ones((batch, frames, agents, neigh, features), device=device, dtype=states.dtype)\n",
    "\n",
    "        if self.neigh_drop > 0:\n",
    "            neigh_mask = (torch.rand(batch, frames, agents, neigh, 1, device=device) > self.neigh_drop).float()\n",
    "\n",
    "        if self.feat_drop > 0:\n",
    "            feat_mask = (torch.rand(batch, frames, agents, 1, features, device=device) > self.feat_drop).float()\n",
    "\n",
    "        return states, neigh_mask, feat_mask\n",
    "\n",
    "\n",
    "class VicRegProjector(nn.Module):\n",
    "    def __init__(self, input_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128))\n",
    "        \n",
    "    def forward(self, states):\n",
    "        return self.net(states)\n",
    "\n",
    "\n",
    "# https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
    "def off_diagonal(tensor):\n",
    "    dim = tensor.size(0)\n",
    "    mask = ~torch.eye(dim, dtype=torch.bool, device=tensor.device)\n",
    "    return tensor[mask]\n",
    "\n",
    "def vicreg_loss(z1, z2, sim_coeff=25.0, std_coeff=15.0, cov_coeff=5.0, eps=1e-4):\n",
    "\n",
    "    sim_loss = torch.mean((z1 - z2) ** 2)\n",
    "\n",
    "    z1 = z1 - z1.mean(dim=0)\n",
    "    z2 = z2 - z2.mean(dim=0)\n",
    "\n",
    "    std_z1 = torch.sqrt(z1.var(dim=0) + eps)\n",
    "    std_z2 = torch.sqrt(z2.var(dim=0) + eps)\n",
    "    std_loss = torch.mean(F.relu(1.0 - std_z1)) + torch.mean(F.relu(1.0 - std_z2))\n",
    "\n",
    "    batch, dim = z1.shape\n",
    "    cov_z1 = (z1.T @ z1) / (batch - 1)\n",
    "    cov_z2 = (z2.T @ z2) / (batch - 1)\n",
    "    cov_loss = (off_diagonal(cov_z1).pow(2).sum() / dim) + (off_diagonal(cov_z2).pow(2).sum() / dim)\n",
    "\n",
    "    loss = sim_coeff * sim_loss + std_coeff * std_loss + cov_coeff * cov_loss\n",
    "    logs = {\n",
    "        \"sim\": sim_loss.detach(),\n",
    "        \"std\": std_loss.detach(),\n",
    "        \"cov\": cov_loss.detach(),\n",
    "        \"std_mean\": 0.5 * (std_z1.mean().detach() + std_z2.mean().detach()),\n",
    "    }\n",
    "    return loss, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2bbaa11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010: loss=23.844624 sim=0.0497 std=1.4030 cov=0.3113 std_mean=0.299\n",
      "epoch 020: loss=22.559509 sim=0.0338 std=1.3162 cov=0.3945 std_mean=0.342\n",
      "epoch 030: loss=21.826788 sim=0.0222 std=1.2564 cov=0.4852 std_mean=0.372\n",
      "epoch 040: loss=21.201599 sim=0.0201 std=1.2302 cov=0.4494 std_mean=0.385\n",
      "epoch 050: loss=20.696140 sim=0.0195 std=1.1682 cov=0.5372 std_mean=0.416\n",
      "epoch 060: loss=20.465712 sim=0.0198 std=1.1382 cov=0.5794 std_mean=0.431\n",
      "epoch 070: loss=20.216318 sim=0.0201 std=1.1264 cov=0.5634 std_mean=0.437\n",
      "epoch 080: loss=20.152771 sim=0.0224 std=1.1000 cov=0.6187 std_mean=0.450\n",
      "epoch 090: loss=20.098295 sim=0.0233 std=1.0927 cov=0.6250 std_mean=0.454\n",
      "epoch 100: loss=20.078236 sim=0.0257 std=1.0913 cov=0.6134 std_mean=0.454\n",
      "epoch 110: loss=19.930140 sim=0.0245 std=1.0828 cov=0.6152 std_mean=0.459\n",
      "epoch 120: loss=19.780333 sim=0.0243 std=1.0738 cov=0.6131 std_mean=0.463\n",
      "epoch 130: loss=19.700085 sim=0.0237 std=1.0608 cov=0.6393 std_mean=0.470\n",
      "epoch 140: loss=19.537148 sim=0.0227 std=1.0599 cov=0.6141 std_mean=0.470\n",
      "epoch 150: loss=19.352573 sim=0.0220 std=1.0499 cov=0.6108 std_mean=0.475\n",
      "epoch 160: loss=19.038456 sim=0.0217 std=1.0336 cov=0.5981 std_mean=0.483\n",
      "epoch 170: loss=18.730856 sim=0.0219 std=1.0098 cov=0.6072 std_mean=0.495\n",
      "epoch 180: loss=18.413979 sim=0.0229 std=0.9857 cov=0.6109 std_mean=0.507\n",
      "epoch 190: loss=18.121506 sim=0.0235 std=0.9546 cov=0.6433 std_mean=0.523\n",
      "epoch 200: loss=17.912586 sim=0.0251 std=0.9305 cov=0.6656 std_mean=0.535\n",
      "epoch 210: loss=17.699299 sim=0.0256 std=0.8980 cov=0.7179 std_mean=0.551\n",
      "epoch 220: loss=17.600685 sim=0.0261 std=0.8784 cov=0.7547 std_mean=0.561\n",
      "epoch 230: loss=17.498531 sim=0.0273 std=0.8851 cov=0.7081 std_mean=0.557\n",
      "epoch 240: loss=17.441719 sim=0.0302 std=0.8526 cov=0.7793 std_mean=0.574\n",
      "epoch 250: loss=17.404757 sim=0.0298 std=0.8635 cov=0.7413 std_mean=0.568\n",
      "epoch 260: loss=17.456409 sim=0.0311 std=0.8540 cov=0.7739 std_mean=0.573\n",
      "epoch 270: loss=17.250799 sim=0.0293 std=0.8505 cov=0.7519 std_mean=0.575\n",
      "epoch 280: loss=17.222240 sim=0.0299 std=0.8458 cov=0.7572 std_mean=0.577\n",
      "epoch 290: loss=17.183056 sim=0.0309 std=0.8361 cov=0.7739 std_mean=0.582\n",
      "epoch 300: loss=17.078321 sim=0.0286 std=0.8433 cov=0.7427 std_mean=0.578\n",
      "epoch 310: loss=17.010422 sim=0.0274 std=0.8459 cov=0.7272 std_mean=0.577\n",
      "epoch 320: loss=16.988768 sim=0.0288 std=0.8335 cov=0.7532 std_mean=0.583\n",
      "epoch 330: loss=16.792318 sim=0.0294 std=0.8262 cov=0.7332 std_mean=0.587\n",
      "epoch 340: loss=16.727617 sim=0.0286 std=0.8193 cov=0.7446 std_mean=0.590\n",
      "epoch 350: loss=16.566355 sim=0.0299 std=0.8068 cov=0.7431 std_mean=0.597\n",
      "epoch 360: loss=16.465771 sim=0.0284 std=0.7918 cov=0.7758 std_mean=0.604\n",
      "epoch 370: loss=16.335209 sim=0.0302 std=0.7802 cov=0.7753 std_mean=0.610\n",
      "epoch 380: loss=16.261612 sim=0.0297 std=0.7724 cov=0.7869 std_mean=0.614\n",
      "epoch 390: loss=16.214344 sim=0.0314 std=0.7693 cov=0.7779 std_mean=0.615\n",
      "epoch 400: loss=16.092384 sim=0.0301 std=0.7552 cov=0.8022 std_mean=0.622\n",
      "epoch 410: loss=16.161423 sim=0.0324 std=0.7521 cov=0.8141 std_mean=0.624\n",
      "epoch 420: loss=15.927761 sim=0.0313 std=0.7412 cov=0.8053 std_mean=0.629\n",
      "epoch 430: loss=15.847834 sim=0.0330 std=0.7279 cov=0.8208 std_mean=0.636\n",
      "epoch 440: loss=15.781231 sim=0.0303 std=0.7258 cov=0.8276 std_mean=0.637\n",
      "epoch 450: loss=15.849554 sim=0.0319 std=0.7173 cov=0.8583 std_mean=0.641\n",
      "epoch 460: loss=15.730280 sim=0.0333 std=0.7174 cov=0.8271 std_mean=0.641\n",
      "epoch 470: loss=15.614438 sim=0.0325 std=0.7091 cov=0.8330 std_mean=0.645\n",
      "epoch 480: loss=15.566689 sim=0.0321 std=0.7056 cov=0.8362 std_mean=0.647\n",
      "epoch 490: loss=15.361557 sim=0.0323 std=0.6960 cov=0.8228 std_mean=0.652\n",
      "epoch 500: loss=15.252491 sim=0.0325 std=0.6881 cov=0.8236 std_mean=0.656\n",
      "epoch 510: loss=15.280657 sim=0.0332 std=0.6824 cov=0.8426 std_mean=0.659\n",
      "epoch 520: loss=15.153704 sim=0.0359 std=0.6740 cov=0.8291 std_mean=0.663\n",
      "epoch 530: loss=15.170187 sim=0.0344 std=0.6657 cov=0.8648 std_mean=0.667\n",
      "epoch 540: loss=15.060741 sim=0.0374 std=0.6629 cov=0.8364 std_mean=0.669\n",
      "epoch 550: loss=15.138021 sim=0.0374 std=0.6530 cov=0.8817 std_mean=0.673\n",
      "epoch 560: loss=14.756380 sim=0.0361 std=0.6369 cov=0.8602 std_mean=0.682\n",
      "epoch 570: loss=14.662993 sim=0.0374 std=0.6236 cov=0.8749 std_mean=0.688\n",
      "epoch 580: loss=14.641769 sim=0.0376 std=0.6199 cov=0.8806 std_mean=0.690\n",
      "epoch 590: loss=14.735660 sim=0.0382 std=0.6188 cov=0.9000 std_mean=0.691\n",
      "epoch 600: loss=14.503504 sim=0.0382 std=0.6018 cov=0.9041 std_mean=0.699\n",
      "epoch 610: loss=14.509369 sim=0.0400 std=0.6016 cov=0.8972 std_mean=0.699\n",
      "epoch 620: loss=14.361300 sim=0.0392 std=0.5925 cov=0.8987 std_mean=0.704\n",
      "epoch 630: loss=14.469660 sim=0.0393 std=0.5898 cov=0.9278 std_mean=0.705\n",
      "epoch 640: loss=14.347054 sim=0.0419 std=0.5842 cov=0.9072 std_mean=0.708\n",
      "epoch 650: loss=14.113562 sim=0.0392 std=0.5750 cov=0.9018 std_mean=0.713\n",
      "epoch 660: loss=14.147076 sim=0.0433 std=0.5665 cov=0.9133 std_mean=0.717\n",
      "epoch 670: loss=14.016450 sim=0.0405 std=0.5641 cov=0.9087 std_mean=0.718\n",
      "epoch 680: loss=14.022246 sim=0.0415 std=0.5591 cov=0.9199 std_mean=0.720\n",
      "epoch 690: loss=13.976172 sim=0.0430 std=0.5535 cov=0.9200 std_mean=0.723\n",
      "epoch 700: loss=13.788071 sim=0.0414 std=0.5488 cov=0.9045 std_mean=0.726\n",
      "epoch 710: loss=13.700178 sim=0.0409 std=0.5432 cov=0.9060 std_mean=0.728\n",
      "epoch 720: loss=13.660921 sim=0.0457 std=0.5256 cov=0.9268 std_mean=0.737\n",
      "epoch 730: loss=13.583941 sim=0.0450 std=0.5261 cov=0.9136 std_mean=0.737\n",
      "epoch 740: loss=13.566929 sim=0.0464 std=0.5276 cov=0.8986 std_mean=0.736\n",
      "epoch 750: loss=13.351912 sim=0.0459 std=0.5046 cov=0.9271 std_mean=0.748\n",
      "epoch 760: loss=13.351612 sim=0.0461 std=0.5075 cov=0.9175 std_mean=0.746\n",
      "epoch 770: loss=13.342572 sim=0.0456 std=0.4972 cov=0.9492 std_mean=0.751\n",
      "epoch 780: loss=13.178892 sim=0.0457 std=0.4906 cov=0.9353 std_mean=0.755\n",
      "epoch 790: loss=13.163533 sim=0.0459 std=0.4830 cov=0.9541 std_mean=0.758\n",
      "epoch 800: loss=13.172575 sim=0.0476 std=0.4788 cov=0.9604 std_mean=0.761\n",
      "epoch 810: loss=13.165969 sim=0.0474 std=0.4758 cov=0.9690 std_mean=0.762\n",
      "epoch 820: loss=12.930227 sim=0.0478 std=0.4733 cov=0.9270 std_mean=0.764\n",
      "epoch 830: loss=13.065631 sim=0.0487 std=0.4597 cov=0.9906 std_mean=0.770\n",
      "epoch 840: loss=12.917300 sim=0.0483 std=0.4647 cov=0.9479 std_mean=0.768\n",
      "epoch 850: loss=12.807856 sim=0.0483 std=0.4566 cov=0.9501 std_mean=0.772\n",
      "epoch 860: loss=12.868668 sim=0.0489 std=0.4549 cov=0.9644 std_mean=0.773\n",
      "epoch 870: loss=12.774642 sim=0.0474 std=0.4482 cov=0.9733 std_mean=0.776\n",
      "epoch 880: loss=12.838444 sim=0.0511 std=0.4445 cov=0.9788 std_mean=0.778\n",
      "epoch 890: loss=12.669312 sim=0.0495 std=0.4381 cov=0.9719 std_mean=0.781\n",
      "epoch 900: loss=12.674162 sim=0.0493 std=0.4341 cov=0.9862 std_mean=0.783\n",
      "epoch 910: loss=12.482916 sim=0.0517 std=0.4264 cov=0.9588 std_mean=0.787\n",
      "epoch 920: loss=12.509377 sim=0.0527 std=0.4196 cov=0.9794 std_mean=0.790\n",
      "epoch 930: loss=12.439927 sim=0.0512 std=0.4216 cov=0.9671 std_mean=0.789\n",
      "epoch 940: loss=12.373508 sim=0.0502 std=0.4091 cov=0.9965 std_mean=0.796\n",
      "epoch 950: loss=12.363729 sim=0.0546 std=0.4075 cov=0.9771 std_mean=0.796\n",
      "epoch 960: loss=12.214876 sim=0.0512 std=0.4043 cov=0.9741 std_mean=0.798\n",
      "epoch 970: loss=12.176714 sim=0.0522 std=0.3931 cov=0.9954 std_mean=0.804\n",
      "epoch 980: loss=12.168339 sim=0.0534 std=0.3929 cov=0.9879 std_mean=0.804\n",
      "epoch 990: loss=12.009665 sim=0.0549 std=0.3865 cov=0.9678 std_mean=0.807\n",
      "epoch 1000: loss=11.954208 sim=0.0553 std=0.3801 cov=0.9739 std_mean=0.810\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "aug_states = TrajectoryAugmentation(noise_std=0.01, neigh_drop=0.10, feat_drop=0.05).to(device)\n",
    "encoder = TransitionEncoder(features=4, embd_dim=32, z=32).to(device)\n",
    "projector = VicRegProjector(input_dim=64).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(list(encoder.parameters()) + list(projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "_, exp_tensor, _ = run_couzin_simulation(visualization=\"off\", max_steps=500, alpha=0.01, \n",
    "                                         constant_speed=5, shark_speed=5, \n",
    "                                         area_width=50, area_height=50,\n",
    "                                         number_of_sharks=0, n=32)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    encoder.train()\n",
    "    projector.train()\n",
    "\n",
    "    expert_batch = sample_data(exp_tensor, consecutive_frames=10, batch_size=64)\n",
    "    states = expert_batch[..., :4].to(device)  # [B,T,A,N,4]\n",
    "\n",
    "    x1, neigh_mask1, feat_mask1 = aug_states(states)\n",
    "    x2, neigh_mask2, feat_mask2 = aug_states(states)\n",
    "\n",
    "    z_state1, trans1 = encoder(x1, neigh_mask=neigh_mask1, feat_mask=feat_mask1)\n",
    "    z_state2, trans2 = encoder(x2, neigh_mask=neigh_mask2, feat_mask=feat_mask2)\n",
    "    \n",
    "    r1 = trans1.reshape(-1, trans1.size(-1))  # [-1, 64]\n",
    "    r2 = trans2.reshape(-1, trans2.size(-1))  # [-1, 64]\n",
    "\n",
    "    y1 = projector(r1)\n",
    "    y2 = projector(r2)\n",
    "\n",
    "    loss, logs = vicreg_loss(y1, y2)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(projector.parameters()), 1.0)\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch:03d}: loss={loss.item():.6f} \"\n",
    "              f\"sim={logs['sim'].item():.4f} std={logs['std'].item():.4f} \"\n",
    "              f\"cov={logs['cov'].item():.4f} std_mean={logs['std_mean'].item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f026c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "aug = TrajectoryAugmentation(noise_std=0.01, neigh_drop=0.10, feat_drop=0.05).to(device)\n",
    "encoder = TransitionEncoder(features=4, embd_dim=32, z=32).to(device)\n",
    "projector = VicRegProjector(input_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "_, exp_tensor, _ = run_couzin_simulation(visualization=\"off\", max_steps=500, alpha=0.01, \n",
    "                                         constant_speed=5, shark_speed=5, \n",
    "                                         area_width=50, area_height=50,\n",
    "                                         number_of_sharks=0, n=32)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "def train_encoder(encoder, projector, aug, exp_tensor, epochs, optimizer, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        encoder.train()\n",
    "        projector.train()\n",
    "\n",
    "        expert_batch = sample_data(exp_tensor, consecutive_frames=10, batch_size=64)\n",
    "        states = expert_batch[..., :4].to(device)  # [B,T,A,N,4]\n",
    "\n",
    "        x1, neigh_mask1, feat_mask1 = aug_states(states)\n",
    "        x2, neigh_mask2, feat_mask2 = aug_states(states)\n",
    "\n",
    "        z_state1, trans1 = encoder(x1, neigh_mask=neigh_mask1, feat_mask=feat_mask1)\n",
    "        z_state2, trans2 = encoder(x2, neigh_mask=neigh_mask2, feat_mask=feat_mask2)\n",
    "        \n",
    "        r1 = trans1.reshape(-1, trans1.size(-1))  # [-1, 64]\n",
    "        r2 = trans2.reshape(-1, trans2.size(-1))  # [-1, 64]\n",
    "\n",
    "        y1 = projector(r1)\n",
    "        y2 = projector(r2)\n",
    "\n",
    "        loss, logs = vicreg_loss(y1, y2)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(projector.parameters()), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch {epoch:03d}: loss={loss.item():.6f} \"\n",
    "                f\"sim={logs['sim'].item():.4f} std={logs['std'].item():.4f} \"\n",
    "                f\"cov={logs['cov'].item():.4f} std_mean={logs['std_mean'].item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "652a285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled expert batch shape: torch.Size([64, 10, 32, 31, 5])\n",
      "States shape: torch.Size([64, 10, 32, 31, 4])\n"
     ]
    }
   ],
   "source": [
    "expert_batch = sample_data(exp_tensor, consecutive_frames=10, batch_size=64)\n",
    "print(\"Sampled expert batch shape:\", expert_batch.shape)\n",
    "states = expert_batch[..., :4].to(device)\n",
    "print(\"States shape:\", states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ee62fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, r'..\\models\\encoder.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
