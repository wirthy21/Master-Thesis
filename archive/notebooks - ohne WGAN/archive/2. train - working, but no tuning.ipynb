{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from utils.OpenAI_ES import *\n",
    "from utils.env_utils import *\n",
    "from utils.train_utils import *\n",
    "from models.Buffer import Buffer\n",
    "from marl_aquarium.aquarium_v0 import parallel_env\n",
    "from models.Generator import GeneratorPolicy\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272a92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nToDo's\\n- Schwarmmetriken berechnen\\n- CUDA-Toolkit installieren, damit Modell auf GPU\\n\\nFalls Training keine Fortschritte:\\n    - Drehwinkel auf 30% beschr채nken 15% links 15% rechts\\n    - Nur nearest Neighbors verwenden\\n    - Struktur Netzwerk anpassen (Layer, Batch Normalization, Dropout)\\n    - PPO statt ES\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo's\n",
    "- Schwarmmetriken berechnen\n",
    "- CUDA-Toolkit installieren, damit Modell auf GPU\n",
    "\n",
    "Falls Training keine Fortschritte:\n",
    "    - Drehwinkel auf 30% beschr채nken 15% links 15% rechts\n",
    "    - Nur nearest Neighbors verwenden\n",
    "    - Struktur Netzwerk anpassen (Layer, Batch Normalization, Dropout)\n",
    "    - PPO statt ES\n",
    "    - Policy-Reward-Normalisierung 체ber die Zeit (manchmal in Literatur empfohlen)\n",
    "    - Clipping des Rewards durch Huber-Klut\n",
    "    - Zu Beginn des Trainings Policies mit Expertendaten trainieren! - richtig gute Idee\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#Environment\n",
    "pred_count = 1\n",
    "prey_count = 32 \n",
    "action_count = 360\n",
    "num_frames = 9\n",
    "inital_gen_episodes = 38 # 38 len(expert_data)\n",
    "num_generative_episodes = 5\n",
    "\n",
    "# Buffer\n",
    "buffer_size = 150\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "num_generations = 80 #convergence in Wu after 80\n",
    "patience = 20\n",
    "gen_dis_ratio = 5 # 1:5\n",
    "\n",
    "# ES-Pertrubation\n",
    "num_perturbations = 32 #32 f체r 8 CPU-Cores #30 in Wu paper\n",
    "sigma = 0.1\n",
    "gamma = 0.9997\n",
    "lr_pred_policy = 0.01\n",
    "lr_prey_policy = 0.05 #high to balance training between predator and prey policy\n",
    "\n",
    "# RMSprop\n",
    "lr_pred_dis =  0.0005\n",
    "lr_prey_dis = 0.0005\n",
    "alpha=0.99\n",
    "eps=1e-08\n",
    "lambda_gp_pred = 10\n",
    "lambda_gp_prey = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef35ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Folder\n",
    "path = r\"..\\data\\training\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%d.%m.%Y_%H.%M\")\n",
    "folder_name = f\"Training - {timestamp}\"\n",
    "\n",
    "save_dir = os.path.join(path, folder_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "data_path = r\"..\\data\\processed\\video_8min\\expert_tensors\"\n",
    "buffer_path = r\"..\\data\\buffer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7593b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Buffer is empty, load data...\n",
      "Storage of Expert Buffer:  38 \n",
      "\n",
      "Generative Buffer is empty, generating data...\n",
      "Storage of Generative Buffer:  38\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pred_policy = GeneratorPolicy().to(device)\n",
    "pred_policy.set_parameters(init=True)\n",
    "optim_policy_pred = torch.optim.Adam(pred_policy.parameters(), lr=lr_pred_policy)\n",
    "\n",
    "prey_policy = GeneratorPolicy().to(device)\n",
    "prey_policy.set_parameters(init=True)\n",
    "optim_policy_prey = torch.optim.Adam(prey_policy.parameters(), lr=lr_prey_policy)\n",
    "\n",
    "pred_discriminator = Discriminator().to(device)\n",
    "pred_discriminator.set_parameters(init=True)\n",
    "optim_dis_pred = torch.optim.RMSprop(pred_discriminator.parameters(), lr=lr_pred_dis, alpha=alpha, eps=eps)\n",
    "\n",
    "prey_discriminator = Discriminator().to(device)\n",
    "prey_discriminator.set_parameters(init=True)\n",
    "optim_dis_prey = torch.optim.RMSprop(prey_discriminator.parameters(), lr=lr_prey_dis, alpha=alpha, eps=eps)\n",
    "\n",
    "env = parallel_env(predator_count=pred_count, prey_count=prey_count, action_count=360)\n",
    "\n",
    "expert_buffer = Buffer(clip_length=num_frames, max_length=buffer_size, device=device)\n",
    "generative_buffer = Buffer(clip_length=num_frames, max_length=buffer_size, device=device)\n",
    "\n",
    "early_stopper_pred = EarlyStopping(patience=patience)\n",
    "early_stopper_prey = EarlyStopping(patience=patience)\n",
    "\n",
    "#expert_buffer.load(buffer_path, type=\"expert\", device=device)\n",
    "#generative_buffer.load(buffer_path, type=\"generative\", device=device)\n",
    "\n",
    "if len(expert_buffer) == 0:\n",
    "    print(\"Expert Buffer is empty, load data...\")\n",
    "    expert_buffer.add_expert(data_path, detections=33)\n",
    "\n",
    "    print(\"Storage of Expert Buffer: \", len(expert_buffer), \"\\n\")\n",
    "\n",
    "if len(generative_buffer) == 0:\n",
    "    print(\"Generative Buffer is empty, generating data...\")\n",
    "    generate_trajectories(buffer=generative_buffer, \n",
    "                          pred_count=pred_count, prey_count=prey_count, action_count=action_count, \n",
    "                          pred_policy=pred_policy, prey_policy=prey_policy, \n",
    "                          num_frames=num_frames, num_generative_episodes=inital_gen_episodes)\n",
    "    \n",
    "    print(\"Storage of Generative Buffer: \", len(generative_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53670d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 1/150] - Time: 13:02 - Estimated Finish: 2025-07-13 03:57:47\n",
      "Predator | Avg. ES-Reward: -0.0019 | DisReward (Fitness): 0.0910 | DisLoss: 5.1951 | LR: 0.0010 | Sigma: 0.9900\n",
      "Prey     | Avg. ES-Reward: -0.0021 | DisReward (Fitness): 0.1422 | DisLoss: 38.9255 | LR: 0.0005 | Sigma: 0.9900 \n",
      "\n",
      "Checkpoint successfully saved! \n",
      " \n",
      "[Generation 2/150] - Time: 06:35 - Estimated Finish: 2025-07-12 12:02:44\n",
      "Predator | Avg. ES-Reward: -0.0033 | DisReward (Fitness): 0.0894 | DisLoss: 4.8232 | LR: 0.0010 | Sigma: 0.9801\n",
      "Prey     | Avg. ES-Reward: 0.0007 | DisReward (Fitness): 0.1452 | DisLoss: 32.1730 | LR: 0.0005 | Sigma: 0.9801 \n",
      "\n",
      "[Generation 3/150] - Time: 04:13 - Estimated Finish: 2025-07-12 06:21:35\n",
      "Predator | Avg. ES-Reward: 0.0068 | DisReward (Fitness): 0.0873 | DisLoss: 4.5734 | LR: 0.0010 | Sigma: 0.9703\n",
      "Prey     | Avg. ES-Reward: 0.0030 | DisReward (Fitness): 0.1483 | DisLoss: 28.4427 | LR: 0.0005 | Sigma: 0.9703 \n",
      "\n",
      "[Generation 4/150] - Time: 03:12 - Estimated Finish: 2025-07-12 04:01:12\n",
      "Predator | Avg. ES-Reward: -0.0019 | DisReward (Fitness): 0.0874 | DisLoss: 4.3884 | LR: 0.0010 | Sigma: 0.9606\n",
      "Prey     | Avg. ES-Reward: 0.0023 | DisReward (Fitness): 0.1509 | DisLoss: 22.1523 | LR: 0.0005 | Sigma: 0.9606 \n",
      "\n",
      "[Generation 5/150] - Time: 02:36 - Estimated Finish: 2025-07-12 02:45:24\n",
      "Predator | Avg. ES-Reward: 0.0040 | DisReward (Fitness): 0.0855 | DisLoss: 3.8725 | LR: 0.0010 | Sigma: 0.9510\n",
      "Prey     | Avg. ES-Reward: 0.0012 | DisReward (Fitness): 0.1531 | DisLoss: 18.8311 | LR: 0.0005 | Sigma: 0.9510 \n",
      "\n",
      "[Generation 6/150] - Time: 02:10 - Estimated Finish: 2025-07-12 01:53:06\n",
      "Predator | Avg. ES-Reward: -0.0027 | DisReward (Fitness): 0.0853 | DisLoss: 3.6824 | LR: 0.0009 | Sigma: 0.9415\n",
      "Prey     | Avg. ES-Reward: -0.0006 | DisReward (Fitness): 0.1555 | DisLoss: 15.6139 | LR: 0.0005 | Sigma: 0.9415 \n",
      "\n",
      "[Generation 7/150] - Time: 01:51 - Estimated Finish: 2025-07-12 01:17:37\n",
      "Predator | Avg. ES-Reward: -0.0073 | DisReward (Fitness): 0.0827 | DisLoss: 3.3968 | LR: 0.0009 | Sigma: 0.9321\n",
      "Prey     | Avg. ES-Reward: -0.0007 | DisReward (Fitness): 0.1581 | DisLoss: 11.7225 | LR: 0.0005 | Sigma: 0.9321 \n",
      "\n",
      "[Generation 8/150] - Time: 01:37 - Estimated Finish: 2025-07-12 00:56:08\n",
      "Predator | Avg. ES-Reward: 0.0079 | DisReward (Fitness): 0.0822 | DisLoss: 3.2413 | LR: 0.0009 | Sigma: 0.9227\n",
      "Prey     | Avg. ES-Reward: -0.0001 | DisReward (Fitness): 0.1600 | DisLoss: 10.0410 | LR: 0.0005 | Sigma: 0.9227 \n",
      "\n",
      "[Generation 9/150] - Time: 01:25 - Estimated Finish: 2025-07-12 00:38:24\n",
      "Predator | Avg. ES-Reward: -0.0009 | DisReward (Fitness): 0.0792 | DisLoss: 2.9659 | LR: 0.0009 | Sigma: 0.9135\n",
      "Prey     | Avg. ES-Reward: 0.0013 | DisReward (Fitness): 0.1624 | DisLoss: 7.4759 | LR: 0.0005 | Sigma: 0.9135 \n",
      "\n",
      "[Generation 10/150] - Time: 01:15 - Estimated Finish: 2025-07-12 00:26:55\n",
      "Predator | Avg. ES-Reward: 0.0214 | DisReward (Fitness): 0.0794 | DisLoss: 2.8263 | LR: 0.0009 | Sigma: 0.9044\n",
      "Prey     | Avg. ES-Reward: -0.0009 | DisReward (Fitness): 0.1644 | DisLoss: 6.3009 | LR: 0.0005 | Sigma: 0.9044 \n",
      "\n",
      "[Generation 11/150] - Time: 01:09 - Estimated Finish: 2025-07-12 00:25:04\n",
      "Predator | Avg. ES-Reward: -0.0080 | DisReward (Fitness): 0.0794 | DisLoss: 2.5080 | LR: 0.0009 | Sigma: 0.8953\n",
      "Prey     | Avg. ES-Reward: 0.0034 | DisReward (Fitness): 0.1662 | DisLoss: 5.2055 | LR: 0.0004 | Sigma: 0.8953 \n",
      "\n",
      "[Generation 12/150] - Time: 01:03 - Estimated Finish: 2025-07-12 00:21:35\n",
      "Predator | Avg. ES-Reward: -0.0080 | DisReward (Fitness): 0.0773 | DisLoss: 2.4190 | LR: 0.0009 | Sigma: 0.8864\n",
      "Prey     | Avg. ES-Reward: 0.0017 | DisReward (Fitness): 0.1676 | DisLoss: 4.2514 | LR: 0.0004 | Sigma: 0.8864 \n",
      "\n",
      "[Generation 13/150] - Time: 00:59 - Estimated Finish: 2025-07-12 00:24:06\n",
      "Predator | Avg. ES-Reward: 0.0031 | DisReward (Fitness): 0.0731 | DisLoss: 2.2080 | LR: 0.0009 | Sigma: 0.8775\n",
      "Prey     | Avg. ES-Reward: -0.0006 | DisReward (Fitness): 0.1692 | DisLoss: 3.7663 | LR: 0.0004 | Sigma: 0.8775 \n",
      "\n",
      "[Generation 14/150] - Time: 00:55 - Estimated Finish: 2025-07-12 00:27:32\n",
      "Predator | Avg. ES-Reward: 0.0080 | DisReward (Fitness): 0.0663 | DisLoss: 2.1453 | LR: 0.0009 | Sigma: 0.8687\n",
      "Prey     | Avg. ES-Reward: -0.0012 | DisReward (Fitness): 0.1706 | DisLoss: 3.1856 | LR: 0.0004 | Sigma: 0.8687 \n",
      "\n",
      "[Generation 15/150] - Time: 00:51 - Estimated Finish: 2025-07-12 00:31:15\n",
      "Predator | Avg. ES-Reward: -0.0086 | DisReward (Fitness): 0.0732 | DisLoss: 1.9725 | LR: 0.0009 | Sigma: 0.8601\n",
      "Prey     | Avg. ES-Reward: 0.0020 | DisReward (Fitness): 0.1721 | DisLoss: 2.7879 | LR: 0.0004 | Sigma: 0.8601 \n",
      "\n",
      "[Generation 16/150] - Time: 00:49 - Estimated Finish: 2025-07-12 00:38:00\n",
      "Predator | Avg. ES-Reward: 0.0056 | DisReward (Fitness): 0.0654 | DisLoss: 1.8856 | LR: 0.0009 | Sigma: 0.8515\n",
      "Prey     | Avg. ES-Reward: 0.0060 | DisReward (Fitness): 0.1732 | DisLoss: 2.4674 | LR: 0.0004 | Sigma: 0.8515 \n",
      "\n",
      "[Generation 17/150] - Time: 00:46 - Estimated Finish: 2025-07-12 00:42:58\n",
      "Predator | Avg. ES-Reward: 0.0049 | DisReward (Fitness): 0.0714 | DisLoss: 1.7897 | LR: 0.0008 | Sigma: 0.8429\n",
      "Prey     | Avg. ES-Reward: -0.0025 | DisReward (Fitness): 0.1743 | DisLoss: 2.2330 | LR: 0.0004 | Sigma: 0.8429 \n",
      "\n",
      "[Generation 18/150] - Time: 00:43 - Estimated Finish: 2025-07-12 00:48:34\n",
      "Predator | Avg. ES-Reward: 0.0321 | DisReward (Fitness): 0.0770 | DisLoss: 1.6523 | LR: 0.0008 | Sigma: 0.8345\n",
      "Prey     | Avg. ES-Reward: -0.0029 | DisReward (Fitness): 0.1757 | DisLoss: 2.2553 | LR: 0.0004 | Sigma: 0.8345 \n",
      "\n",
      "[Generation 19/150] - Time: 00:40 - Estimated Finish: 2025-07-12 00:55:50\n",
      "Predator | Avg. ES-Reward: 0.0139 | DisReward (Fitness): 0.0648 | DisLoss: 1.5974 | LR: 0.0008 | Sigma: 0.8262\n",
      "Prey     | Avg. ES-Reward: 0.0011 | DisReward (Fitness): 0.1766 | DisLoss: 2.0443 | LR: 0.0004 | Sigma: 0.8262 \n",
      "\n",
      "[Generation 20/150] - Time: 00:38 - Estimated Finish: 2025-07-12 01:03:42\n",
      "Predator | Avg. ES-Reward: 0.0158 | DisReward (Fitness): 0.0596 | DisLoss: 1.5380 | LR: 0.0008 | Sigma: 0.8179\n",
      "Prey     | Avg. ES-Reward: -0.0080 | DisReward (Fitness): 0.1775 | DisLoss: 2.0152 | LR: 0.0004 | Sigma: 0.8179 \n",
      "\n",
      "[Generation 21/150] - Time: 00:36 - Estimated Finish: 2025-07-12 01:12:03\n",
      "Predator | Avg. ES-Reward: 0.0281 | DisReward (Fitness): 0.0705 | DisLoss: 1.4824 | LR: 0.0008 | Sigma: 0.8097\n",
      "Prey     | Avg. ES-Reward: -0.0005 | DisReward (Fitness): 0.1784 | DisLoss: 1.9890 | LR: 0.0004 | Sigma: 0.8097 \n",
      "\n",
      "[Generation 22/150] - Time: 00:35 - Estimated Finish: 2025-07-12 01:20:09\n",
      "Predator | Avg. ES-Reward: -0.0025 | DisReward (Fitness): 0.0575 | DisLoss: 1.4556 | LR: 0.0008 | Sigma: 0.8016\n",
      "Prey     | Avg. ES-Reward: -0.0002 | DisReward (Fitness): 0.1792 | DisLoss: 2.0166 | LR: 0.0004 | Sigma: 0.8016 \n",
      "\n",
      "[Generation 23/150] - Time: 00:33 - Estimated Finish: 2025-07-12 01:28:36\n",
      "Predator | Avg. ES-Reward: 0.0090 | DisReward (Fitness): 0.0577 | DisLoss: 1.4727 | LR: 0.0008 | Sigma: 0.7936\n",
      "Prey     | Avg. ES-Reward: 0.0053 | DisReward (Fitness): 0.1800 | DisLoss: 1.8267 | LR: 0.0004 | Sigma: 0.7936 \n",
      "\n",
      "[Generation 24/150] - Time: 00:31 - Estimated Finish: 2025-07-12 01:37:45\n",
      "Predator | Avg. ES-Reward: 0.0064 | DisReward (Fitness): 0.0588 | DisLoss: 1.4459 | LR: 0.0008 | Sigma: 0.7857\n",
      "Prey     | Avg. ES-Reward: -0.0018 | DisReward (Fitness): 0.1808 | DisLoss: 1.7940 | LR: 0.0004 | Sigma: 0.7857 \n",
      "\n",
      "[Generation 25/150] - Time: 00:30 - Estimated Finish: 2025-07-12 01:47:19\n",
      "Predator | Avg. ES-Reward: -0.0167 | DisReward (Fitness): 0.0595 | DisLoss: 1.4409 | LR: 0.0008 | Sigma: 0.7778\n",
      "Prey     | Avg. ES-Reward: -0.0010 | DisReward (Fitness): 0.1817 | DisLoss: 1.8244 | LR: 0.0004 | Sigma: 0.7778 \n",
      "\n",
      "[Generation 26/150] - Time: 00:29 - Estimated Finish: 2025-07-12 01:56:34\n",
      "Predator | Avg. ES-Reward: -0.0186 | DisReward (Fitness): 0.0608 | DisLoss: 1.4666 | LR: 0.0008 | Sigma: 0.7700\n",
      "Prey     | Avg. ES-Reward: -0.0022 | DisReward (Fitness): 0.1822 | DisLoss: 1.7593 | LR: 0.0004 | Sigma: 0.7700 \n",
      "\n",
      "[Generation 27/150] - Time: 00:28 - Estimated Finish: 2025-07-12 02:07:47\n",
      "Predator | Avg. ES-Reward: 0.0058 | DisReward (Fitness): 0.0665 | DisLoss: 1.4481 | LR: 0.0008 | Sigma: 0.7623\n",
      "Prey     | Avg. ES-Reward: 0.0005 | DisReward (Fitness): 0.1830 | DisLoss: 1.6575 | LR: 0.0004 | Sigma: 0.7623 \n",
      "\n",
      "[Generation 28/150] - Time: 00:27 - Estimated Finish: 2025-07-12 02:18:13\n",
      "Predator | Avg. ES-Reward: 0.0041 | DisReward (Fitness): 0.0652 | DisLoss: 1.4807 | LR: 0.0008 | Sigma: 0.7547\n",
      "Prey     | Avg. ES-Reward: -0.0044 | DisReward (Fitness): 0.1835 | DisLoss: 1.6365 | LR: 0.0004 | Sigma: 0.7547 \n",
      "\n",
      "[Generation 29/150] - Time: 00:26 - Estimated Finish: 2025-07-12 02:28:46\n",
      "Predator | Avg. ES-Reward: -0.0063 | DisReward (Fitness): 0.0614 | DisLoss: 1.4905 | LR: 0.0007 | Sigma: 0.7472\n",
      "Prey     | Avg. ES-Reward: -0.0032 | DisReward (Fitness): 0.1840 | DisLoss: 1.6580 | LR: 0.0004 | Sigma: 0.7472 \n",
      "\n",
      "[Generation 30/150] - Time: 00:25 - Estimated Finish: 2025-07-12 02:39:21\n",
      "Predator | Avg. ES-Reward: -0.0191 | DisReward (Fitness): 0.0642 | DisLoss: 1.5055 | LR: 0.0007 | Sigma: 0.7397\n",
      "Prey     | Avg. ES-Reward: -0.0051 | DisReward (Fitness): 0.1846 | DisLoss: 1.5425 | LR: 0.0004 | Sigma: 0.7397 \n",
      "\n",
      "[Generation 31/150] - Time: 00:25 - Estimated Finish: 2025-07-12 02:50:25\n",
      "Predator | Avg. ES-Reward: -0.0364 | DisReward (Fitness): 0.0527 | DisLoss: 1.5457 | LR: 0.0007 | Sigma: 0.7323\n",
      "Prey     | Avg. ES-Reward: 0.0011 | DisReward (Fitness): 0.1851 | DisLoss: 1.5150 | LR: 0.0004 | Sigma: 0.7323 \n",
      "\n",
      "[Generation 32/150] - Time: 00:23 - Estimated Finish: 2025-07-12 03:00:19\n",
      "Predator | Avg. ES-Reward: -0.0150 | DisReward (Fitness): 0.0637 | DisLoss: 1.5648 | LR: 0.0007 | Sigma: 0.7250\n",
      "Prey     | Avg. ES-Reward: -0.0032 | DisReward (Fitness): 0.1854 | DisLoss: 1.5439 | LR: 0.0004 | Sigma: 0.7250 \n",
      "\n",
      "[Generation 33/150] - Time: 00:23 - Estimated Finish: 2025-07-12 03:11:17\n",
      "Predator | Avg. ES-Reward: 0.0185 | DisReward (Fitness): 0.0650 | DisLoss: 1.5322 | LR: 0.0007 | Sigma: 0.7177\n",
      "Prey     | Avg. ES-Reward: -0.0018 | DisReward (Fitness): 0.1860 | DisLoss: 1.6293 | LR: 0.0004 | Sigma: 0.7177 \n",
      "\n",
      "[Generation 34/150] - Time: 00:22 - Estimated Finish: 2025-07-12 03:23:09\n",
      "Predator | Avg. ES-Reward: -0.0693 | DisReward (Fitness): 0.0590 | DisLoss: 1.5639 | LR: 0.0007 | Sigma: 0.7106\n",
      "Prey     | Avg. ES-Reward: 0.0066 | DisReward (Fitness): 0.1863 | DisLoss: 1.5188 | LR: 0.0004 | Sigma: 0.7106 \n",
      "\n",
      "[Generation 35/150] - Time: 00:22 - Estimated Finish: 2025-07-12 03:34:26\n",
      "Predator | Avg. ES-Reward: 0.0270 | DisReward (Fitness): 0.0631 | DisLoss: 1.5416 | LR: 0.0007 | Sigma: 0.7034\n",
      "Prey     | Avg. ES-Reward: 0.0023 | DisReward (Fitness): 0.1870 | DisLoss: 1.4979 | LR: 0.0004 | Sigma: 0.7034 \n",
      "\n",
      "[Generation 36/150] - Time: 00:21 - Estimated Finish: 2025-07-12 03:46:01\n",
      "Predator | Avg. ES-Reward: 0.0180 | DisReward (Fitness): 0.0644 | DisLoss: 1.5045 | LR: 0.0007 | Sigma: 0.6964\n",
      "Prey     | Avg. ES-Reward: 0.0000 | DisReward (Fitness): 0.1871 | DisLoss: 1.4786 | LR: 0.0003 | Sigma: 0.6964 \n",
      "\n",
      "[Generation 37/150] - Time: 00:20 - Estimated Finish: 2025-07-12 03:57:10\n",
      "Predator | Avg. ES-Reward: 0.0047 | DisReward (Fitness): 0.0626 | DisLoss: 1.6050 | LR: 0.0007 | Sigma: 0.6894\n",
      "Prey     | Avg. ES-Reward: 0.0025 | DisReward (Fitness): 0.1874 | DisLoss: 1.4924 | LR: 0.0003 | Sigma: 0.6894 \n",
      "\n",
      "[Generation 38/150] - Time: 00:20 - Estimated Finish: 2025-07-12 04:08:45\n",
      "Predator | Avg. ES-Reward: 0.0096 | DisReward (Fitness): 0.0667 | DisLoss: 1.4843 | LR: 0.0007 | Sigma: 0.6826\n",
      "Prey     | Avg. ES-Reward: -0.0003 | DisReward (Fitness): 0.1879 | DisLoss: 1.5018 | LR: 0.0003 | Sigma: 0.6826 \n",
      "\n",
      "[PREDATOR] Early stopping triggered after 20 epochs.\n",
      "Models successfully saved!\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "dis_metrics_pred = []\n",
    "dis_metrics_prey = []\n",
    "\n",
    "es_metrics_pred = []\n",
    "es_metrics_prey = []\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    start_time = time.time()\n",
    "    # Sample traj from expert and generative buffer\n",
    "    expert_pred_batch, expert_prey_batch = expert_buffer.sample(batch_size)\n",
    "    policy_pred_batch, policy_prey_batch = generative_buffer.sample(batch_size)\n",
    "\n",
    "    # Predator discriminator update\n",
    "    dis_metric_pred = pred_discriminator.update(expert_pred_batch, policy_pred_batch, optim_dis_pred, lambda_gp_pred)\n",
    "    dis_metrics_pred.append(dis_metric_pred)\n",
    "                                     \n",
    "    # Prey discriminator update\n",
    "    dis_metric_prey = prey_discriminator.update(expert_prey_batch, policy_prey_batch, optim_dis_prey, lambda_gp_prey)\n",
    "    dis_metrics_prey.append(dis_metric_prey)\n",
    "\n",
    "    for _ in range(gen_dis_ratio):\n",
    "        pred_stats = pred_policy.update(\"predator\", \"pairwise\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma)\n",
    "        \n",
    "        pred_stats += pred_policy.update(\"predator\", \"attention\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma)\n",
    "        es_metrics_pred.append(pred_stats)\n",
    "\n",
    "        prey_stats = prey_policy.update(\"prey\", \"pairwise\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma)\n",
    "        \n",
    "        prey_stats += prey_policy.update(\"prey\", \"attention\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma)\n",
    "        es_metrics_prey.append(prey_stats)\n",
    "\n",
    "    # also reduce globally\n",
    "    sigma *= gamma\n",
    "    lr_pred_policy *= gamma\n",
    "    lr_prey_policy *= gamma\n",
    "\n",
    "    # Generate new trajectories with updated policies\n",
    "    generate_trajectories(buffer=generative_buffer, \n",
    "                            pred_count=pred_count, prey_count=prey_count, action_count=action_count, \n",
    "                            pred_policy=pred_policy, prey_policy=prey_policy, \n",
    "                            num_frames=num_frames, num_generative_episodes=num_generative_episodes)\n",
    "\n",
    "    pred_batch, prey_batch = generative_buffer.get_latest(num_generative_episodes)\n",
    "    r_pred, r_prey = discriminator_reward(pred_batch, prey_batch, pred_discriminator, prey_discriminator)\n",
    "\n",
    "    finish_time, time_str = remaining_time(start_time, num_generations, generation)\n",
    "\n",
    "    avg_es_pred = np.mean([m['avg_reward_diff'] for m in pred_stats])\n",
    "    avg_es_prey = np.mean([m['avg_reward_diff'] for m in prey_stats])\n",
    "        \n",
    "    print(f\"[Generation {generation+1}/{num_generations}] - Time: {time_str} - Estimated Finish: {finish_time}\" )\n",
    "    print(f\"Predator | Avg. ES-Reward: {avg_es_pred:.4f} | DisReward (Fitness): {r_pred:.4f} | DisLoss: {dis_metric_pred[0]:.4f} | LR: {lr_pred_policy:.4f} | Sigma: {sigma:.4f}\")\n",
    "    print(f\"Prey     | Avg. ES-Reward: {avg_es_prey:.4f} | DisReward (Fitness): {r_prey:.4f} | DisLoss: {dis_metric_prey[0]:.4f} | LR: {lr_prey_policy:.4f} | Sigma: {sigma:.4f} \\n\")\n",
    "\n",
    "    if early_stopper_pred(avg_es_pred, \"predator\") or early_stopper_prey(avg_es_prey, \"prey\"):\n",
    "        break\n",
    "\n",
    "\n",
    "    if generation % 50 == 0:\n",
    "        save_checkpoint(save_dir, generation,\n",
    "                        pred_policy, prey_policy,\n",
    "                        pred_discriminator, prey_discriminator,\n",
    "                        optim_dis_pred, optim_dis_prey,\n",
    "                        expert_buffer, generative_buffer,\n",
    "                        dis_metrics_pred, dis_metrics_prey,\n",
    "                        es_metrics_pred, es_metrics_prey)\n",
    "\n",
    "\n",
    "# Save models\n",
    "save_models(save_dir,\n",
    "            pred_policy, prey_policy,\n",
    "            pred_discriminator, prey_discriminator,\n",
    "            optim_dis_pred, optim_dis_prey,\n",
    "            expert_buffer, generative_buffer,\n",
    "            dis_metrics_pred, dis_metrics_prey,\n",
    "            es_metrics_pred, es_metrics_prey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
