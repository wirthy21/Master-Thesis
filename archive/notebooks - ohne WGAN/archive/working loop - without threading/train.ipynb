{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "from utils.OpenAI_ES import *\n",
    "from utils.env_utils import *\n",
    "from utils.train_utils import *\n",
    "from models.Buffer import Buffer\n",
    "from marl_aquarium import aquarium_v0\n",
    "from models.Generator import GeneratorPolicy\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272a92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nToDo's\\n- Anscheinend wird in prey_to_tensor der Räuber nicht berücksichtigt! - anscheinend auch bei generativen Daten (get_state_actions)\\n\\nBeschleunigen:\\n- Parallelization von ES einbauen\\n- parallel_step() in env nutzen!\\n- Samples für generative Buffer direkt während update threaden\\n- to.device() einbauen (auch get_rollouts)\\n- get_env_state_actions() mit Numpy Arrays umsetzen statt df\\n\\nAnschließend (während training):\\n- Evaluation der models\\n- File schreiben indem trainiere Policies geladen und Env gezeigt wird\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo's\n",
    "- Anscheinend wird in prey_to_tensor der Räuber nicht berücksichtigt! - anscheinend auch bei generativen Daten (get_state_actions)\n",
    "\n",
    "Beschleunigen:\n",
    "- Parallelization von ES einbauen\n",
    "- parallel_step() in env nutzen!\n",
    "- Samples für generative Buffer direkt während update threaden\n",
    "- to.device() einbauen (auch get_rollouts)\n",
    "- get_env_state_actions() mit Numpy Arrays umsetzen statt df\n",
    "\n",
    "- Early-Stopping\n",
    "\n",
    "Anschließend (während training):\n",
    "- Evaluation der models\n",
    "- File schreiben indem trainiere Policies geladen und Env gezeigt wird\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aa3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#Environment\n",
    "pred_count = 1\n",
    "prey_count = 32 \n",
    "num_frames = 9\n",
    "num_generative_episodes = 8 # 150 len(expert_data)\n",
    "\n",
    "# Buffer\n",
    "buffer_size = 200 # 300 is length expert data\n",
    "num_trim = 20\n",
    "\n",
    "# Training\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "\n",
    "# Policy\n",
    "lr_pred_policy = 0.1 #like  Wu paper\n",
    "lr_prey_policy = 0.1\n",
    "sigma = 0.1\n",
    "gamma = 0.99\n",
    "num_generations = 2 #10\n",
    "num_perturbations = 2 #30 in Wu paper\n",
    "\n",
    "# Discriminator\n",
    "lr_pred_dis =  0.0001\n",
    "lr_prey_dis = 0.0001\n",
    "\n",
    "lambda_gp_pred = 1\n",
    "lambda_gp_prey = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef35ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Folder\n",
    "path = r\"..\\data\\training\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%d.%m.%Y_%H.%M\")\n",
    "folder_name = f\"Training - {timestamp}\"\n",
    "\n",
    "save_dir = os.path.join(path, folder_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "data_path = r\"..\\data\\processed\\video_8min\\tensors\\32 Preys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7593b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Buffer is empty, load data...\n",
      "Storage of Expert Buffer:  150 \n",
      "\n",
      "Generative Buffer is empty, generating data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:25<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage of Generative Buffer:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pred_policy = GeneratorPolicy().to(device)\n",
    "pred_policy.set_parameters(init=True)\n",
    "optim_policy_pred = torch.optim.Adam(pred_policy.parameters(), lr=lr_pred_policy)\n",
    "\n",
    "prey_policy = GeneratorPolicy().to(device)\n",
    "prey_policy.set_parameters(init=True)\n",
    "optim_policy_prey = torch.optim.Adam(prey_policy.parameters(), lr=lr_prey_policy)\n",
    "\n",
    "pred_discriminator = Discriminator().to(device)\n",
    "pred_discriminator.set_parameters(init=True)\n",
    "optim_dis_pred = torch.optim.Adam(pred_discriminator.parameters(), lr=lr_pred_dis)\n",
    "\n",
    "prey_discriminator = Discriminator().to(device)\n",
    "prey_discriminator.set_parameters(init=True)\n",
    "optim_dis_prey = torch.optim.Adam(prey_discriminator.parameters(), lr=lr_prey_dis)\n",
    "\n",
    "env = aquarium_v0.env(predator_count=pred_count, prey_count=prey_count, action_count=360)\n",
    "\n",
    "# Buffer handling\n",
    "expert_buffer = Buffer(clip_length=num_frames, max_length=buffer_size)\n",
    "generative_buffer = Buffer(clip_length=num_frames, max_length=buffer_size)\n",
    "\n",
    "try:\n",
    "    expert_buffer.load(os.path.join(save_dir, \"buffers\"), \"expert\")\n",
    "    generative_buffer.load(os.path.join(save_dir, \"buffers\"), \"generative\")\n",
    "\n",
    "except:\n",
    "    if len(expert_buffer) == 0:\n",
    "        print(\"Expert Buffer is empty, load data...\")\n",
    "        expert_buffer.add_expert(data_path)\n",
    "        print(\"Storage of Expert Buffer: \", len(expert_buffer), \"\\n\")\n",
    "\n",
    "    if len(generative_buffer) == 0:\n",
    "        print(\"Generative Buffer is empty, generating data...\")\n",
    "        for episode in tqdm.tqdm(range(num_generative_episodes)):\n",
    "            pred_tensors, prey_tensors = get_rollouts(env, pred_policy, prey_policy, num_frames=num_frames, render=False)\n",
    "            generative_buffer.add_generative(pred_tensors, prey_tensors)\n",
    "        print(\"Storage of Generative Buffer: \", len(generative_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53670d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/3\n",
      "Updating Discriminators...\n",
      "Updating Policies...\n",
      "Generating new trajectories... \n",
      "\n",
      "\n",
      " Checkpoint successfully saved!\n",
      "Iteration 0, Estimated Finish: 2025-06-30 16:06:10\n",
      "Predator | Avg. ES-Reward: -0.00 | DisReward (Fitness): -0.19 | DisLoss: 25.60 | LR: 0.1 | Sigma: 0.1\n",
      "Prey     | Avg. ES-Reward: 0.01 | DisReward (Fitness): -0.03 | DisLoss: 1.14 | LR: 0.1 | Sigma: 0.1 \n",
      "\n",
      "Starting Epoch 2/3\n",
      "Updating Discriminators...\n",
      "Updating Policies...\n",
      "Generating new trajectories... \n",
      "\n",
      "Starting Epoch 3/3\n",
      "Updating Discriminators...\n",
      "Updating Policies...\n",
      "Generating new trajectories... \n",
      "\n",
      "\n",
      " Checkpoint successfully saved!\n",
      "Iteration 2, Estimated Finish: 2025-06-30 16:05:56\n",
      "Predator | Avg. ES-Reward: 0.00 | DisReward (Fitness): -0.08 | DisLoss: 28.49 | LR: 0.1 | Sigma: 0.1\n",
      "Prey     | Avg. ES-Reward: -0.01 | DisReward (Fitness): 0.09 | DisLoss: 0.94 | LR: 0.1 | Sigma: 0.1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses_pred_discriminator = []\n",
    "losses_prey_discriminator = []\n",
    "\n",
    "es_metrics_pred = []\n",
    "es_metrics_prey = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"[Epoch {i+1}/{epochs}]\")\n",
    "    start_time = time.time()\n",
    "    # Sample traj from expert and generative buffer\n",
    "    expert_pred_batch, expert_prey_batch = expert_buffer.sample(batch_size, device=device)\n",
    "    policy_pred_batch, policy_prey_batch = generative_buffer.sample(batch_size, device=device)\n",
    "\n",
    "    # Predator discriminator update\n",
    "    print(\"Updating Discriminators...\")\n",
    "    loss_pred_discriminator = pred_discriminator.update(expert_pred_batch, policy_pred_batch, optim_dis_pred, lambda_gp_pred)\n",
    "    losses_pred_discriminator.append(loss_pred_discriminator)\n",
    "                                     \n",
    "    # Prey discriminator update\n",
    "    loss_prey_discriminator = prey_discriminator.update(expert_prey_batch, policy_prey_batch, optim_dis_prey, lambda_gp_prey)\n",
    "    losses_prey_discriminator.append(loss_prey_discriminator)\n",
    "\n",
    "    # Policy updates mit OpenAI-ES\n",
    "    print(\"Updating Policies...\")\n",
    "    pred_stats = pred_policy.update(\"predator\", \"pairwise\", env, \n",
    "                                    pred_policy, prey_policy,\n",
    "                                    pred_discriminator, prey_discriminator,\n",
    "                                    num_generations, num_perturbations,\n",
    "                                    lr_pred_policy, lr_prey_policy,\n",
    "                                    sigma, gamma)\n",
    "    \n",
    "    pred_stats += pred_policy.update(\"predator\", \"attention\", env, \n",
    "                                    pred_policy, prey_policy,\n",
    "                                    pred_discriminator, prey_discriminator,\n",
    "                                    num_generations, num_perturbations, \n",
    "                                    lr_pred_policy, lr_prey_policy,\n",
    "                                    sigma, gamma)\n",
    "    es_metrics_pred.append(pred_stats)\n",
    "\n",
    "    prey_stats = prey_policy.update(\"prey\", \"pairwise\", env, \n",
    "                                    pred_policy, prey_policy,\n",
    "                                    pred_discriminator, prey_discriminator,\n",
    "                                    num_generations, num_perturbations, \n",
    "                                    lr_pred_policy, lr_prey_policy,\n",
    "                                    sigma, gamma)\n",
    "    \n",
    "    prey_stats += prey_policy.update(\"prey\", \"attention\", env, \n",
    "                                    pred_policy, prey_policy,\n",
    "                                    pred_discriminator, prey_discriminator,\n",
    "                                    num_generations, num_perturbations, \n",
    "                                    lr_pred_policy, lr_prey_policy,\n",
    "                                    sigma, gamma)\n",
    "    es_metrics_prey.append(prey_stats)\n",
    "\n",
    "\n",
    "    # Generate new trajectories with updated policies\n",
    "    print(\"Generating new trajectories... \\n\")\n",
    "    pred_tensors = []\n",
    "    prey_tensors = []\n",
    "\n",
    "    for j in range(num_generative_episodes):\n",
    "        pred_tensor, prey_tensor = get_rollouts(env, pred_policy, prey_policy, num_frames=num_frames, render=False)\n",
    "        pred_tensors.append(pred_tensor)\n",
    "        prey_tensors.append(prey_tensor)\n",
    "        generative_buffer.add_generative(pred_tensor, prey_tensor)\n",
    "\n",
    "    pred_batch = torch.stack(pred_tensors, dim=0).to(device)\n",
    "    prey_batch = torch.stack(prey_tensors, dim=0).to(device)\n",
    "    r_pred, r_prey = discriminator_reward(pred_batch, prey_batch, pred_discriminator, prey_discriminator)\n",
    "\n",
    "    finish_time = remaining_time(start_time, epochs, i)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        avg_es_pred = np.mean([m['avg_reward_diff'] for m in pred_stats])\n",
    "        avg_es_prey = np.mean([m['avg_reward_diff'] for m in prey_stats])\n",
    "        \n",
    "        print(f\"Iteration {i}, Estimated Finish: {finish_time}\" )\n",
    "        print(f\"Predator | Avg. ES-Reward: {avg_es_pred:.2f} | DisReward (Fitness): {r_pred:.2f} | DisLoss: {loss_pred_discriminator:.2f} | LR: {lr_pred_policy} | Sigma: {sigma}\")\n",
    "        print(f\"Prey     | Avg. ES-Reward: {avg_es_prey:.2f} | DisReward (Fitness): {r_prey:.2f} | DisLoss: {loss_prey_discriminator:.2f} | LR: {lr_prey_policy} | Sigma: {sigma} \\n\")\n",
    "    \n",
    "\n",
    "    if i % 50 == 0:\n",
    "        save_checkpoint(path, i,\n",
    "                        pred_policy, prey_policy,\n",
    "                        pred_discriminator, prey_discriminator,\n",
    "                        optim_dis_pred, optim_dis_prey,\n",
    "                        expert_buffer, generative_buffer,\n",
    "                        losses_pred_discriminator, losses_prey_discriminator,\n",
    "                        es_metrics_pred, es_metrics_prey)\n",
    "\n",
    "\n",
    "# Save models\n",
    "save_models(path,\n",
    "            pred_policy, prey_policy,\n",
    "            pred_discriminator, prey_discriminator,\n",
    "            optim_dis_pred, optim_dis_prey,\n",
    "            expert_buffer, generative_buffer,\n",
    "            losses_pred_discriminator, losses_prey_discriminator,\n",
    "            es_metrics_pred, es_metrics_prey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
