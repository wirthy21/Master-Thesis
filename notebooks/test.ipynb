{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset_utils import *\n",
    "from collections import defaultdict\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8e826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = \"video_8min\"\n",
    "num_frames=1\n",
    "total_detections=33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf610913",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_video_folder = r'..\\data\\raw\\videos'\n",
    "video_path = raw_video_folder + \"\\\\\" + video + \".mp4\"\n",
    "\n",
    "yolo_path = r'..\\models\\costumized_yolo\\costumized_yolo\\costumized_yolo.pt'\n",
    "processed_video_folder = rf'..\\data\\processed\\{video}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a885acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\anaconda3\\envs\\GAIL\\lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(yolo_path)\n",
    "tracker = DeepSort(max_age=30)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a9e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_frames\n",
    "os.makedirs(os.path.join(processed_video_folder, \"total_frames\"), exist_ok=True)\n",
    "tf_path = os.path.join(processed_video_folder, \"total_frames\", \"total_frames.pkl\")\n",
    "\n",
    "if os.path.exists(tf_path):\n",
    "    with open(tf_path, \"rb\") as f:\n",
    "        total_frames = pickle.load(f)\n",
    "else:\n",
    "    total_frames = []\n",
    "    for frame in tqdm.tqdm(range(total_frame_count), desc=\"Processing frames\"):\n",
    "        frame_records = process_frame(cap, model, tracker, frame)\n",
    "        total_frames.extend(frame_records)\n",
    "    cap.release()\n",
    "    with open(tf_path, \"wb\") as f:\n",
    "        pickle.dump(total_frames, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18dd973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_frames\n",
    "os.makedirs(os.path.join(processed_video_folder, \"filtered_frames\"), exist_ok=True)\n",
    "ff_path = os.path.join(processed_video_folder, \"filtered_frames\", \"filtered_frames.pkl\")\n",
    "ms_path = os.path.join(processed_video_folder, \"filtered_frames\", \"max_speed.pkl\")\n",
    "\n",
    "if os.path.exists(ff_path) and os.path.exists(ms_path):\n",
    "    with open(ff_path, \"rb\") as f:\n",
    "        filtered_frames = pickle.load(f)\n",
    "\n",
    "    with open(ms_path, \"rb\") as f:\n",
    "        max_speed = pickle.load(f)\n",
    "else:\n",
    "    filtered_frames, max_speed = filter_frames(total_frames)\n",
    "    with open(ff_path, \"wb\") as f:\n",
    "        pickle.dump(filtered_frames, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d8dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 742 windows with 33 continuous detections.\n"
     ]
    }
   ],
   "source": [
    "# full_track_windows\n",
    "os.makedirs(os.path.join(processed_video_folder, \"full_track_windows\"), exist_ok=True)\n",
    "ftw_path = os.path.join(processed_video_folder, \"full_track_windows\", f\"full_track_windows_{total_detections}.pkl\")\n",
    "vw_path = os.path.join(processed_video_folder, \"full_track_windows\", f\"valid_windows_{total_detections}.pkl\")\n",
    "\n",
    "if os.path.exists(ftw_path) and os.path.exists(vw_path):\n",
    "    with open(ftw_path, \"rb\") as f:\n",
    "        full_track_windows = pickle.load(f)\n",
    "\n",
    "    with open(vw_path, \"rb\") as f:\n",
    "        valid_windows = pickle.load(f)\n",
    "else:\n",
    "    full_track_windows, valid_windows = find_valid_windows(filtered_frames, num_frames=num_frames, total_detections=total_detections)\n",
    "    with open(ftw_path, \"wb\") as f:\n",
    "        pickle.dump(full_track_windows, f)\n",
    "\n",
    "    with open(vw_path, \"wb\") as f:\n",
    "        pickle.dump(valid_windows, f)\n",
    "\n",
    "print(f\"Found {len(valid_windows)} windows with {total_detections} continuous detections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415794f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_features(frame, width, height, max_speed):    \n",
    "    frame = sorted(frame, key=lambda det: det['label'] != '0') # sort so that Pred is always first\n",
    "\n",
    "    vscale = np.vectorize(scale)\n",
    "\n",
    "    xs = np.array([det['x'] for det in frame])\n",
    "    ys = np.array([det['y'] for det in frame])\n",
    "    scaled_xs = vscale(xs, 0, width, 0, 1)\n",
    "    scaled_ys = vscale(ys, 0, height, 0, 1)\n",
    "\n",
    "    vxs = np.array([det['vx'] for det in frame])\n",
    "    vys = np.array([det['vy'] for det in frame])\n",
    "\n",
    "    thetas = np.array([det['angle'] for det in frame])\n",
    "    scaled_thetas = vscale(thetas, -np.pi, np.pi, -1, 1)\n",
    "\n",
    "    speed = np.array([det['speed'] for det in frame])\n",
    "    scaled_speed = vscale(speed, 0, max_speed, 0, 1)\n",
    "\n",
    "    cos_t = np.cos(thetas)                        \n",
    "    sin_t = np.sin(thetas)\n",
    "\n",
    "    # pairwise distances\n",
    "    dx = scaled_xs[None, :] - scaled_xs[:, None]\n",
    "    dy = scaled_ys[None, :] - scaled_ys[:, None]\n",
    "\n",
    "    # relative velocities\n",
    "    rel_vx = cos_t[:, None] * vxs[None, :] + sin_t[:, None] * vys[None, :]\n",
    "    rel_vy = -sin_t[:, None] * vxs[None, :] + cos_t[:, None] * vys[None, :]\n",
    "    scaled_rel_vx = vscale(rel_vx, -max_speed, max_speed, -1, 1)\n",
    "    scaled_rel_vy = vscale(rel_vy, -max_speed, max_speed, -1, 1)\n",
    "\n",
    "    n = scaled_xs.shape[0]\n",
    "    thetas_mat = np.tile(scaled_thetas[:, None], (1, n))\n",
    "    speed_mat = np.tile(scaled_speed[:, None], (1, n))\n",
    "    features = np.stack([dx, dy, scaled_rel_vx, scaled_rel_vy, speed_mat, thetas_mat], axis=-1)\n",
    "\n",
    "    mask = ~np.eye(n, dtype=bool) # shape (N, N)\n",
    "    neigh = features[mask].reshape(n, n-1, 6)\n",
    "\n",
    "    pred_tensor = torch.from_numpy(neigh[0]).unsqueeze(0)\n",
    "    prey_tensor = torch.from_numpy(neigh[1:]) # shape (N-1, N-1, 5)\n",
    "\n",
    "    return pred_tensor, prey_tensor\n",
    "\n",
    "\n",
    "def get_expert_tensors(full_track_windows, valid_windows, width, height, max_speed, window_size=9):\n",
    "    start_frames = [vw['start_frame'] for vw in valid_windows]\n",
    "    pred_windows = []\n",
    "    prey_windows = []\n",
    "\n",
    "    for idx, start in enumerate(start_frames):\n",
    "        window_detections = []\n",
    "        for frame in range(start, start + window_size):\n",
    "            dets = [det for det in full_track_windows[idx] if det['frame'] == frame]\n",
    "            window_detections.append(dets)\n",
    "\n",
    "        preds = []\n",
    "        preys = []\n",
    "        for dets in window_detections:\n",
    "            pred_tensor, prey_tensor = get_expert_features(dets, width, height, max_speed)\n",
    "            preds.append(pred_tensor)\n",
    "            preys.append(prey_tensor)\n",
    "\n",
    "        pred_windows.append(torch.stack(preds, dim=0))\n",
    "        prey_windows.append(torch.stack(preys, dim=0))\n",
    "\n",
    "    pred_tensor = torch.stack(pred_windows, dim=0)\n",
    "    prey_tensor = torch.stack(prey_windows, dim=0)\n",
    "\n",
    "    return pred_tensor, prey_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d2c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Tensors Shape: torch.Size([742, 1, 1, 32, 6])\n",
      "Prey Tensors Shape: torch.Size([742, 1, 32, 32, 6])\n"
     ]
    }
   ],
   "source": [
    "pred_tensors, prey_tensors = get_expert_tensors(full_track_windows, valid_windows, width, height, max_speed, window_size=num_frames)\n",
    "\n",
    "print(\"Pred Tensors Shape:\", pred_tensors.shape)\n",
    "print(\"Prey Tensors Shape:\", prey_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c1d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([23744, 4])\n",
      "y shape: torch.Size([23744])\n"
     ]
    }
   ],
   "source": [
    "# Dein Tensor: pred_tensors shape (742,1,1,32,6)\n",
    "data = pred_tensors.squeeze()  # -> shape (742,32,6)\n",
    "\n",
    "# Features und Target extrahieren\n",
    "X = data[..., :4]  # dx,dy,vx,vy (742,32,4)\n",
    "y = data[..., 4]   # v (742,32)\n",
    "\n",
    "# In 2D-Shape umformen (Samples, Features)\n",
    "X = X.reshape(-1, 4)  # (742*32, 4)\n",
    "y = y.reshape(-1)     # (742*32,)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831e70ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koeffizienten: [  0.0076661    0.028263    -0.10229    0.010941]\n",
      "Intercept: 0.06784917995377002\n",
      "R² Score: 0.038920663912849984\n",
      "MSE: 0.0031988220544649022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_np = X.numpy()\n",
    "y_np = y.numpy()\n",
    "\n",
    "# Modell erstellen und fitten\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_np, y_np)\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred = reg.predict(X_np)\n",
    "\n",
    "# Auswertung\n",
    "print(\"Koeffizienten:\", reg.coef_)\n",
    "print(\"Intercept:\", reg.intercept_)\n",
    "print(\"R² Score:\", r2_score(y_np, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_np, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad35bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\anaconda3\\envs\\GAIL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([23744])) that is different to the input size (torch.Size([23744, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200 - Loss: 0.006297\n",
      "Epoch 20/200 - Loss: 0.003539\n",
      "Epoch 40/200 - Loss: 0.003354\n",
      "Epoch 60/200 - Loss: 0.003343\n",
      "Epoch 80/200 - Loss: 0.003339\n",
      "Epoch 100/200 - Loss: 0.003337\n",
      "Epoch 120/200 - Loss: 0.003335\n",
      "Epoch 140/200 - Loss: 0.003333\n",
      "Epoch 160/200 - Loss: 0.003332\n",
      "Epoch 180/200 - Loss: 0.003332\n",
      "R² Score: 0.0009\n",
      "MSE: 0.003325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "class VelocityMLP(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Stelle sicher, dass X und y Float32 sind\n",
    "X = X.float()\n",
    "y = y.float()\n",
    "\n",
    "# Rest deines Codes bleibt gleich\n",
    "model = VelocityMLP(input_dim=4, hidden_dim=32)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "r2 = r2_score(y.numpy(), y_pred.numpy())\n",
    "mse = mean_squared_error(y.numpy(), y_pred.numpy())\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
