{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\anaconda3\\envs\\GAIL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : No C++ compiler found. Define CXX environment variable or install g++.\n",
      "[KeOps] Warning : No C++ compiler found. You need to either define the CXX environment variable pointing to a valid compiler, or ensure that 'g++' is installed and in your PATH.\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n",
      "[KeOps] Warning : No C++ compiler found. You need to either define the CXX environment variable pointing to a valid compiler, or ensure that 'g++' is installed and in your PATH.\n",
      "[KeOps] Warning : No C++ compiler available to check for OpenMP support.\n",
      "[KeOps] Warning : OpenMP support is not available. Disabling OpenMP.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ema_pytorch import EMA\n",
    "from datetime import datetime\n",
    "from utils.sim_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.train_utils import *\n",
    "from utils.couzin_utils import *\n",
    "from utils.vec_sim_utils import *\n",
    "from utils.encoder_utils import *\n",
    "from utils.dataset_utils import *\n",
    "from geomloss import SamplesLoss\n",
    "from utils.mmd_loss import MMDLoss\n",
    "from models.Generator import ModularPolicy\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert\n",
    "max_steps = 300\n",
    "\n",
    "# Training\n",
    "num_generations = 4000\n",
    "gamma = 0.999\n",
    "deterministic=False # BC pretrain\n",
    "performance_eval = 5\n",
    "num_perturbations = 64\n",
    "\n",
    "### Prey ###\n",
    "lr_prey_policy = 2e-4\n",
    "sigma_prey = 0.1\n",
    "\n",
    "prey_dis_balance_factor = 2\n",
    "prey_noise = 0.005\n",
    "lr_prey_disc = 5e-4\n",
    "lambda_gp_prey = 5\n",
    "prey_update_mode = \"avoid\"\n",
    "\n",
    "\n",
    "### Predator ###\n",
    "lr_pred_policy = 1e-4\n",
    "sigma_pred = 0.08\n",
    "\n",
    "pred_dis_balance_factor = 2\n",
    "pred_noise = 0.005\n",
    "lr_pred_disc = 2e-4\n",
    "lambda_gp_pred = 10\n",
    "pred_update_mode = \"mean\"\n",
    "\n",
    "\n",
    "# Env Settings\n",
    "height = 50\n",
    "width = 50\n",
    "prey_speed = 5\n",
    "pred_speed = 5\n",
    "step_size = 0.5\n",
    "theta_dot_max = 0.5\n",
    "max_turn = float(theta_dot_max * step_size) + 1e-12\n",
    "\n",
    "pert_steps = 100\n",
    "init_steps = 500\n",
    "\n",
    "env_settings = (height, width, prey_speed, pred_speed, step_size, max_turn, pert_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce97496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pred Shape: torch.Size([500, 1, 32, 5])\n",
      "Prey Shape: torch.Size([500, 32, 32, 6])\n",
      "\n",
      "Pred Tensor Shape: torch.Size([491, 10, 1, 32, 5])\n",
      "Prey Tensor Shape: torch.Size([491, 10, 32, 32, 6])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "exp_pred_sequence, exp_prey_sequence, couzin_metrics, actions, init_pool = run_couzin_simulation(\n",
    "                                        visualization=\"off\", \n",
    "                                        max_steps=init_steps, \n",
    "                                        constant_speed=prey_speed, shark_speed=pred_speed, \n",
    "                                        area_width=width, area_height=height,\n",
    "                                        dt = step_size,\n",
    "                                        alpha=0.01,\n",
    "                                        theta_dot_max=theta_dot_max, theta_dot_max_shark=theta_dot_max,\n",
    "                                        number_of_sharks=1, n=32)\n",
    "\n",
    "exp_pred_sequence = exp_pred_sequence.to(device)\n",
    "exp_prey_sequence = exp_prey_sequence.to(device)\n",
    "init_pool = init_pool.to(device)\n",
    "\n",
    "print(\"\\nPred Shape:\", exp_pred_sequence.shape)\n",
    "print(\"Prey Shape:\", exp_prey_sequence.shape)\n",
    "\n",
    "exp_pred_tensor = sliding_window(exp_pred_sequence, window_size=10)\n",
    "exp_prey_tensor = sliding_window(exp_prey_sequence, window_size=10)\n",
    "\n",
    "print(\"\\nPred Tensor Shape:\", exp_pred_tensor.shape)\n",
    "print(\"Prey Tensor Shape:\", exp_prey_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8323ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010: loss=23.101410 sim=0.0550 std=1.2995 cov=0.4469 std_mean=0.350\n",
      "epoch 020: loss=22.217934 sim=0.0427 std=1.2902 cov=0.3597 std_mean=0.355\n",
      "epoch 030: loss=21.511539 sim=0.0409 std=1.2332 cov=0.3982 std_mean=0.383\n",
      "epoch 040: loss=21.125330 sim=0.0470 std=1.1800 cov=0.4501 std_mean=0.410\n",
      "epoch 050: loss=20.637005 sim=0.0476 std=1.1417 cov=0.4644 std_mean=0.429\n",
      "epoch 060: loss=20.217930 sim=0.0431 std=1.1076 cov=0.5051 std_mean=0.446\n",
      "epoch 070: loss=20.332235 sim=0.0508 std=1.0937 cov=0.5313 std_mean=0.453\n",
      "epoch 080: loss=20.114101 sim=0.0484 std=1.0799 cov=0.5414 std_mean=0.460\n",
      "epoch 090: loss=20.111403 sim=0.0522 std=1.0651 cov=0.5658 std_mean=0.467\n",
      "epoch 100: loss=19.892855 sim=0.0527 std=1.0612 cov=0.5317 std_mean=0.469\n",
      "epoch 110: loss=19.920315 sim=0.0508 std=1.0565 cov=0.5606 std_mean=0.472\n",
      "epoch 120: loss=19.913464 sim=0.0552 std=1.0376 cov=0.5937 std_mean=0.481\n",
      "epoch 130: loss=20.070309 sim=0.0615 std=1.0589 cov=0.5296 std_mean=0.471\n",
      "epoch 140: loss=19.719280 sim=0.0577 std=1.0348 cov=0.5512 std_mean=0.483\n",
      "epoch 150: loss=19.762335 sim=0.0601 std=1.0362 cov=0.5434 std_mean=0.482\n",
      "epoch 160: loss=19.734421 sim=0.0574 std=1.0361 cov=0.5513 std_mean=0.482\n",
      "epoch 170: loss=19.740803 sim=0.0646 std=1.0247 cov=0.5510 std_mean=0.488\n",
      "epoch 180: loss=20.007639 sim=0.0649 std=1.0191 cov=0.6201 std_mean=0.490\n",
      "epoch 190: loss=19.806324 sim=0.0636 std=1.0231 cov=0.5738 std_mean=0.488\n",
      "epoch 200: loss=19.494263 sim=0.0583 std=1.0210 cov=0.5444 std_mean=0.490\n",
      "epoch 210: loss=19.846466 sim=0.0686 std=1.0281 cov=0.5422 std_mean=0.486\n",
      "epoch 220: loss=19.797886 sim=0.0682 std=1.0164 cov=0.5696 std_mean=0.492\n",
      "epoch 230: loss=19.590263 sim=0.0568 std=1.0166 cov=0.5844 std_mean=0.492\n",
      "epoch 240: loss=19.450081 sim=0.0608 std=1.0049 cov=0.5711 std_mean=0.498\n",
      "epoch 250: loss=19.734972 sim=0.0678 std=1.0210 cov=0.5448 std_mean=0.489\n",
      "epoch 260: loss=19.776571 sim=0.0703 std=1.0183 cov=0.5489 std_mean=0.491\n",
      "epoch 270: loss=19.431902 sim=0.0571 std=1.0055 cov=0.5844 std_mean=0.497\n",
      "epoch 280: loss=19.506319 sim=0.0601 std=1.0040 cov=0.5888 std_mean=0.498\n",
      "epoch 290: loss=19.242561 sim=0.0637 std=0.9852 cov=0.5744 std_mean=0.507\n",
      "epoch 300: loss=19.321213 sim=0.0605 std=0.9896 cov=0.5930 std_mean=0.505\n",
      "epoch 310: loss=19.416416 sim=0.0673 std=0.9987 cov=0.5506 std_mean=0.501\n",
      "epoch 320: loss=19.459406 sim=0.0664 std=1.0014 cov=0.5560 std_mean=0.499\n",
      "epoch 330: loss=19.235371 sim=0.0667 std=0.9813 cov=0.5698 std_mean=0.509\n",
      "epoch 340: loss=19.269325 sim=0.0607 std=0.9877 cov=0.5874 std_mean=0.506\n",
      "epoch 350: loss=19.433828 sim=0.0744 std=0.9531 cov=0.6556 std_mean=0.523\n",
      "epoch 360: loss=19.108234 sim=0.0649 std=0.9744 cov=0.5742 std_mean=0.513\n",
      "epoch 370: loss=19.146223 sim=0.0647 std=0.9743 cov=0.5828 std_mean=0.513\n",
      "epoch 380: loss=19.176607 sim=0.0666 std=0.9599 cov=0.6229 std_mean=0.520\n",
      "epoch 390: loss=19.614250 sim=0.0750 std=0.9740 cov=0.6257 std_mean=0.513\n",
      "epoch 400: loss=18.871048 sim=0.0620 std=0.9678 cov=0.5608 std_mean=0.516\n",
      "epoch 410: loss=18.778080 sim=0.0680 std=0.9478 cov=0.5721 std_mean=0.526\n",
      "epoch 420: loss=19.192793 sim=0.0757 std=0.9740 cov=0.5378 std_mean=0.513\n",
      "epoch 430: loss=18.654730 sim=0.0669 std=0.9398 cov=0.5774 std_mean=0.530\n",
      "epoch 440: loss=19.270485 sim=0.0786 std=0.9565 cov=0.5915 std_mean=0.522\n",
      "epoch 450: loss=18.770308 sim=0.0753 std=0.9342 cov=0.5751 std_mean=0.533\n",
      "epoch 460: loss=18.398125 sim=0.0659 std=0.9274 cov=0.5678 std_mean=0.536\n",
      "epoch 470: loss=18.629105 sim=0.0669 std=0.9353 cov=0.5855 std_mean=0.532\n",
      "epoch 480: loss=18.662979 sim=0.0774 std=0.9141 cov=0.6032 std_mean=0.543\n",
      "epoch 490: loss=18.615477 sim=0.0681 std=0.9372 cov=0.5709 std_mean=0.531\n",
      "epoch 500: loss=18.701508 sim=0.0667 std=0.9385 cov=0.5913 std_mean=0.531\n",
      "epoch 510: loss=18.409771 sim=0.0696 std=0.9048 cov=0.6194 std_mean=0.548\n",
      "epoch 520: loss=18.694887 sim=0.0745 std=0.9390 cov=0.5496 std_mean=0.531\n",
      "epoch 530: loss=18.380291 sim=0.0725 std=0.8960 cov=0.6256 std_mean=0.552\n",
      "epoch 540: loss=18.393152 sim=0.0759 std=0.9041 cov=0.5871 std_mean=0.548\n",
      "epoch 550: loss=18.629053 sim=0.0780 std=0.9030 cov=0.6268 std_mean=0.548\n",
      "epoch 560: loss=18.227428 sim=0.0760 std=0.8956 cov=0.5786 std_mean=0.552\n",
      "epoch 570: loss=18.281267 sim=0.0770 std=0.8938 cov=0.5897 std_mean=0.553\n",
      "epoch 580: loss=17.947268 sim=0.0714 std=0.8871 cov=0.5712 std_mean=0.556\n",
      "epoch 590: loss=17.962692 sim=0.0780 std=0.8664 cov=0.6036 std_mean=0.567\n",
      "epoch 600: loss=18.125872 sim=0.0798 std=0.8699 cov=0.6163 std_mean=0.565\n",
      "epoch 610: loss=18.221420 sim=0.0815 std=0.8749 cov=0.6120 std_mean=0.563\n",
      "epoch 620: loss=18.039711 sim=0.0752 std=0.8768 cov=0.6015 std_mean=0.562\n",
      "epoch 630: loss=18.020332 sim=0.0821 std=0.8587 cov=0.6174 std_mean=0.571\n",
      "epoch 640: loss=18.192646 sim=0.0815 std=0.8799 cov=0.5915 std_mean=0.560\n",
      "epoch 650: loss=17.888504 sim=0.0815 std=0.8447 cov=0.6360 std_mean=0.578\n",
      "epoch 660: loss=17.994658 sim=0.0837 std=0.8399 cov=0.6608 std_mean=0.580\n",
      "epoch 670: loss=18.078323 sim=0.0811 std=0.8495 cov=0.6616 std_mean=0.575\n",
      "epoch 680: loss=17.748781 sim=0.0810 std=0.8439 cov=0.6127 std_mean=0.578\n",
      "epoch 690: loss=17.585869 sim=0.0820 std=0.8213 cov=0.6433 std_mean=0.589\n",
      "epoch 700: loss=17.813694 sim=0.0831 std=0.8374 cov=0.6351 std_mean=0.581\n",
      "epoch 710: loss=17.505241 sim=0.0800 std=0.8167 cov=0.6508 std_mean=0.592\n",
      "epoch 720: loss=17.730932 sim=0.0851 std=0.8276 cov=0.6381 std_mean=0.586\n",
      "epoch 730: loss=17.537521 sim=0.0851 std=0.8277 cov=0.5987 std_mean=0.586\n",
      "epoch 740: loss=17.330744 sim=0.0818 std=0.8125 cov=0.6196 std_mean=0.594\n",
      "epoch 750: loss=17.330805 sim=0.0842 std=0.8051 cov=0.6299 std_mean=0.597\n",
      "epoch 760: loss=17.305140 sim=0.0775 std=0.8136 cov=0.6328 std_mean=0.593\n",
      "epoch 770: loss=17.092640 sim=0.0751 std=0.8083 cov=0.6181 std_mean=0.596\n",
      "epoch 780: loss=17.425776 sim=0.0824 std=0.8055 cov=0.6567 std_mean=0.597\n",
      "epoch 790: loss=17.156206 sim=0.0789 std=0.7870 cov=0.6756 std_mean=0.607\n",
      "epoch 800: loss=17.156847 sim=0.0848 std=0.7939 cov=0.6255 std_mean=0.603\n",
      "epoch 810: loss=17.362190 sim=0.0851 std=0.7988 cov=0.6506 std_mean=0.601\n",
      "epoch 820: loss=17.325462 sim=0.0817 std=0.7843 cov=0.7038 std_mean=0.608\n",
      "epoch 830: loss=16.893139 sim=0.0805 std=0.7840 cov=0.6242 std_mean=0.608\n",
      "epoch 840: loss=16.879507 sim=0.0748 std=0.7924 cov=0.6245 std_mean=0.604\n",
      "epoch 850: loss=16.894775 sim=0.0867 std=0.7777 cov=0.6122 std_mean=0.611\n",
      "epoch 860: loss=17.053453 sim=0.0862 std=0.7598 cov=0.7002 std_mean=0.620\n",
      "epoch 870: loss=17.026596 sim=0.0913 std=0.7550 cov=0.6841 std_mean=0.623\n",
      "epoch 880: loss=16.812618 sim=0.0872 std=0.7673 cov=0.6247 std_mean=0.616\n",
      "epoch 890: loss=16.981672 sim=0.0829 std=0.7426 cov=0.7543 std_mean=0.629\n",
      "epoch 900: loss=17.313293 sim=0.0850 std=0.7506 cov=0.7857 std_mean=0.625\n",
      "epoch 910: loss=17.039705 sim=0.0890 std=0.7678 cov=0.6595 std_mean=0.616\n",
      "epoch 920: loss=16.463745 sim=0.0820 std=0.7492 cov=0.6351 std_mean=0.625\n",
      "epoch 930: loss=16.980404 sim=0.0935 std=0.7437 cov=0.6976 std_mean=0.628\n",
      "epoch 940: loss=16.834505 sim=0.0961 std=0.7317 cov=0.6910 std_mean=0.634\n",
      "epoch 950: loss=16.783152 sim=0.0886 std=0.7459 cov=0.6760 std_mean=0.627\n",
      "epoch 960: loss=16.863972 sim=0.0959 std=0.7410 cov=0.6704 std_mean=0.629\n",
      "epoch 970: loss=16.491711 sim=0.0824 std=0.7495 cov=0.6375 std_mean=0.625\n",
      "epoch 980: loss=16.510609 sim=0.0889 std=0.7263 cov=0.6786 std_mean=0.637\n",
      "epoch 990: loss=16.528275 sim=0.0947 std=0.7140 cov=0.6898 std_mean=0.643\n",
      "epoch 1000: loss=16.518869 sim=0.0953 std=0.7080 cov=0.7032 std_mean=0.646\n",
      "epoch 1010: loss=16.372192 sim=0.0949 std=0.7214 cov=0.6358 std_mean=0.639\n",
      "epoch 1020: loss=16.587585 sim=0.0893 std=0.7162 cov=0.7225 std_mean=0.642\n",
      "epoch 1030: loss=16.521589 sim=0.0958 std=0.7146 cov=0.6815 std_mean=0.643\n",
      "epoch 1040: loss=16.177292 sim=0.0882 std=0.6929 cov=0.7158 std_mean=0.654\n",
      "epoch 1050: loss=16.237499 sim=0.0913 std=0.7151 cov=0.6456 std_mean=0.642\n",
      "epoch 1060: loss=16.436752 sim=0.0920 std=0.6995 cov=0.7292 std_mean=0.650\n",
      "epoch 1070: loss=16.392675 sim=0.0892 std=0.7214 cov=0.6683 std_mean=0.639\n",
      "epoch 1080: loss=16.313198 sim=0.0967 std=0.7026 cov=0.6716 std_mean=0.649\n",
      "epoch 1090: loss=16.456463 sim=0.0981 std=0.7030 cov=0.6919 std_mean=0.648\n",
      "epoch 1100: loss=16.079607 sim=0.0931 std=0.6950 cov=0.6656 std_mean=0.653\n",
      "epoch 1110: loss=16.603916 sim=0.0981 std=0.6998 cov=0.7307 std_mean=0.650\n",
      "epoch 1120: loss=16.861916 sim=0.0997 std=0.7120 cov=0.7380 std_mean=0.644\n",
      "epoch 1130: loss=16.144258 sim=0.0940 std=0.6807 cov=0.7164 std_mean=0.660\n",
      "epoch 1140: loss=16.109707 sim=0.0924 std=0.6767 cov=0.7296 std_mean=0.662\n",
      "epoch 1150: loss=16.645918 sim=0.0993 std=0.7031 cov=0.7235 std_mean=0.648\n",
      "epoch 1160: loss=15.947380 sim=0.0890 std=0.6724 cov=0.7272 std_mean=0.664\n",
      "epoch 1170: loss=16.125387 sim=0.0968 std=0.6683 cov=0.7360 std_mean=0.666\n",
      "epoch 1180: loss=16.043913 sim=0.0910 std=0.6656 cov=0.7569 std_mean=0.667\n",
      "epoch 1190: loss=16.438210 sim=0.1066 std=0.6862 cov=0.6959 std_mean=0.657\n",
      "epoch 1200: loss=15.943186 sim=0.0980 std=0.6663 cov=0.6999 std_mean=0.667\n",
      "Prey Encoder trained & frozen.\n",
      "\n",
      "epoch 010: loss=23.742674 sim=0.0511 std=1.4090 cov=0.2659 std_mean=0.295\n",
      "epoch 020: loss=23.253445 sim=0.0443 std=1.3427 cov=0.4008 std_mean=0.329\n",
      "epoch 030: loss=22.765778 sim=0.0396 std=1.3416 cov=0.3306 std_mean=0.329\n",
      "epoch 040: loss=22.576605 sim=0.0410 std=1.3066 cov=0.3905 std_mean=0.347\n",
      "epoch 050: loss=22.245211 sim=0.0378 std=1.3103 cov=0.3293 std_mean=0.345\n",
      "epoch 060: loss=22.682888 sim=0.0519 std=1.3041 cov=0.3647 std_mean=0.348\n",
      "epoch 070: loss=22.685606 sim=0.0531 std=1.2829 cov=0.4229 std_mean=0.359\n",
      "epoch 080: loss=22.382029 sim=0.0403 std=1.2565 cov=0.5054 std_mean=0.372\n",
      "epoch 090: loss=22.490557 sim=0.0623 std=1.2491 cov=0.4393 std_mean=0.375\n",
      "epoch 100: loss=21.988516 sim=0.0294 std=1.2767 cov=0.4207 std_mean=0.362\n",
      "epoch 110: loss=22.174824 sim=0.0528 std=1.2670 cov=0.3697 std_mean=0.366\n",
      "epoch 120: loss=21.482307 sim=0.0373 std=1.2344 cov=0.4066 std_mean=0.383\n",
      "epoch 130: loss=21.875771 sim=0.0382 std=1.2246 cov=0.5105 std_mean=0.388\n",
      "epoch 140: loss=21.346563 sim=0.0355 std=1.2301 cov=0.4017 std_mean=0.385\n",
      "epoch 150: loss=23.023930 sim=0.0594 std=1.2533 cov=0.5480 std_mean=0.373\n",
      "epoch 160: loss=21.546444 sim=0.0377 std=1.2000 cov=0.5207 std_mean=0.400\n",
      "epoch 170: loss=21.461338 sim=0.0389 std=1.2077 cov=0.4746 std_mean=0.396\n",
      "epoch 180: loss=21.642971 sim=0.0405 std=1.1988 cov=0.5294 std_mean=0.401\n",
      "epoch 190: loss=22.326521 sim=0.0533 std=1.2122 cov=0.5621 std_mean=0.394\n",
      "epoch 200: loss=22.013243 sim=0.0437 std=1.2106 cov=0.5525 std_mean=0.395\n",
      "epoch 210: loss=21.984221 sim=0.0573 std=1.2246 cov=0.4368 std_mean=0.388\n",
      "epoch 220: loss=21.827055 sim=0.0491 std=1.2185 cov=0.4641 std_mean=0.391\n",
      "epoch 230: loss=21.745867 sim=0.0503 std=1.2191 cov=0.4406 std_mean=0.390\n",
      "epoch 240: loss=21.445885 sim=0.0485 std=1.2086 cov=0.4206 std_mean=0.396\n",
      "epoch 250: loss=22.024170 sim=0.0561 std=1.2150 cov=0.4792 std_mean=0.393\n",
      "epoch 260: loss=21.583368 sim=0.0517 std=1.2133 cov=0.4180 std_mean=0.393\n",
      "epoch 270: loss=21.549782 sim=0.0522 std=1.1957 cov=0.4618 std_mean=0.402\n",
      "epoch 280: loss=21.422764 sim=0.0494 std=1.1620 cov=0.5516 std_mean=0.419\n",
      "epoch 290: loss=21.351576 sim=0.0438 std=1.1704 cov=0.5402 std_mean=0.415\n",
      "epoch 300: loss=21.463654 sim=0.0478 std=1.1767 cov=0.5234 std_mean=0.412\n",
      "epoch 310: loss=21.702494 sim=0.0504 std=1.2085 cov=0.4629 std_mean=0.396\n",
      "epoch 320: loss=21.644066 sim=0.0543 std=1.2172 cov=0.4056 std_mean=0.391\n",
      "epoch 330: loss=21.905724 sim=0.0528 std=1.2062 cov=0.4987 std_mean=0.397\n",
      "epoch 340: loss=21.043037 sim=0.0326 std=1.1876 cov=0.4825 std_mean=0.406\n",
      "epoch 350: loss=21.511551 sim=0.0464 std=1.1688 cov=0.5639 std_mean=0.416\n",
      "epoch 360: loss=21.061131 sim=0.0462 std=1.1644 cov=0.4880 std_mean=0.418\n",
      "epoch 370: loss=22.159199 sim=0.0430 std=1.2157 cov=0.5698 std_mean=0.392\n",
      "epoch 380: loss=21.738567 sim=0.0602 std=1.1953 cov=0.4610 std_mean=0.402\n",
      "epoch 390: loss=21.756918 sim=0.0655 std=1.1859 cov=0.4660 std_mean=0.407\n",
      "epoch 400: loss=21.733276 sim=0.0553 std=1.1878 cov=0.5068 std_mean=0.406\n",
      "epoch 410: loss=21.078133 sim=0.0404 std=1.1898 cov=0.4439 std_mean=0.405\n",
      "epoch 420: loss=21.694506 sim=0.0539 std=1.1912 cov=0.4960 std_mean=0.404\n",
      "epoch 430: loss=21.830822 sim=0.0563 std=1.1862 cov=0.5262 std_mean=0.407\n",
      "epoch 440: loss=21.250189 sim=0.0421 std=1.2052 cov=0.4240 std_mean=0.397\n",
      "epoch 450: loss=21.939873 sim=0.0571 std=1.1636 cov=0.6116 std_mean=0.418\n",
      "epoch 460: loss=21.243509 sim=0.0458 std=1.1712 cov=0.5062 std_mean=0.414\n",
      "epoch 470: loss=21.386433 sim=0.0652 std=1.1668 cov=0.4510 std_mean=0.417\n",
      "epoch 480: loss=21.726967 sim=0.0658 std=1.1988 cov=0.4203 std_mean=0.401\n",
      "epoch 490: loss=21.230419 sim=0.0584 std=1.1659 cov=0.4565 std_mean=0.417\n",
      "epoch 500: loss=21.025391 sim=0.0382 std=1.1918 cov=0.4387 std_mean=0.404\n",
      "epoch 510: loss=21.758905 sim=0.0617 std=1.1793 cov=0.5053 std_mean=0.410\n",
      "epoch 520: loss=22.428617 sim=0.0714 std=1.2277 cov=0.4457 std_mean=0.386\n",
      "epoch 530: loss=21.169680 sim=0.0447 std=1.1687 cov=0.5044 std_mean=0.416\n",
      "epoch 540: loss=22.014935 sim=0.0654 std=1.1681 cov=0.5717 std_mean=0.416\n",
      "epoch 550: loss=21.405321 sim=0.0558 std=1.1719 cov=0.4860 std_mean=0.414\n",
      "epoch 560: loss=22.018501 sim=0.0671 std=1.2107 cov=0.4363 std_mean=0.395\n",
      "epoch 570: loss=21.396252 sim=0.0545 std=1.1754 cov=0.4805 std_mean=0.412\n",
      "epoch 580: loss=21.437389 sim=0.0541 std=1.1727 cov=0.4989 std_mean=0.414\n",
      "epoch 590: loss=21.898941 sim=0.0661 std=1.1826 cov=0.5016 std_mean=0.409\n",
      "epoch 600: loss=21.257048 sim=0.0466 std=1.1573 cov=0.5466 std_mean=0.421\n",
      "epoch 610: loss=21.244015 sim=0.0527 std=1.1797 cov=0.4463 std_mean=0.410\n",
      "epoch 620: loss=21.366030 sim=0.0577 std=1.1709 cov=0.4719 std_mean=0.415\n",
      "epoch 630: loss=20.982994 sim=0.0514 std=1.1475 cov=0.4971 std_mean=0.426\n",
      "epoch 640: loss=21.325142 sim=0.0556 std=1.1684 cov=0.4816 std_mean=0.416\n",
      "epoch 650: loss=21.143867 sim=0.0405 std=1.1873 cov=0.4644 std_mean=0.406\n",
      "epoch 660: loss=21.160255 sim=0.0527 std=1.1581 cov=0.4943 std_mean=0.421\n",
      "epoch 670: loss=21.563339 sim=0.0543 std=1.1964 cov=0.4521 std_mean=0.402\n",
      "epoch 680: loss=21.362251 sim=0.0427 std=1.1907 cov=0.4868 std_mean=0.405\n",
      "epoch 690: loss=22.411499 sim=0.0661 std=1.1545 cov=0.6883 std_mean=0.423\n",
      "epoch 700: loss=21.106339 sim=0.0487 std=1.1561 cov=0.5092 std_mean=0.422\n",
      "epoch 710: loss=21.741528 sim=0.0734 std=1.1715 cov=0.4669 std_mean=0.414\n",
      "epoch 720: loss=22.050663 sim=0.0790 std=1.1875 cov=0.4526 std_mean=0.406\n",
      "epoch 730: loss=21.680676 sim=0.0580 std=1.1516 cov=0.5915 std_mean=0.424\n",
      "epoch 740: loss=21.022449 sim=0.0487 std=1.1563 cov=0.4919 std_mean=0.422\n",
      "epoch 750: loss=20.690975 sim=0.0393 std=1.1400 cov=0.5216 std_mean=0.430\n",
      "epoch 760: loss=20.857868 sim=0.0559 std=1.1347 cov=0.4877 std_mean=0.433\n",
      "epoch 770: loss=21.460032 sim=0.0551 std=1.1856 cov=0.4596 std_mean=0.407\n",
      "epoch 780: loss=21.390448 sim=0.0644 std=1.1645 cov=0.4626 std_mean=0.418\n",
      "epoch 790: loss=21.872633 sim=0.0462 std=1.1396 cov=0.7249 std_mean=0.430\n",
      "epoch 800: loss=21.297014 sim=0.0689 std=1.1654 cov=0.4188 std_mean=0.417\n",
      "Predator Encoder trained & frozen.\n"
     ]
    }
   ],
   "source": [
    "aug = TrajectoryAugmentation(noise_std=0.01, neigh_drop=0.10, feat_drop=0.05).to(device)\n",
    "prey_encoder = TransitionEncoder(features=5, embd_dim=32, z=32).to(device)\n",
    "prey_projector = VicRegProjector(input_dim=64).to(device)\n",
    "prey_optimizer = torch.optim.Adam(list(prey_encoder.parameters()) + list(prey_projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "train_encoder(prey_encoder, prey_projector, aug=aug, exp_tensor=exp_prey_tensor, epochs=1200, optimizer=prey_optimizer, role=\"prey\")\n",
    "\n",
    "for p in prey_encoder.parameters():\n",
    "    p.requires_grad = False \n",
    "\n",
    "print(\"Prey Encoder trained & frozen.\\n\")\n",
    "\n",
    "pred_encoder = TransitionEncoder(features=4, embd_dim=32, z=32).to(device)\n",
    "pred_projector = VicRegProjector(input_dim=64).to(device)\n",
    "pred_optimizer = torch.optim.Adam(list(pred_encoder.parameters()) + list(pred_projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "train_encoder(pred_encoder, pred_projector, aug=aug, exp_tensor=exp_pred_tensor, epochs=800, optimizer=pred_optimizer, role=\"predator\")\n",
    "\n",
    "for p in pred_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Predator Encoder trained & frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8b00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_discriminator = Discriminator(encoder=prey_encoder, role=\"prey\", z_dim=32).to(device)\n",
    "prey_discriminator.set_parameters(init=True)\n",
    "optim_disc_prey = torch.optim.RMSprop(prey_discriminator.parameters(), lr=lr_prey_disc, alpha=0.99, eps=1e-08)\n",
    "\n",
    "pred_discriminator = Discriminator(encoder=pred_encoder, role=\"predator\", z_dim=32).to(device)\n",
    "pred_discriminator.set_parameters(init=True)\n",
    "optim_disc_pred = torch.optim.RMSprop(pred_discriminator.parameters(), lr=lr_pred_disc, alpha=0.99, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_reward(discriminator, gen_tensor, mode=\"mean\", lambda_avoid=None, lambda_attack=None):\n",
    "    # get discriminator output matrix\n",
    "    matrix = discriminator(gen_tensor)\n",
    "\n",
    "    # compute mean discriminator reward\n",
    "    dis_reward = matrix.mean(dim=(1, 2))\n",
    "\n",
    "    if mode == \"mean\": # mean discriminator reward\n",
    "        return dis_reward\n",
    "\n",
    "    if mode == \"avoid\" and lambda_avoid is not None: # compute avoidance reward (prey)\n",
    "        # compute euclidean distances\n",
    "        dx = gen_tensor[:, :-1, :, :, 1]\n",
    "        dy = gen_tensor[:, :-1, :, :, 2]\n",
    "        dist = torch.sqrt(dx**2 + dy**2) + 1e-8\n",
    "\n",
    "        # distance to predator\n",
    "        pred_dist = dist[:, :, :, 0]\n",
    "\n",
    "        # compute avoidance reward, higher reward for larger distances\n",
    "        avoid_reward = pred_dist.mean(dim=(1, 2))\n",
    "\n",
    "        # combine rewards\n",
    "        reward = dis_reward + lambda_avoid * avoid_reward\n",
    "        return reward\n",
    "    \n",
    "\n",
    "    if mode == \"attack\" and lambda_attack is not None: # compute attack reward (predator)\n",
    "        # compute euclidean distances\n",
    "        dx = gen_tensor[:, :-1, :, :, 1]\n",
    "        dy = gen_tensor[:, :-1, :, :, 2]\n",
    "        dist = torch.sqrt(dx**2 + dy**2) + 1e-8\n",
    "\n",
    "        # distance to preys\n",
    "        prey_dist = dist[:, :, :, 1:]\n",
    "\n",
    "        # get nearest prey \n",
    "        nearest_prey_dist = prey_dist.min(dim=-1).values\n",
    "\n",
    "        # compute attack reward, gets higher reward for closer distances\n",
    "        attack_reward = (-nearest_prey_dist).mean(dim=(1, 2))\n",
    "\n",
    "        # combine rewards\n",
    "        reward = dis_reward + lambda_attack * attack_reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c169c04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([491, 10, 1, 32, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pred_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff264801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoidance Reward Mean: 0.6321227550506592\n",
      "Discriminator Reward Mean: 0.12390846759080887\n",
      "Combined Reward Mean: 0.2503330111503601\n",
      "\n",
      "Attack Reward Mean: -0.2360522747039795\n",
      "Discriminator Reward Mean: 0.06180749833583832\n",
      "Combined Reward Mean: 0.03820226714015007\n"
     ]
    }
   ],
   "source": [
    "prey_r = discriminator_reward(prey_discriminator, exp_prey_tensor, mode=\"avoid\", lambda_avoid=0.2)\n",
    "pred_r = discriminator_reward(pred_discriminator, exp_pred_tensor, mode=\"attack\", lambda_attack=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
