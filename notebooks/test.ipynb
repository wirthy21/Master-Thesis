{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\anaconda3\\envs\\GAIL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : No C++ compiler found. Define CXX environment variable or install g++.\n",
      "[KeOps] Warning : No C++ compiler found. You need to either define the CXX environment variable pointing to a valid compiler, or ensure that 'g++' is installed and in your PATH.\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n",
      "[KeOps] Warning : No C++ compiler found. You need to either define the CXX environment variable pointing to a valid compiler, or ensure that 'g++' is installed and in your PATH.\n",
      "[KeOps] Warning : No C++ compiler available to check for OpenMP support.\n",
      "[KeOps] Warning : OpenMP support is not available. Disabling OpenMP.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from ema_pytorch import EMA\n",
    "from datetime import datetime\n",
    "from utils.sim_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.train_utils import *\n",
    "from utils.couzin_utils import *\n",
    "from utils.vec_sim_utils import *\n",
    "from utils.encoder_utils import *\n",
    "from utils.dataset_utils import *\n",
    "from geomloss import SamplesLoss\n",
    "from utils.mmd_loss import MMDLoss\n",
    "from models.Generator import ModularPolicy\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert\n",
    "max_steps = 300\n",
    "\n",
    "# Training\n",
    "num_generations = 4000\n",
    "gamma = 0.999\n",
    "deterministic=False # BC pretrain\n",
    "performance_eval = 5\n",
    "num_perturbations = 64\n",
    "\n",
    "### Prey ###\n",
    "lr_prey_policy = 2e-4\n",
    "sigma_prey = 0.1\n",
    "\n",
    "prey_dis_balance_factor = 2\n",
    "prey_noise = 0.005\n",
    "lr_prey_disc = 5e-4\n",
    "lambda_gp_prey = 5\n",
    "prey_update_mode = \"avoid\"\n",
    "\n",
    "\n",
    "### Predator ###\n",
    "lr_pred_policy = 1e-4\n",
    "sigma_pred = 0.08\n",
    "\n",
    "pred_dis_balance_factor = 2\n",
    "pred_noise = 0.005\n",
    "lr_pred_disc = 2e-4\n",
    "lambda_gp_pred = 10\n",
    "pred_update_mode = \"mean\"\n",
    "\n",
    "\n",
    "# Env Settings\n",
    "height = 50\n",
    "width = 50\n",
    "prey_speed = 5\n",
    "pred_speed = 5\n",
    "step_size = 0.5\n",
    "theta_dot_max = 0.5\n",
    "max_turn = float(theta_dot_max * step_size) + 1e-12\n",
    "\n",
    "pert_steps = 100\n",
    "init_steps = 500\n",
    "\n",
    "env_settings = (height, width, prey_speed, pred_speed, step_size, max_turn, pert_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce97496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pred Shape: torch.Size([500, 1, 32, 5])\n",
      "Prey Shape: torch.Size([500, 32, 32, 6])\n",
      "\n",
      "Pred Tensor Shape: torch.Size([491, 10, 1, 32, 5])\n",
      "Prey Tensor Shape: torch.Size([491, 10, 32, 32, 6])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "exp_pred_sequence, exp_prey_sequence, couzin_metrics, actions, init_pool = run_couzin_simulation(\n",
    "                                        visualization=\"off\", \n",
    "                                        max_steps=init_steps, \n",
    "                                        constant_speed=prey_speed, shark_speed=pred_speed, \n",
    "                                        area_width=width, area_height=height,\n",
    "                                        dt = step_size,\n",
    "                                        alpha=0.01,\n",
    "                                        theta_dot_max=theta_dot_max, theta_dot_max_shark=theta_dot_max,\n",
    "                                        number_of_sharks=1, n=32)\n",
    "\n",
    "exp_pred_sequence = exp_pred_sequence.to(device)\n",
    "exp_prey_sequence = exp_prey_sequence.to(device)\n",
    "init_pool = init_pool.to(device)\n",
    "\n",
    "print(\"\\nPred Shape:\", exp_pred_sequence.shape)\n",
    "print(\"Prey Shape:\", exp_prey_sequence.shape)\n",
    "\n",
    "exp_pred_tensor = sliding_window(exp_pred_sequence, window_size=10)\n",
    "exp_prey_tensor = sliding_window(exp_prey_sequence, window_size=10)\n",
    "\n",
    "print(\"\\nPred Tensor Shape:\", exp_pred_tensor.shape)\n",
    "print(\"Prey Tensor Shape:\", exp_prey_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8323ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010: loss=22.635822 sim=0.0469 std=1.2981 cov=0.3984 std_mean=0.351\n",
      "epoch 020: loss=21.943047 sim=0.0436 std=1.2534 cov=0.4103 std_mean=0.373\n",
      "epoch 030: loss=21.222828 sim=0.0426 std=1.2041 cov=0.4190 std_mean=0.398\n",
      "epoch 040: loss=20.749269 sim=0.0434 std=1.1622 cov=0.4463 std_mean=0.419\n",
      "epoch 050: loss=20.569157 sim=0.0488 std=1.1379 cov=0.4563 std_mean=0.431\n",
      "epoch 060: loss=20.033260 sim=0.0433 std=1.0872 cov=0.5288 std_mean=0.456\n",
      "epoch 070: loss=19.889326 sim=0.0513 std=1.0633 cov=0.5313 std_mean=0.468\n",
      "epoch 080: loss=19.827898 sim=0.0542 std=1.0590 cov=0.5174 std_mean=0.471\n",
      "epoch 090: loss=19.882730 sim=0.0570 std=1.0394 cov=0.5731 std_mean=0.480\n",
      "epoch 100: loss=19.722971 sim=0.0530 std=1.0380 cov=0.5655 std_mean=0.481\n",
      "epoch 110: loss=19.772097 sim=0.0574 std=1.0335 cov=0.5668 std_mean=0.483\n",
      "epoch 120: loss=19.629839 sim=0.0577 std=1.0246 cov=0.5635 std_mean=0.488\n",
      "epoch 130: loss=19.511559 sim=0.0598 std=1.0127 cov=0.5650 std_mean=0.494\n",
      "epoch 140: loss=19.472654 sim=0.0605 std=1.0004 cov=0.5906 std_mean=0.500\n",
      "epoch 150: loss=19.735340 sim=0.0709 std=1.0147 cov=0.5487 std_mean=0.493\n",
      "epoch 160: loss=19.391655 sim=0.0609 std=0.9989 cov=0.5771 std_mean=0.501\n",
      "epoch 170: loss=19.467102 sim=0.0631 std=0.9951 cov=0.5925 std_mean=0.502\n",
      "epoch 180: loss=19.331102 sim=0.0629 std=0.9963 cov=0.5626 std_mean=0.502\n",
      "epoch 190: loss=19.455635 sim=0.0674 std=0.9905 cov=0.5827 std_mean=0.505\n",
      "epoch 200: loss=19.277435 sim=0.0597 std=0.9975 cov=0.5648 std_mean=0.501\n",
      "epoch 210: loss=19.411135 sim=0.0633 std=0.9908 cov=0.5933 std_mean=0.505\n",
      "epoch 220: loss=19.247091 sim=0.0631 std=0.9863 cov=0.5752 std_mean=0.507\n",
      "epoch 230: loss=19.567297 sim=0.0685 std=0.9963 cov=0.5823 std_mean=0.502\n",
      "epoch 240: loss=19.464081 sim=0.0692 std=0.9890 cov=0.5798 std_mean=0.505\n",
      "epoch 250: loss=19.396959 sim=0.0693 std=1.0002 cov=0.5322 std_mean=0.500\n",
      "epoch 260: loss=19.713949 sim=0.0763 std=1.0036 cov=0.5505 std_mean=0.498\n",
      "epoch 270: loss=19.523767 sim=0.0679 std=1.0027 cov=0.5570 std_mean=0.499\n",
      "epoch 280: loss=19.286341 sim=0.0637 std=0.9815 cov=0.5941 std_mean=0.509\n",
      "epoch 290: loss=19.355844 sim=0.0710 std=0.9970 cov=0.5248 std_mean=0.501\n",
      "epoch 300: loss=19.085693 sim=0.0620 std=0.9688 cov=0.6010 std_mean=0.516\n",
      "epoch 310: loss=19.068188 sim=0.0690 std=0.9584 cov=0.5935 std_mean=0.521\n",
      "epoch 320: loss=18.962910 sim=0.0675 std=0.9716 cov=0.5405 std_mean=0.514\n",
      "epoch 330: loss=18.895119 sim=0.0624 std=0.9718 cov=0.5514 std_mean=0.514\n",
      "epoch 340: loss=18.887762 sim=0.0650 std=0.9610 cov=0.5698 std_mean=0.520\n",
      "epoch 350: loss=18.714357 sim=0.0674 std=0.9546 cov=0.5423 std_mean=0.523\n",
      "epoch 360: loss=18.716242 sim=0.0682 std=0.9437 cov=0.5713 std_mean=0.528\n",
      "epoch 370: loss=18.528484 sim=0.0687 std=0.9310 cov=0.5690 std_mean=0.534\n",
      "epoch 380: loss=18.839090 sim=0.0686 std=0.9479 cov=0.5814 std_mean=0.526\n",
      "epoch 390: loss=18.828873 sim=0.0709 std=0.9322 cov=0.6146 std_mean=0.534\n",
      "epoch 400: loss=18.468019 sim=0.0717 std=0.9205 cov=0.5732 std_mean=0.540\n",
      "epoch 410: loss=18.464460 sim=0.0692 std=0.9197 cov=0.5878 std_mean=0.540\n",
      "epoch 420: loss=18.743715 sim=0.0781 std=0.9092 cov=0.6304 std_mean=0.545\n",
      "epoch 430: loss=18.330673 sim=0.0715 std=0.9015 cov=0.6038 std_mean=0.549\n",
      "epoch 440: loss=18.360067 sim=0.0801 std=0.8903 cov=0.6007 std_mean=0.555\n",
      "epoch 450: loss=18.124945 sim=0.0733 std=0.8869 cov=0.5981 std_mean=0.557\n",
      "epoch 460: loss=18.379406 sim=0.0816 std=0.8777 cov=0.6348 std_mean=0.561\n",
      "epoch 470: loss=18.466339 sim=0.0788 std=0.9121 cov=0.5632 std_mean=0.544\n",
      "epoch 480: loss=18.008390 sim=0.0746 std=0.8753 cov=0.6028 std_mean=0.562\n",
      "epoch 490: loss=18.160040 sim=0.0726 std=0.8771 cov=0.6375 std_mean=0.561\n",
      "epoch 500: loss=17.972090 sim=0.0772 std=0.8797 cov=0.5692 std_mean=0.560\n",
      "epoch 510: loss=18.003223 sim=0.0796 std=0.8634 cov=0.6126 std_mean=0.568\n",
      "epoch 520: loss=18.012634 sim=0.0813 std=0.8626 cov=0.6082 std_mean=0.569\n",
      "epoch 530: loss=18.137354 sim=0.0842 std=0.8629 cov=0.6178 std_mean=0.569\n",
      "epoch 540: loss=17.719809 sim=0.0738 std=0.8551 cov=0.6095 std_mean=0.572\n",
      "epoch 550: loss=18.050583 sim=0.0895 std=0.8459 cov=0.6247 std_mean=0.577\n",
      "epoch 560: loss=17.731823 sim=0.0837 std=0.8466 cov=0.5878 std_mean=0.577\n",
      "epoch 570: loss=17.974483 sim=0.0927 std=0.8382 cov=0.6165 std_mean=0.581\n",
      "epoch 580: loss=17.660898 sim=0.0833 std=0.8292 cov=0.6282 std_mean=0.585\n",
      "epoch 590: loss=17.988949 sim=0.0839 std=0.8298 cov=0.6892 std_mean=0.585\n",
      "epoch 600: loss=17.466993 sim=0.0821 std=0.8151 cov=0.6376 std_mean=0.592\n",
      "epoch 610: loss=17.427666 sim=0.0840 std=0.8137 cov=0.6244 std_mean=0.593\n",
      "epoch 620: loss=17.432058 sim=0.0869 std=0.8151 cov=0.6066 std_mean=0.592\n",
      "epoch 630: loss=17.639565 sim=0.0831 std=0.8201 cov=0.6520 std_mean=0.590\n",
      "epoch 640: loss=17.359638 sim=0.0850 std=0.8064 cov=0.6278 std_mean=0.597\n",
      "epoch 650: loss=17.824650 sim=0.0946 std=0.8080 cov=0.6677 std_mean=0.596\n",
      "epoch 660: loss=17.660530 sim=0.0940 std=0.8027 cov=0.6536 std_mean=0.599\n",
      "epoch 670: loss=17.323269 sim=0.0827 std=0.8094 cov=0.6227 std_mean=0.595\n",
      "epoch 680: loss=17.581785 sim=0.0915 std=0.7963 cov=0.6700 std_mean=0.602\n",
      "epoch 690: loss=17.290436 sim=0.0919 std=0.7837 cov=0.6476 std_mean=0.608\n",
      "epoch 700: loss=17.756973 sim=0.0966 std=0.7976 cov=0.6755 std_mean=0.601\n",
      "epoch 710: loss=17.213753 sim=0.0953 std=0.7774 cov=0.6342 std_mean=0.611\n",
      "epoch 720: loss=17.430405 sim=0.0933 std=0.7874 cov=0.6576 std_mean=0.606\n",
      "epoch 730: loss=17.081673 sim=0.0938 std=0.7717 cov=0.6320 std_mean=0.614\n",
      "epoch 740: loss=17.070951 sim=0.0901 std=0.7772 cov=0.6321 std_mean=0.611\n",
      "epoch 750: loss=17.279413 sim=0.0921 std=0.7825 cov=0.6477 std_mean=0.609\n",
      "epoch 760: loss=17.260456 sim=0.0979 std=0.7712 cov=0.6490 std_mean=0.614\n",
      "epoch 770: loss=17.094372 sim=0.0899 std=0.7776 cov=0.6365 std_mean=0.611\n",
      "epoch 780: loss=16.907906 sim=0.0901 std=0.7635 cov=0.6408 std_mean=0.618\n",
      "epoch 790: loss=17.067274 sim=0.1016 std=0.7608 cov=0.6228 std_mean=0.620\n",
      "epoch 800: loss=16.981230 sim=0.0968 std=0.7515 cov=0.6577 std_mean=0.624\n",
      "epoch 810: loss=16.892242 sim=0.0976 std=0.7524 cov=0.6334 std_mean=0.624\n",
      "epoch 820: loss=16.808580 sim=0.0901 std=0.7437 cov=0.6800 std_mean=0.628\n",
      "epoch 830: loss=17.304743 sim=0.1029 std=0.7606 cov=0.6645 std_mean=0.620\n",
      "epoch 840: loss=16.705997 sim=0.0843 std=0.7468 cov=0.6792 std_mean=0.627\n",
      "epoch 850: loss=16.970705 sim=0.0998 std=0.7321 cov=0.6989 std_mean=0.634\n",
      "epoch 860: loss=16.761333 sim=0.0977 std=0.7228 cov=0.6952 std_mean=0.639\n",
      "epoch 870: loss=17.076481 sim=0.0952 std=0.7296 cov=0.7506 std_mean=0.635\n",
      "epoch 880: loss=17.215410 sim=0.1102 std=0.7537 cov=0.6312 std_mean=0.623\n",
      "epoch 890: loss=16.824043 sim=0.0995 std=0.7271 cov=0.6858 std_mean=0.636\n",
      "epoch 900: loss=16.929094 sim=0.1057 std=0.7315 cov=0.6629 std_mean=0.634\n",
      "epoch 910: loss=16.618126 sim=0.0989 std=0.7276 cov=0.6461 std_mean=0.636\n",
      "epoch 920: loss=16.307556 sim=0.0921 std=0.7143 cov=0.6580 std_mean=0.643\n",
      "epoch 930: loss=16.667521 sim=0.1012 std=0.7149 cov=0.6825 std_mean=0.643\n",
      "epoch 940: loss=16.608906 sim=0.1009 std=0.7131 cov=0.6780 std_mean=0.643\n",
      "epoch 950: loss=16.301260 sim=0.1001 std=0.6926 cov=0.6820 std_mean=0.654\n",
      "epoch 960: loss=16.404913 sim=0.0991 std=0.7123 cov=0.6482 std_mean=0.644\n",
      "epoch 970: loss=16.479465 sim=0.0916 std=0.6955 cov=0.7516 std_mean=0.652\n",
      "epoch 980: loss=16.515606 sim=0.1094 std=0.6942 cov=0.6737 std_mean=0.653\n",
      "epoch 990: loss=16.633917 sim=0.1057 std=0.7116 cov=0.6635 std_mean=0.644\n",
      "epoch 1000: loss=16.573511 sim=0.1097 std=0.7024 cov=0.6589 std_mean=0.649\n",
      "epoch 1010: loss=16.160219 sim=0.0977 std=0.6855 cov=0.6870 std_mean=0.657\n",
      "epoch 1020: loss=15.986603 sim=0.1021 std=0.6832 cov=0.6371 std_mean=0.658\n",
      "epoch 1030: loss=15.859451 sim=0.0931 std=0.6710 cov=0.6933 std_mean=0.664\n",
      "epoch 1040: loss=16.036865 sim=0.1006 std=0.6808 cov=0.6617 std_mean=0.660\n",
      "epoch 1050: loss=16.416931 sim=0.1116 std=0.6656 cov=0.7283 std_mean=0.667\n",
      "epoch 1060: loss=16.287626 sim=0.1105 std=0.6753 cov=0.6791 std_mean=0.662\n",
      "epoch 1070: loss=16.246738 sim=0.1053 std=0.6661 cov=0.7245 std_mean=0.667\n",
      "epoch 1080: loss=15.778582 sim=0.1016 std=0.6573 cov=0.6759 std_mean=0.671\n",
      "epoch 1090: loss=16.123205 sim=0.1050 std=0.6683 cov=0.6949 std_mean=0.666\n",
      "epoch 1100: loss=16.080561 sim=0.1067 std=0.6590 cov=0.7057 std_mean=0.670\n",
      "epoch 1110: loss=15.780142 sim=0.0986 std=0.6632 cov=0.6734 std_mean=0.668\n",
      "epoch 1120: loss=15.802330 sim=0.0950 std=0.6656 cov=0.6887 std_mean=0.667\n",
      "epoch 1130: loss=15.903933 sim=0.1084 std=0.6520 cov=0.6830 std_mean=0.674\n",
      "epoch 1140: loss=15.956068 sim=0.1096 std=0.6520 cov=0.6870 std_mean=0.674\n",
      "epoch 1150: loss=15.650875 sim=0.1022 std=0.6501 cov=0.6690 std_mean=0.675\n",
      "epoch 1160: loss=16.204655 sim=0.1081 std=0.6615 cov=0.7161 std_mean=0.669\n",
      "epoch 1170: loss=15.974590 sim=0.1016 std=0.6443 cov=0.7541 std_mean=0.678\n",
      "epoch 1180: loss=15.762765 sim=0.1134 std=0.6356 cov=0.6788 std_mean=0.682\n",
      "epoch 1190: loss=15.675018 sim=0.1134 std=0.6315 cov=0.6735 std_mean=0.684\n",
      "epoch 1200: loss=15.969654 sim=0.1106 std=0.6212 cov=0.7774 std_mean=0.689\n",
      "Prey Encoder trained & frozen.\n",
      "\n",
      "epoch 010: loss=23.359077 sim=0.0494 std=1.3562 cov=0.3563 std_mean=0.322\n",
      "epoch 020: loss=23.159571 sim=0.0517 std=1.3719 cov=0.2575 std_mean=0.314\n",
      "epoch 030: loss=22.793161 sim=0.0485 std=1.3235 cov=0.3459 std_mean=0.338\n",
      "epoch 040: loss=22.588926 sim=0.0368 std=1.3163 cov=0.3850 std_mean=0.342\n",
      "epoch 050: loss=22.867100 sim=0.0499 std=1.2803 cov=0.4828 std_mean=0.360\n",
      "epoch 060: loss=22.236839 sim=0.0333 std=1.3001 cov=0.3802 std_mean=0.350\n",
      "epoch 070: loss=22.137688 sim=0.0311 std=1.2600 cov=0.4917 std_mean=0.370\n",
      "epoch 080: loss=22.180273 sim=0.0416 std=1.2768 cov=0.3977 std_mean=0.362\n",
      "epoch 090: loss=22.061440 sim=0.0435 std=1.2701 cov=0.3842 std_mean=0.365\n",
      "epoch 100: loss=22.525124 sim=0.0466 std=1.2572 cov=0.5003 std_mean=0.371\n",
      "epoch 110: loss=22.310787 sim=0.0476 std=1.2330 cov=0.5251 std_mean=0.384\n",
      "epoch 120: loss=21.905010 sim=0.0587 std=1.2333 cov=0.3875 std_mean=0.383\n",
      "epoch 130: loss=22.666801 sim=0.0807 std=1.2290 cov=0.4430 std_mean=0.386\n",
      "epoch 140: loss=21.541122 sim=0.0404 std=1.2464 cov=0.3669 std_mean=0.377\n",
      "epoch 150: loss=21.894102 sim=0.0386 std=1.2385 cov=0.4706 std_mean=0.381\n",
      "epoch 160: loss=21.537121 sim=0.0544 std=1.2147 cov=0.3914 std_mean=0.393\n",
      "epoch 170: loss=21.304716 sim=0.0368 std=1.2112 cov=0.4432 std_mean=0.394\n",
      "epoch 180: loss=21.852291 sim=0.0589 std=1.2182 cov=0.4215 std_mean=0.391\n",
      "epoch 190: loss=21.724237 sim=0.0359 std=1.2105 cov=0.5338 std_mean=0.395\n",
      "epoch 200: loss=21.680967 sim=0.0573 std=1.1796 cov=0.5106 std_mean=0.410\n",
      "epoch 210: loss=21.541773 sim=0.0441 std=1.2247 cov=0.4136 std_mean=0.388\n",
      "epoch 220: loss=21.616129 sim=0.0485 std=1.1922 cov=0.5040 std_mean=0.404\n",
      "epoch 230: loss=21.687593 sim=0.0454 std=1.2014 cov=0.5066 std_mean=0.399\n",
      "epoch 240: loss=21.748579 sim=0.0492 std=1.2070 cov=0.4824 std_mean=0.396\n",
      "epoch 250: loss=22.039761 sim=0.0516 std=1.2055 cov=0.5337 std_mean=0.397\n",
      "epoch 260: loss=21.868958 sim=0.0519 std=1.2061 cov=0.4962 std_mean=0.397\n",
      "epoch 270: loss=22.239326 sim=0.0808 std=1.1847 cov=0.4898 std_mean=0.408\n",
      "epoch 280: loss=21.296761 sim=0.0417 std=1.1688 cov=0.5444 std_mean=0.416\n",
      "epoch 290: loss=21.422894 sim=0.0537 std=1.1886 cov=0.4503 std_mean=0.406\n",
      "epoch 300: loss=20.862106 sim=0.0342 std=1.1795 cov=0.4626 std_mean=0.410\n",
      "epoch 310: loss=20.784092 sim=0.0384 std=1.1689 cov=0.4581 std_mean=0.416\n",
      "epoch 320: loss=21.129864 sim=0.0425 std=1.1737 cov=0.4925 std_mean=0.413\n",
      "epoch 330: loss=21.585596 sim=0.0595 std=1.1913 cov=0.4458 std_mean=0.404\n",
      "epoch 340: loss=21.874987 sim=0.0481 std=1.1738 cov=0.6133 std_mean=0.413\n",
      "epoch 350: loss=21.331949 sim=0.0487 std=1.1887 cov=0.4570 std_mean=0.406\n",
      "epoch 360: loss=21.159344 sim=0.0478 std=1.1760 cov=0.4649 std_mean=0.412\n",
      "epoch 370: loss=21.028397 sim=0.0456 std=1.1727 cov=0.4594 std_mean=0.414\n",
      "epoch 380: loss=21.463320 sim=0.0526 std=1.1794 cov=0.4914 std_mean=0.410\n",
      "epoch 390: loss=21.567148 sim=0.0497 std=1.1531 cov=0.6054 std_mean=0.423\n",
      "epoch 400: loss=21.757069 sim=0.0538 std=1.1732 cov=0.5628 std_mean=0.413\n",
      "epoch 410: loss=20.995840 sim=0.0457 std=1.1707 cov=0.4587 std_mean=0.415\n",
      "epoch 420: loss=21.391407 sim=0.0596 std=1.1689 cov=0.4736 std_mean=0.416\n",
      "epoch 430: loss=21.248413 sim=0.0543 std=1.1711 cov=0.4649 std_mean=0.414\n",
      "epoch 440: loss=21.013876 sim=0.0498 std=1.1594 cov=0.4755 std_mean=0.420\n",
      "epoch 450: loss=21.115515 sim=0.0524 std=1.1559 cov=0.4933 std_mean=0.422\n",
      "epoch 460: loss=21.408628 sim=0.0514 std=1.1769 cov=0.4940 std_mean=0.412\n",
      "epoch 470: loss=21.967142 sim=0.0674 std=1.1963 cov=0.4676 std_mean=0.402\n",
      "epoch 480: loss=21.590000 sim=0.0637 std=1.1885 cov=0.4342 std_mean=0.406\n",
      "epoch 490: loss=21.566950 sim=0.0698 std=1.1742 cov=0.4417 std_mean=0.413\n",
      "epoch 500: loss=21.433355 sim=0.0504 std=1.1780 cov=0.5009 std_mean=0.411\n",
      "epoch 510: loss=21.527184 sim=0.0703 std=1.1598 cov=0.4747 std_mean=0.420\n",
      "epoch 520: loss=20.989857 sim=0.0426 std=1.1303 cov=0.5943 std_mean=0.435\n",
      "epoch 530: loss=21.297718 sim=0.0540 std=1.1932 cov=0.4097 std_mean=0.403\n",
      "epoch 540: loss=21.060038 sim=0.0489 std=1.1810 cov=0.4248 std_mean=0.410\n",
      "epoch 550: loss=20.861698 sim=0.0492 std=1.1652 cov=0.4308 std_mean=0.417\n",
      "epoch 560: loss=21.554146 sim=0.0577 std=1.1549 cov=0.5574 std_mean=0.423\n",
      "epoch 570: loss=21.350891 sim=0.0604 std=1.1605 cov=0.4870 std_mean=0.420\n",
      "epoch 580: loss=21.373552 sim=0.0433 std=1.1828 cov=0.5095 std_mean=0.409\n",
      "epoch 590: loss=21.462664 sim=0.0521 std=1.1637 cov=0.5410 std_mean=0.418\n",
      "epoch 600: loss=21.943130 sim=0.0677 std=1.1586 cov=0.5746 std_mean=0.421\n",
      "epoch 610: loss=21.122869 sim=0.0469 std=1.1757 cov=0.4629 std_mean=0.412\n",
      "epoch 620: loss=21.283073 sim=0.0567 std=1.1512 cov=0.5194 std_mean=0.424\n",
      "epoch 630: loss=21.313858 sim=0.0451 std=1.1669 cov=0.5367 std_mean=0.417\n",
      "epoch 640: loss=21.344521 sim=0.0488 std=1.1650 cov=0.5302 std_mean=0.418\n",
      "epoch 650: loss=20.660852 sim=0.0384 std=1.1488 cov=0.4938 std_mean=0.426\n",
      "epoch 660: loss=20.725849 sim=0.0368 std=1.1556 cov=0.4942 std_mean=0.422\n",
      "epoch 670: loss=20.772364 sim=0.0489 std=1.1376 cov=0.4972 std_mean=0.431\n",
      "epoch 680: loss=21.462290 sim=0.0609 std=1.1774 cov=0.4557 std_mean=0.411\n",
      "epoch 690: loss=21.426022 sim=0.0635 std=1.1432 cov=0.5384 std_mean=0.428\n",
      "epoch 700: loss=21.252312 sim=0.0634 std=1.1489 cov=0.4867 std_mean=0.426\n",
      "epoch 710: loss=21.479939 sim=0.0697 std=1.1328 cov=0.5492 std_mean=0.434\n",
      "epoch 720: loss=21.079805 sim=0.0476 std=1.1384 cov=0.5627 std_mean=0.431\n",
      "epoch 730: loss=20.702511 sim=0.0311 std=1.1675 cov=0.4826 std_mean=0.416\n",
      "epoch 740: loss=21.000187 sim=0.0494 std=1.1449 cov=0.5182 std_mean=0.428\n",
      "epoch 750: loss=21.639763 sim=0.0540 std=1.1716 cov=0.5431 std_mean=0.414\n",
      "epoch 760: loss=21.511612 sim=0.0475 std=1.1510 cov=0.6115 std_mean=0.424\n",
      "epoch 770: loss=21.368753 sim=0.0518 std=1.1611 cov=0.5311 std_mean=0.419\n",
      "epoch 780: loss=21.152054 sim=0.0650 std=1.1472 cov=0.4638 std_mean=0.426\n",
      "epoch 790: loss=20.777576 sim=0.0388 std=1.1458 cov=0.5241 std_mean=0.427\n",
      "epoch 800: loss=20.899057 sim=0.0501 std=1.1460 cov=0.4912 std_mean=0.427\n",
      "Predator Encoder trained & frozen.\n"
     ]
    }
   ],
   "source": [
    "aug = TrajectoryAugmentation(noise_std=0.01, neigh_drop=0.10, feat_drop=0.05).to(device)\n",
    "prey_encoder = TransitionEncoder(features=5, embd_dim=32, z=32).to(device)\n",
    "prey_projector = VicRegProjector(input_dim=64).to(device)\n",
    "prey_optimizer = torch.optim.Adam(list(prey_encoder.parameters()) + list(prey_projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "train_encoder(prey_encoder, prey_projector, aug=aug, exp_tensor=exp_prey_tensor, epochs=1200, optimizer=prey_optimizer, role=\"prey\")\n",
    "\n",
    "for p in prey_encoder.parameters():\n",
    "    p.requires_grad = False \n",
    "\n",
    "print(\"Prey Encoder trained & frozen.\\n\")\n",
    "\n",
    "pred_encoder = TransitionEncoder(features=4, embd_dim=32, z=32).to(device)\n",
    "pred_projector = VicRegProjector(input_dim=64).to(device)\n",
    "pred_optimizer = torch.optim.Adam(list(pred_encoder.parameters()) + list(pred_projector.parameters()), lr=1e-3, weight_decay=1e-6)\n",
    "train_encoder(pred_encoder, pred_projector, aug=aug, exp_tensor=exp_pred_tensor, epochs=800, optimizer=pred_optimizer, role=\"predator\")\n",
    "\n",
    "for p in pred_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Predator Encoder trained & frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8b00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_discriminator = Discriminator(encoder=prey_encoder, role=\"prey\", z_dim=32).to(device)\n",
    "prey_discriminator.set_parameters(init=True)\n",
    "optim_disc_prey = torch.optim.RMSprop(prey_discriminator.parameters(), lr=lr_prey_disc, alpha=0.99, eps=1e-08)\n",
    "\n",
    "pred_discriminator = Discriminator(encoder=pred_encoder, role=\"predator\", z_dim=32).to(device)\n",
    "pred_discriminator.set_parameters(init=True)\n",
    "optim_disc_pred = torch.optim.RMSprop(pred_discriminator.parameters(), lr=lr_pred_disc, alpha=0.99, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9bc5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_reward(discriminator, gen_tensor, mode=\"mean\"):\n",
    "    matrix = discriminator(gen_tensor)\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        return matrix.mean(dim=(1, 2))\n",
    "\n",
    "    if mode == \"avoid\":\n",
    "        dis_reward = matrix.mean(dim=(1, 2))\n",
    "        print(\"Discriminator Reward Mean:\", dis_reward.mean().item())\n",
    "\n",
    "        dx = gen_tensor[:, :-1, :, :, 1]\n",
    "        dy = gen_tensor[:, :-1, :, :, 2]\n",
    "        print(\"DX Mean:\", dx.mean().item())\n",
    "        print(\"DY Mean:\", dy.mean().item())\n",
    "\n",
    "        dist = torch.sqrt(dx**2 + dy**2)\n",
    "        print(\"Distance Mean:\", dist.mean().item())\n",
    "        pred_dist = dist[:, :, :, 0]\n",
    "\n",
    "\n",
    "        alpha_coeff=1.0 \n",
    "        r_avoid=0.12 \n",
    "        eps=1e-8\n",
    "\n",
    "        avoid_reward = (-torch.relu(r_avoid - pred_dist)).mean(dim=(1, 2))\n",
    "        print(\"Avoidance Reward Mean:\", avoid_reward.mean().item())\n",
    "\n",
    "        avoid_centered = avoid_reward - avoid_reward.mean().detach()\n",
    "\n",
    "\n",
    "        dis_scale = dis_reward.detach().std().clamp(min=eps)  \n",
    "        avoid_scale = avoid_centered.detach().std().clamp(min=eps)\n",
    "        print(\"Discriminator Reward Scale:\", dis_scale.item())\n",
    "        print(\"Avoidance Reward Scale:\", avoid_scale.item())\n",
    "\n",
    "        alpha = alpha_coeff * dis_scale / avoid_scale\n",
    "        print(\"Alpha:\", alpha.item())\n",
    "\n",
    "        reward = dis_reward + alpha * avoid_reward\n",
    "        print(\"Total Reward Mean:\", reward.mean().item())\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff264801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Reward Mean: -0.010941068641841412\n",
      "DX Mean: 0.0007546551642008126\n",
      "DY Mean: 0.0019254297949373722\n",
      "Distance Mean: 0.46906596422195435\n",
      "Avoidance Reward Mean: -0.0012265769764780998\n",
      "Discriminator Reward Scale: 4.19268362747971e-05\n",
      "Avoidance Reward Scale: 0.0014619167195633054\n",
      "Alpha: 0.028679359704256058\n",
      "Total Reward Mean: -0.010976246558129787\n"
     ]
    }
   ],
   "source": [
    "prey_r = discriminator_reward(prey_discriminator, exp_prey_tensor, mode=\"avoid\")\n",
    "pred_r = discriminator_reward(pred_discriminator, exp_pred_tensor, mode=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0624a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def discriminator_reward(\n",
    "    discriminator,\n",
    "    gen_tensor,\n",
    "    mode=\"mean\",\n",
    "    alpha_coeff=1.0,\n",
    "    r_avoid=0.5,\n",
    "    r_avoid_quantile=None,\n",
    "    tau_attack=0.05,\n",
    "    scale_mode=\"meanabs\",\n",
    "    center=True,\n",
    "    eps=1e-8,\n",
    "):\n",
    "    matrix = discriminator(gen_tensor)\n",
    "    dis_reward = matrix.mean(dim=(1, 2))\n",
    "\n",
    "    def _scale(x):\n",
    "        if scale_mode == \"std\":\n",
    "            return x.detach().std().clamp(min=eps)\n",
    "        return x.detach().abs().mean().clamp(min=eps)\n",
    "\n",
    "    if mode == \"mean\":\n",
    "        return dis_reward\n",
    "\n",
    "    gt = gen_tensor[:, :-1]\n",
    "    feat_dim = gen_tensor.shape[-1]\n",
    "\n",
    "    if mode == \"avoid\":\n",
    "        dx = gt[..., 1]\n",
    "        dy = gt[..., 2]\n",
    "        dist = torch.sqrt(dx * dx + dy * dy + eps)\n",
    "        pred_dist = dist[..., 0]\n",
    "\n",
    "        if r_avoid_quantile is not None:\n",
    "            r_avoid = torch.quantile(pred_dist.reshape(-1), float(r_avoid_quantile)).item()\n",
    "\n",
    "        avoid_reward = (-torch.relu(r_avoid - pred_dist)).mean(dim=(1, 2))\n",
    "        term = avoid_reward - avoid_reward.mean().detach() if center else avoid_reward\n",
    "\n",
    "        alpha = alpha_coeff * (_scale(dis_reward) / _scale(term))\n",
    "        return dis_reward + alpha * term\n",
    "\n",
    "    if mode == \"attack\":\n",
    "        dx = gt[..., 0]\n",
    "        dy = gt[..., 1]\n",
    "        dist = torch.sqrt(dx * dx + dy * dy + eps)\n",
    "\n",
    "        softmin = -tau_attack * torch.logsumexp(-dist / tau_attack, dim=-1)\n",
    "        attack_reward = (-softmin).mean(dim=(1, 2))\n",
    "\n",
    "        term = attack_reward - attack_reward.mean().detach() if center else attack_reward\n",
    "\n",
    "        alpha = alpha_coeff * (_scale(dis_reward) / _scale(term))\n",
    "        return dis_reward + alpha * term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f294d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prey_r = discriminator_reward(prey_discriminator, exp_prey_tensor, mode=\"avoid\")\n",
    "pred_r = discriminator_reward(pred_discriminator, exp_pred_tensor, mode=\"attack\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
