{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3666eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "from utils.es_utils import *\n",
    "from utils.env_utils import *\n",
    "from utils.train_utils import *\n",
    "from models.Buffer import Buffer\n",
    "from models.PredatorPolicy import PredatorPolicy\n",
    "from models.PreyPolicy import PreyPolicy\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from models.Buffer import Pool\n",
    "from utils.env_utils import *\n",
    "from utils.eval_utils import *\n",
    "from utils.train_utils import pretrain_policy_with_validation\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import scipy.stats\n",
    "import torch.nn as nn\n",
    "from utils.es_utils import *\n",
    "from utils.env_utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from multiprocessing import Pool, set_start_method\n",
    "from models.ModularNetworks import PairwiseInteraction, Attention, PredatorInteraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba0100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert Data\n",
    "traj_path = rf'..\\data\\1. Data Processing\\processed\\video\\expert_tensors\\yolo_detected'\n",
    "couzin_path = rf'..\\data\\1. Data Processing\\processed\\couzin'\n",
    "hl_path = rf'..\\data\\1. Data Processing\\processed\\video\\expert_tensors\\hand_labeled'\n",
    "ftw_path = rf'..\\data\\1. Data Processing\\processed\\video\\3. full_track_windows'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50cac05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_buffer = Buffer(pred_max_length=23000, prey_max_length=200000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b18fb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gail_folder = \"GAIL Training - 19.11.2025_18.52 - Couzin Data\"\n",
    "bc_folder = \"BC Training - 19.11.2025_18.52 - Couzin Data\"\n",
    "\n",
    "model_folder = rf\"..\\data\\2. Training\\training\"\n",
    "gail_path = os.path.join(model_folder, \"GAIL\", gail_folder)\n",
    "bc_path = os.path.join(model_folder, \"BC\", bc_folder)\n",
    "pred_policy = torch.load(os.path.join(gail_path, \"gail_pred_policy.pt\"), weights_only=False)\n",
    "\n",
    "prey_policy = PreyPolicy().to(device)\n",
    "prey_policy.set_parameters(init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69afe642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Buffer is empty, load data...\n",
      "Storage of Predator Expert Buffer:  1000\n",
      "Storage of Prey Expert Buffer:  32000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Expert Data from local storage\n",
    "print(\"Expert Buffer is empty, load data...\")\n",
    "expert_buffer.add_expert(couzin_path)\n",
    "\n",
    "len_exp_pred, len_exp_prey = expert_buffer.lengths()\n",
    "print(\"Storage of Predator Expert Buffer: \", len_exp_pred)\n",
    "print(\"Storage of Prey Expert Buffer: \", len_exp_prey, \"\\n\")\n",
    "\n",
    "_, expert_prey_batch = expert_buffer.sample(1, 1)\n",
    "states = expert_prey_batch[..., :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5417312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Gain from Attention Weights:  tensor([0.2459], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.9500, 0.2109, 0.2646, 0.1213, 0.2121, 0.2457, 0.3799, 0.1408, 0.1417,\n",
      "         0.1917, 0.3720, 0.2876, 0.4599, 0.0622, 0.2665, 0.0890, 0.1499, 0.2622,\n",
      "         0.3348, 0.1979, 0.1511, 0.2943, 0.1040, 0.3105, 0.1617, 0.3126, 0.2420,\n",
      "         0.2925, 0.0826, 0.1810, 0.3447, 0.0500]], grad_fn=<AddBackward0>)\n",
      "tensor(0.2459, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.7072], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_pred_gain(states):\n",
    "    weights = torch.softmax(pred_policy.attention(states).view(1, 32), dim=1)\n",
    "    w_min = weights.min()\n",
    "    w_max = weights.max()\n",
    "    scaled_weights = (weights - w_min) / (w_max - w_min) * 0.9 + 0.05  # scale to [0.05, 0.95]\n",
    "    return scaled_weights\n",
    "\n",
    "action_prey, mu_prey, sigma_prey, weights_prey, pred_gain = prey_policy.forward(states, weights)\n",
    "print(weights)\n",
    "print(weights.mean())\n",
    "print(action_prey)\n",
    "\n",
    "# clip auf 0.05 - 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2af96559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Actions:  tensor([-2.4492])\n",
      "Pred Gain:  tensor([[0.9500, 0.2109, 0.2646, 0.1213, 0.2121, 0.2457, 0.3799, 0.1408, 0.1417,\n",
      "         0.1917, 0.3720, 0.2876, 0.4599, 0.0622, 0.2665, 0.0890, 0.1499, 0.2622,\n",
      "         0.3348, 0.1979, 0.1511, 0.2943, 0.1040, 0.3105, 0.1617, 0.3126, 0.2420,\n",
      "         0.2925, 0.0826, 0.1810, 0.3447, 0.0500]], grad_fn=<AddBackward0>)\n",
      "Prey Actions:  tensor([[ 1.8440e+00, -2.5470e+00, -1.2248e+00, -1.4999e+00, -3.8322e-01,\n",
      "         -9.0433e-01, -1.0150e+00,  2.7126e+00, -1.9035e+00,  2.6387e+00,\n",
      "         -8.2270e-01,  1.1807e+00,  1.8885e+00,  8.8653e-01, -3.7311e-02,\n",
      "          2.9208e+00, -1.6126e+00, -3.9730e-01,  2.1645e+00, -2.3790e+00,\n",
      "          2.1130e+00, -1.4247e+00, -1.1093e+00, -3.4951e-02,  1.5150e+00,\n",
      "          1.8907e+00, -9.9560e-04,  9.6808e-01, -1.2022e+00, -8.2934e-01,\n",
      "         -1.6889e+00]])\n",
      "Prey Weights:  tensor([[0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
      "         0.0323, 0.0323, 0.0323, 0.0323]], grad_fn=<SqueezeBackward1>)\n",
      "Prey Action per Prey:  tensor([0.0550], grad_fn=<SumBackward1>)\n",
      "Final Action:  tensor([[-2.3240, -0.4732, -0.6076, -0.2489, -0.4761, -0.5602, -0.8963, -0.2976,\n",
      "         -0.2998, -0.4249, -0.8766, -0.6651, -1.0966, -0.1009, -0.6125, -0.1680,\n",
      "         -0.3205, -0.6015, -0.7834, -0.4406, -0.3235, -0.6819, -0.2054, -0.7225,\n",
      "         -0.3501, -0.7279, -0.5511, -0.6774, -0.1519, -0.3982, -0.8082, -0.0702]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agents, neigh, feat = states.shape                  # Shape: (32,32,4)\n",
    "\n",
    "device = states.device\n",
    "dtype  = states.dtype\n",
    "\n",
    "##### Predator #####\n",
    "pred_states = states[:, 0, :]                               # Shape: (32,1,4)\n",
    "mu_pred, sigma_pred = prey_policy.pred_pairwise(pred_states)       # mu=32, simga=32\n",
    "sampled_pred_action = Normal(mu_pred, sigma_pred).sample()  # actions=32\n",
    "pred_actions = torch.tanh(sampled_pred_action) * math.pi    # Value Range [-pi:pi]\n",
    "pred_action_flat = pred_actions.squeeze(-1)\n",
    "print(\"Pred Actions: \", pred_action_flat)\n",
    "\n",
    "if weights is not None:\n",
    "    pred_gain = weights\n",
    "else:\n",
    "    pred_gain = torch.full((agents,), 1/33, device=states.device, dtype=states.dtype) # treat every action equal\n",
    "print(\"Pred Gain: \", pred_gain)\n",
    "\n",
    "##### Prey #####\n",
    "prey_states = states[:, 1:, :]                                      # Shape: (32,31,4)\n",
    "prey_states_flat   = prey_states.reshape(agents * (neigh-1), feat)  # Shape: (32*31,4)\n",
    "\n",
    "mu_prey, sigma_prey = prey_policy.prey_pairwise(prey_states_flat)          # mu=32*31, simga=32*31\n",
    "sampled_prey_action = Normal(mu_prey, sigma_prey).sample()          # actions=32*31\n",
    "prey_actions = (torch.tanh(sampled_prey_action) * math.pi).view(agents, neigh - 1, 1)\n",
    "print(\"Prey Actions: \", prey_actions.squeeze(-1))\n",
    "\n",
    "prey_weight_logits = prey_policy.prey_attention(prey_states_flat)\n",
    "prey_weight_logits = prey_weight_logits.view(agents, neigh-1)       # [A, N-1]\n",
    "prey_weights = torch.softmax(prey_weight_logits, dim=1).view(agents, neigh-1, 1)\n",
    "print(\"Prey Weights: \", prey_weights.squeeze(-1))\n",
    "\n",
    "##### Action Aggregation #####\n",
    "\n",
    "# Aggregation of Prey Actions per Prey\n",
    "prey_actions_nei = prey_actions.squeeze(-1)\n",
    "prey_weights_nei = prey_weights.squeeze(-1)\n",
    "prey_action_per_prey = (prey_actions_nei * prey_weights_nei).sum(dim=1)\n",
    "print(\"Prey Action per Prey: \", prey_action_per_prey)\n",
    "\n",
    "final_action = pred_gain * pred_action_flat + (1.0 - pred_gain) * prey_action_per_prey\n",
    "print(\"Final Action: \", final_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
