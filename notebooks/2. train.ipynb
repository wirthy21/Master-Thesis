{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "from utils.es_utils import *\n",
    "from utils.env_utils import *\n",
    "from utils.train_utils import *\n",
    "from marl_aquarium import aquarium_v0\n",
    "from models.Buffer import Buffer, Pool\n",
    "from models.Generator import GeneratorPolicy\n",
    "from models.Discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272a92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulti-Agent Imitation lernt nur so schnell wie das schwächste Glied.\\n\\nToDo\\'s\\n- Training auf HL Data\\n- GitHub anlegen mit Env\\n- Unterlagen Anmeldung vorbereiten: \"Imitating Predator-Prey Swarm Dynamics using Coevolutionary Generative Adversarial Imitation Learning\"\\n\\nNach Urlaub:\\n    - Velocity in Netz einbauen\\n    - Struktur Netzwerk anpassen (Layer, Batch Normalization, Dropout)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Imitation lernt nur so schnell wie das schwächste Glied.\n",
    "\n",
    "ToDo's\n",
    "- Training auf HL Data\n",
    "- GitHub anlegen mit Env\n",
    "\n",
    "Nach Urlaub:\n",
    "    - Velocity in Netz einbauen\n",
    "    - Struktur Netzwerk anpassen (Layer, Batch Normalization, Dropout)\n",
    "    - Thesis anmelden\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aa3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#Environment\n",
    "pred_count = 1\n",
    "prey_count = 32 \n",
    "action_count = 360\n",
    "total_detections = 33\n",
    "use_walls = True\n",
    "\n",
    "# generated_trajectories\n",
    "gt_gen_episodes = 20\n",
    "gt_clip_length = 30\n",
    "\n",
    "# Buffer\n",
    "pred_buffer_size = 800\n",
    "prey_buffer_size = 24000\n",
    "\n",
    "# Training\n",
    "num_generations = 50\n",
    "pred_batch_size = 512\n",
    "prey_batch_size = 1024\n",
    "gen_dis_ratio = 4\n",
    "\n",
    "# Early Stopping\n",
    "start_es_pred = 70\n",
    "start_es_prey = 50\n",
    "patience = 20\n",
    "\n",
    "# ES-Pertrubation\n",
    "num_perturbations = 16\n",
    "pert_clip_length = 28\n",
    "sigma = 0.12\n",
    "gamma = 0.9998\n",
    "lr_pred_policy = 0.003\n",
    "lr_prey_policy = 0.009\n",
    "\n",
    "# RMSprop\n",
    "lr_pred_dis =  0.0015\n",
    "lr_prey_dis = 0.008\n",
    "alpha=0.99\n",
    "eps_dis=1e-08\n",
    "lambda_gp_pred = 4\n",
    "lambda_gp_prey = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef35ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training folder\n",
    "path = r\"..\\data\\training\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%d.%m.%Y_%H.%M\")\n",
    "folder_name = f\"Training - {timestamp}\"\n",
    "save_dir = os.path.join(path, folder_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Expert Data\n",
    "data_path = rf'..\\data\\processed\\pred_prey_interactions\\expert_tensors\\{total_detections}'\n",
    "ftw_path = rf'..\\data\\processed\\pred_prey_interactions\\full_track_windows\\{total_detections}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7593b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pred_policy = GeneratorPolicy().to(device)\n",
    "pred_policy.set_parameters(init=True)\n",
    "\n",
    "prey_policy = GeneratorPolicy().to(device)\n",
    "prey_policy.set_parameters(init=True)\n",
    "\n",
    "pred_discriminator = Discriminator().to(device)\n",
    "pred_discriminator.set_parameters(init=True)\n",
    "optim_dis_pred = torch.optim.RMSprop(pred_discriminator.parameters(), lr=lr_pred_dis, alpha=alpha, eps=eps_dis)\n",
    "\n",
    "prey_discriminator = Discriminator().to(device)\n",
    "prey_discriminator.set_parameters(init=True)\n",
    "optim_dis_prey = torch.optim.RMSprop(prey_discriminator.parameters(), lr=lr_prey_dis, alpha=alpha, eps=eps_dis)\n",
    "\n",
    "expert_buffer = Buffer(pred_max_length=pred_buffer_size, prey_max_length=prey_buffer_size, device=device)\n",
    "generative_buffer = Buffer(pred_max_length=pred_buffer_size, prey_max_length=prey_buffer_size, device=device)\n",
    "\n",
    "start_frame_pool = Pool(max_length=1000, device=device)\n",
    "start_frame_pool.generate_startframes(ftw_path)\n",
    "\n",
    "early_stopper_pred = EarlyStoppingWasserstein(patience=patience, start_es=start_es_pred)\n",
    "early_stopper_prey = EarlyStoppingWasserstein(patience=patience, start_es=start_es_prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23d55eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert Buffer is empty, load data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Expert Data from local storage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpert Buffer is empty, load data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mexpert_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_expert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m len_exp_pred, len_exp_prey \u001b[38;5;241m=\u001b[39m expert_buffer\u001b[38;5;241m.\u001b[39mlengths()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage of Predator Expert Buffer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, len_exp_pred)\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Dokumente\\Privat\\Bildung\\M. Sc. Social and Economic Data Science\\4. Semester\\Master Thesis\\Code\\notebooks\\models\\Buffer.py:73\u001b[0m, in \u001b[0;36mBuffer.add_expert\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(prey_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     71\u001b[0m     prey_tensors \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m---> 73\u001b[0m n_clips, agent, neigh, feat \u001b[38;5;241m=\u001b[39m prey_tensors\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     74\u001b[0m flat \u001b[38;5;241m=\u001b[39m prey_tensors\u001b[38;5;241m.\u001b[39mreshape(n_clips \u001b[38;5;241m*\u001b[39m agent, neigh, feat)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m single_tensor \u001b[38;5;129;01min\u001b[39;00m flat:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Load Expert Data from local storage\n",
    "print(\"Expert Buffer is empty, load data...\")\n",
    "expert_buffer.add_expert(data_path)\n",
    "len_exp_pred, len_exp_prey = expert_buffer.lengths()\n",
    "\n",
    "print(\"Storage of Predator Expert Buffer: \", len_exp_pred)\n",
    "print(\"Storage of Prey Expert Buffer: \", len_exp_prey, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9574015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain Policies with Expert Data\n",
    "print(\"Pretraining Policies with Behavioral Cloning on Expert Data...\\n\")\n",
    "pred_policy = pretrain_policy(pred_policy, expert_buffer, role='predator', pred_bs=512, epochs=200, lr=1e-3, save_dir=save_dir)\n",
    "prey_policy = pretrain_policy(prey_policy, expert_buffer, role='prey', prey_bs=1024, epochs=200, lr=1e-3, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d702e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Buffer is empty, generating data...\n",
      "Storage of Predator Generative Buffer:  600\n",
      "Storage of Prey Generative Buffer:  19200\n"
     ]
    }
   ],
   "source": [
    "# Generate Trajectories for Generative Buffer\n",
    "print(\"Generative Buffer is empty, generating data...\")\n",
    "generate_trajectories(buffer=generative_buffer, start_frame_pool=start_frame_pool,\n",
    "                        pred_count=pred_count, prey_count=prey_count, action_count=action_count, \n",
    "                        pred_policy=pred_policy, prey_policy=prey_policy, \n",
    "                        clip_length=gt_clip_length, num_generative_episodes=gt_gen_episodes,\n",
    "                        use_walls=True)\n",
    "\n",
    "len_gen_pred, len_gen_prey = generative_buffer.lengths()\n",
    "print(\"Storage of Predator Generative Buffer: \", len_gen_pred)\n",
    "print(\"Storage of Prey Generative Buffer: \", len_gen_prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53670d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 1/50] - Time: 19:01 - Estimated Finish: 06.08.2025 03:39:59\n",
      "Predator | Avg. ES-Reward: 0.0279 | Wasserstein Loss: 3.1616 | Expert Scores: -0.0520 | Policy Scores: -0.0149\n",
      "Prey     | Avg. ES-Reward: 0.7099 | Wasserstein Loss: 2.4791 | Expert Scores: 0.1916 | Policy Scores: 0.2092\n",
      "\n",
      "Checkpoint successfully saved! \n",
      " \n",
      "[Generation 2/50] - Time: 19:22 - Estimated Finish: 06.08.2025 03:56:31\n",
      "Predator | Avg. ES-Reward: 0.3146 | Wasserstein Loss: 1.7984 | Expert Scores: 0.0576 | Policy Scores: -0.4058\n",
      "Prey     | Avg. ES-Reward: -0.2045 | Wasserstein Loss: 4.4741 | Expert Scores: 1.5678 | Policy Scores: -3.1331\n",
      "\n",
      "[Generation 3/50] - Time: 19:08 - Estimated Finish: 06.08.2025 03:45:35\n",
      "Predator | Avg. ES-Reward: 0.5242 | Wasserstein Loss: -1.0042 | Expert Scores: 0.4239 | Policy Scores: -1.1570\n",
      "Prey     | Avg. ES-Reward: 0.4680 | Wasserstein Loss: 2.1963 | Expert Scores: -0.0532 | Policy Scores: 1.4880\n",
      "\n",
      "[Generation 4/50] - Time: 19:06 - Estimated Finish: 06.08.2025 03:44:10\n",
      "Predator | Avg. ES-Reward: 0.5441 | Wasserstein Loss: -2.4316 | Expert Scores: 1.2082 | Policy Scores: -2.2617\n",
      "Prey     | Avg. ES-Reward: 1.2695 | Wasserstein Loss: -2.5171 | Expert Scores: 1.1560 | Policy Scores: -2.3993\n",
      "\n",
      "[Generation 5/50] - Time: 19:10 - Estimated Finish: 06.08.2025 03:47:09\n",
      "Predator | Avg. ES-Reward: 0.9584 | Wasserstein Loss: -3.6503 | Expert Scores: 1.6781 | Policy Scores: -2.3191\n",
      "Prey     | Avg. ES-Reward: 0.2462 | Wasserstein Loss: -3.8186 | Expert Scores: 1.4313 | Policy Scores: -7.7373\n",
      "\n",
      "[Generation 6/50] - Time: 18:35 - Estimated Finish: 06.08.2025 03:21:03\n",
      "Predator | Avg. ES-Reward: 1.2269 | Wasserstein Loss: -4.8207 | Expert Scores: 2.3371 | Policy Scores: -3.6290\n",
      "Prey     | Avg. ES-Reward: 2.0119 | Wasserstein Loss: -1.2895 | Expert Scores: 1.6219 | Policy Scores: 0.1929\n",
      "\n",
      "[Generation 7/50] - Time: 20:06 - Estimated Finish: 06.08.2025 04:27:27\n",
      "Predator | Avg. ES-Reward: 1.5965 | Wasserstein Loss: -5.2310 | Expert Scores: 2.4697 | Policy Scores: -3.3554\n",
      "Prey     | Avg. ES-Reward: 0.7932 | Wasserstein Loss: -3.4297 | Expert Scores: -0.0523 | Policy Scores: -10.7321\n",
      "\n",
      "[Generation 8/50] - Time: 17:12 - Estimated Finish: 06.08.2025 02:22:43\n",
      "Predator | Avg. ES-Reward: 1.1593 | Wasserstein Loss: -5.6711 | Expert Scores: 2.4346 | Policy Scores: -5.3103\n",
      "Prey     | Avg. ES-Reward: 1.0754 | Wasserstein Loss: -3.0873 | Expert Scores: 1.4078 | Policy Scores: -1.9049\n",
      "\n",
      "[Generation 9/50] - Time: 18:29 - Estimated Finish: 06.08.2025 03:16:39\n",
      "Predator | Avg. ES-Reward: 1.5710 | Wasserstein Loss: -5.7546 | Expert Scores: 2.3047 | Policy Scores: -3.9715\n",
      "Prey     | Avg. ES-Reward: 1.0025 | Wasserstein Loss: -5.3029 | Expert Scores: 2.7210 | Policy Scores: -4.1822\n",
      "\n",
      "[Generation 10/50] - Time: 19:33 - Estimated Finish: 06.08.2025 04:00:21\n",
      "Predator | Avg. ES-Reward: 0.9775 | Wasserstein Loss: -6.2526 | Expert Scores: 2.0944 | Policy Scores: -6.6171\n",
      "Prey     | Avg. ES-Reward: 1.3859 | Wasserstein Loss: -5.5796 | Expert Scores: 1.7688 | Policy Scores: -5.2905\n",
      "\n",
      "[Generation 11/50] - Time: 19:07 - Estimated Finish: 06.08.2025 03:43:14\n",
      "Predator | Avg. ES-Reward: 1.1767 | Wasserstein Loss: -5.9225 | Expert Scores: 2.0556 | Policy Scores: -4.4664\n",
      "Prey     | Avg. ES-Reward: 0.7766 | Wasserstein Loss: -5.3836 | Expert Scores: 1.4692 | Policy Scores: -6.1550\n",
      "\n",
      "[Generation 12/50] - Time: 19:11 - Estimated Finish: 06.08.2025 03:45:39\n",
      "Predator | Avg. ES-Reward: 1.4639 | Wasserstein Loss: -6.2035 | Expert Scores: 2.2369 | Policy Scores: -6.2228\n",
      "Prey     | Avg. ES-Reward: 1.3487 | Wasserstein Loss: -6.0656 | Expert Scores: 0.5457 | Policy Scores: -6.2803\n",
      "\n",
      "[Generation 13/50] - Time: 19:20 - Estimated Finish: 06.08.2025 03:51:34\n",
      "Predator | Avg. ES-Reward: 1.3645 | Wasserstein Loss: -6.0135 | Expert Scores: 1.7032 | Policy Scores: -5.4059\n",
      "Prey     | Avg. ES-Reward: 0.8367 | Wasserstein Loss: -4.6508 | Expert Scores: 0.2587 | Policy Scores: -7.9636\n",
      "\n",
      "[Generation 14/50] - Time: 19:07 - Estimated Finish: 06.08.2025 03:43:42\n",
      "Predator | Avg. ES-Reward: 1.4071 | Wasserstein Loss: -6.3514 | Expert Scores: 1.7836 | Policy Scores: -6.1309\n",
      "Prey     | Avg. ES-Reward: 1.4353 | Wasserstein Loss: -5.2017 | Expert Scores: 0.5043 | Policy Scores: -4.9267\n",
      "\n",
      "[Generation 15/50] - Time: 19:02 - Estimated Finish: 06.08.2025 03:40:42\n",
      "Predator | Avg. ES-Reward: 1.1258 | Wasserstein Loss: -6.3370 | Expert Scores: 1.7553 | Policy Scores: -6.0300\n",
      "Prey     | Avg. ES-Reward: 1.3735 | Wasserstein Loss: -5.4518 | Expert Scores: 0.3676 | Policy Scores: -8.0219\n",
      "\n",
      "[Generation 16/50] - Time: 19:11 - Estimated Finish: 06.08.2025 03:45:48\n",
      "Predator | Avg. ES-Reward: 0.9898 | Wasserstein Loss: -6.2277 | Expert Scores: 1.9141 | Policy Scores: -5.8703\n",
      "Prey     | Avg. ES-Reward: 0.6588 | Wasserstein Loss: -5.3628 | Expert Scores: 0.0427 | Policy Scores: -5.8169\n",
      "\n",
      "[Generation 17/50] - Time: 19:23 - Estimated Finish: 06.08.2025 03:52:31\n",
      "Predator | Avg. ES-Reward: 1.3938 | Wasserstein Loss: -6.4642 | Expert Scores: 1.5569 | Policy Scores: -6.2930\n",
      "Prey     | Avg. ES-Reward: 1.1483 | Wasserstein Loss: -2.8696 | Expert Scores: 0.2121 | Policy Scores: -5.9011\n",
      "\n",
      "[Generation 18/50] - Time: 19:06 - Estimated Finish: 06.08.2025 03:43:04\n",
      "Predator | Avg. ES-Reward: 0.8713 | Wasserstein Loss: -6.4566 | Expert Scores: 1.6786 | Policy Scores: -6.4706\n",
      "Prey     | Avg. ES-Reward: 0.4942 | Wasserstein Loss: -5.6224 | Expert Scores: -0.5173 | Policy Scores: -6.5248\n",
      "\n",
      "[Generation 19/50] - Time: 19:09 - Estimated Finish: 06.08.2025 03:45:04\n",
      "Predator | Avg. ES-Reward: 1.5169 | Wasserstein Loss: -6.1771 | Expert Scores: 1.6373 | Policy Scores: -6.0185\n",
      "Prey     | Avg. ES-Reward: 0.9469 | Wasserstein Loss: -2.2000 | Expert Scores: -0.4355 | Policy Scores: -5.9972\n",
      "\n",
      "[Generation 20/50] - Time: 19:02 - Estimated Finish: 06.08.2025 03:41:25\n",
      "Predator | Avg. ES-Reward: 1.2589 | Wasserstein Loss: -6.2962 | Expert Scores: 1.2136 | Policy Scores: -6.7113\n",
      "Prey     | Avg. ES-Reward: 1.4104 | Wasserstein Loss: -4.8022 | Expert Scores: -1.0233 | Policy Scores: -5.9019\n",
      "\n",
      "[Generation 21/50] - Time: 19:09 - Estimated Finish: 06.08.2025 03:44:29\n",
      "Predator | Avg. ES-Reward: 1.1230 | Wasserstein Loss: -6.4006 | Expert Scores: 1.2143 | Policy Scores: -6.5905\n",
      "Prey     | Avg. ES-Reward: 1.2237 | Wasserstein Loss: -5.6430 | Expert Scores: -1.2917 | Policy Scores: -9.2683\n",
      "\n",
      "[Generation 22/50] - Time: 19:13 - Estimated Finish: 06.08.2025 03:46:27\n",
      "Predator | Avg. ES-Reward: 1.1359 | Wasserstein Loss: -6.4672 | Expert Scores: 1.2614 | Policy Scores: -6.8029\n",
      "Prey     | Avg. ES-Reward: 0.9811 | Wasserstein Loss: -5.7967 | Expert Scores: -0.9168 | Policy Scores: -7.6857\n",
      "\n",
      "[Generation 23/50] - Time: 18:59 - Estimated Finish: 06.08.2025 03:40:11\n",
      "Predator | Avg. ES-Reward: 1.2232 | Wasserstein Loss: -6.5289 | Expert Scores: 1.3881 | Policy Scores: -6.5949\n",
      "Prey     | Avg. ES-Reward: 1.3577 | Wasserstein Loss: -5.5584 | Expert Scores: -0.9441 | Policy Scores: -8.5541\n",
      "\n",
      "[PREDATOR] Early stopping triggered: No improvement for 20 epochs.\n",
      "Models successfully saved!\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "dis_metrics_pred = []\n",
    "dis_metrics_prey = []\n",
    "\n",
    "es_metrics_pred = []\n",
    "es_metrics_prey = []\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Sample traj from expert and generative buffer\n",
    "    expert_pred_batch, expert_prey_batch = expert_buffer.sample(pred_batch_size, prey_batch_size)\n",
    "    policy_pred_batch, policy_prey_batch = generative_buffer.sample(pred_batch_size, prey_batch_size)\n",
    "\n",
    "    # Predator discriminator update\n",
    "    dis_metric_pred = pred_discriminator.update(expert_pred_batch, policy_pred_batch, optim_dis_pred, lambda_gp_pred)\n",
    "    dis_metrics_pred.append(dis_metric_pred)\n",
    "                                     \n",
    "    # Prey discriminator update\n",
    "    dis_metric_prey = prey_discriminator.update(expert_prey_batch, policy_prey_batch, optim_dis_prey, lambda_gp_prey)\n",
    "    dis_metrics_prey.append(dis_metric_prey)\n",
    "\n",
    "    for i in range(gen_dis_ratio):\n",
    "        pred_stats = pred_policy.update(\"predator\", \"pairwise\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma, clip_length=pert_clip_length,\n",
    "                                        use_walls=use_walls, start_frame_pool=start_frame_pool)\n",
    "        \n",
    "\n",
    "        pred_stats += pred_policy.update(\"predator\", \"attention\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma, clip_length=pert_clip_length,\n",
    "                                        use_walls=use_walls, start_frame_pool=start_frame_pool)\n",
    "        es_metrics_pred.append(pred_stats)\n",
    "\n",
    "\n",
    "        prey_stats = prey_policy.update(\"prey\", \"pairwise\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma, clip_length=pert_clip_length,\n",
    "                                        use_walls=use_walls, start_frame_pool=start_frame_pool)\n",
    "        \n",
    "        prey_stats += prey_policy.update(\"prey\", \"attention\",\n",
    "                                        pred_count, prey_count, action_count,\n",
    "                                        pred_policy, prey_policy,\n",
    "                                        pred_discriminator, prey_discriminator,\n",
    "                                        num_perturbations, generation,\n",
    "                                        lr_pred_policy, lr_prey_policy,\n",
    "                                        sigma, gamma, clip_length=pert_clip_length,\n",
    "                                        use_walls=use_walls, start_frame_pool=start_frame_pool)\n",
    "        es_metrics_prey.append(prey_stats)\n",
    "\n",
    "        # Generate new trajectories with updated policies\n",
    "        generate_trajectories(buffer=generative_buffer, start_frame_pool=start_frame_pool,\n",
    "                                pred_count=pred_count, prey_count=prey_count, action_count=action_count, \n",
    "                                pred_policy=pred_policy, prey_policy=prey_policy, \n",
    "                                clip_length=gt_clip_length, num_generative_episodes=gt_gen_episodes,\n",
    "                                use_walls=use_walls)\n",
    "\n",
    "    # also reduce globally\n",
    "    sigma *= gamma\n",
    "    lr_pred_policy *= gamma\n",
    "    lr_prey_policy *= gamma\n",
    "\n",
    "    last_epoch_duration = time.time() - start_time\n",
    "    estimated_time, epoch_time = remaining_time(num_generations, last_epoch_duration, current_generation=generation)\n",
    "\n",
    "    avg_es_pred = np.mean([m['avg_reward_diff'] for m in pred_stats])\n",
    "    avg_es_prey = np.mean([m['avg_reward_diff'] for m in prey_stats])\n",
    "        \n",
    "    print(f\"[Generation {generation+1}/{num_generations}] - Time: {epoch_time} - Estimated Finish: {estimated_time}\" )\n",
    "    print(f\"Predator | Avg. ES-Reward: {avg_es_pred:.4f} | Wasserstein Loss: {dis_metric_pred[0]:.4f} | Expert Scores: {dis_metric_pred[2]:.4f} | Policy Scores: {dis_metric_pred[3]:.4f}\")\n",
    "    print(f\"Prey     | Avg. ES-Reward: {avg_es_prey:.4f} | Wasserstein Loss: {dis_metric_prey[0]:.4f} | Expert Scores: {dis_metric_prey[2]:.4f} | Policy Scores: {dis_metric_prey[3]:.4f}\\n\")\n",
    "\n",
    "    if early_stopper_pred(dis_metric_pred[0], generation, \"predator\") or early_stopper_prey(dis_metric_prey[0], generation, \"prey\"):\n",
    "        break\n",
    "\n",
    "\n",
    "    if generation % 25 == 0:\n",
    "        save_checkpoint(save_dir, generation,\n",
    "                        pred_policy, prey_policy,\n",
    "                        pred_discriminator, prey_discriminator,\n",
    "                        optim_dis_pred, optim_dis_prey,\n",
    "                        expert_buffer, generative_buffer,\n",
    "                        dis_metrics_pred, dis_metrics_prey,\n",
    "                        es_metrics_pred, es_metrics_prey)\n",
    "\n",
    "\n",
    "# Save models\n",
    "save_models(save_dir,\n",
    "            pred_policy, prey_policy,\n",
    "            pred_discriminator, prey_discriminator,\n",
    "            optim_dis_pred, optim_dis_prey,\n",
    "            expert_buffer, generative_buffer,\n",
    "            dis_metrics_pred, dis_metrics_prey,\n",
    "            es_metrics_pred, es_metrics_prey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
